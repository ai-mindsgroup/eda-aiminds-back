# Arquitetura Refatorada - HybridQueryProcessorV2

## ğŸ“‹ VisÃ£o Geral

O **HybridQueryProcessorV2** Ã© uma refatoraÃ§Ã£o completa do processador de queries hÃ­brido, integrando:

1. âœ… **Sistema de fragmentaÃ§Ã£o de queries** (FastQueryFragmenter) para GROQ 6000 TPM
2. âœ… **Busca de histÃ³rico/cache** no Supabase antes de processar
3. âœ… **Fallback guiado** pelos 6 chunks analÃ­ticos originais (evita redundÃ¢ncia)
4. âœ… **Controle dinÃ¢mico** entre embeddings RAG e CSV com fragmentaÃ§Ã£o
5. âœ… **Logging detalhado** para auditoria completa
6. âœ… **Camada de abstraÃ§Ã£o LLM** consistente (LangChain)

---

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HybridQueryProcessorV2                        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  QueryAnalyzerâ”‚  â”‚ LLMManager   â”‚  â”‚MemoryManager     â”‚   â”‚
â”‚  â”‚  (semantic)   â”‚  â”‚ (abstraction)â”‚  â”‚ (Supabase)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         FLUXO DE PROCESSAMENTO                        â”‚    â”‚
â”‚  â”‚                                                        â”‚    â”‚
â”‚  â”‚  1. Buscar Cache/HistÃ³rico (Supabase)                â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  2. Analisar Query (LLM + QueryAnalyzer)             â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  3. Buscar Chunks Existentes (RAG)                   â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  4. Decidir EstratÃ©gia (rag_only/csv_fallback/       â”‚    â”‚
â”‚  â”‚                         csv_fragmented)               â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  5. Processar baseado na estratÃ©gia                  â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  6. Gerar Resposta (LLM com contexto)                â”‚    â”‚
â”‚  â”‚        â†“                                               â”‚    â”‚
â”‚  â”‚  7. Armazenar em Cache (24h TTL)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ EstratÃ©gias de Processamento

### 1. RAG ONLY
**Quando usar:**
- Chunks existentes cobrem â‰¥80% dos aspectos necessÃ¡rios
- â‰¥3 chunks relevantes encontrados
- Query simples

**Vantagens:**
- âš¡ Mais rÃ¡pido (sem acesso a disco)
- ğŸ’° Menor custo (apenas busca vetorial)
- ğŸ¯ Contexto prÃ©-otimizado

**Fluxo:**
```
Query â†’ Embedding â†’ Busca Vetorial â†’ Chunks (6) â†’ Contexto â†’ LLM â†’ Resposta
```

**Exemplo:**
```python
query = "Qual a distribuiÃ§Ã£o de valores do dataset?"
# Usa chunks: metadata_distribution, metadata_central_variability
# Resposta em ~2s
```

---

### 2. CSV FALLBACK
**Quando usar:**
- Chunks cobrem <80% dos aspectos
- Query menciona anÃ¡lises especÃ­ficas nÃ£o cobertas
- Dataset cabe em memÃ³ria sem fragmentar

**Vantagens:**
- ğŸ¯ Preenche gaps especÃ­ficos
- ğŸ”„ Gera chunks complementares (nÃ£o duplicados)
- ğŸ’¾ Armazena novos chunks no Supabase

**Fluxo:**
```
Query â†’ Chunks Existentes â†’ Identificar Gaps â†’ Carregar CSV (focado) 
    â†’ AnÃ¡lise Complementar â†’ Novos Chunks â†’ Contexto HÃ­brido â†’ LLM â†’ Resposta
```

**Exemplo:**
```python
query = "Quais outliers na coluna Amount?"
# Chunks existentes: metadata_types, metadata_distribution
# Gap identificado: outliers detalhados
# Carrega CSV â†’ AnÃ¡lise IQR â†’ Gera chunk complementary_outliers
# Resposta em ~5s
```

---

### 3. CSV FRAGMENTED
**Quando usar:**
- Dataset grande (>3000 linhas ou >20 colunas)
- Query complexa (>50 palavras)
- Limite GROQ 6000 tokens seria ultrapassado

**Vantagens:**
- âœ… Respeita limite GROQ 6000 TPM
- ğŸš€ Usa FastQueryFragmenter (heurÃ­stico, 600x mais rÃ¡pido)
- ğŸ“¦ Processa fragmentos sequencialmente
- ğŸ”— Agrega resultados parciais

**Fluxo:**
```
Query â†’ AnÃ¡lise â†’ Carregar CSV â†’ FragmentaÃ§Ã£o (Fast) 
    â†’ Processar Fragmentos (paralelo) â†’ AgregaÃ§Ã£o â†’ Chunks Fragmentados 
    â†’ Contexto Completo â†’ LLM â†’ Resposta
```

**Exemplo:**
```python
query = "CorrelaÃ§Ã£o entre Amount, Time e V1-V28, identificando fraudes"
df.shape = (5000, 28)  # Grande!
# FastQueryFragmenter divide em:
# - Fragmento 1: Amount, Time, Class (linhas 0-1250)
# - Fragmento 2: V1-V14 (linhas 0-1250)
# - Fragmento 3: V15-V28 (linhas 0-1250)
# - ... (total 12 fragmentos)
# Cada fragmento ~5000 tokens < 6000 limite
# Resposta em ~15s
```

---

## ğŸ§© Componentes Integrados

### 1. FastQueryFragmenter
**Arquivo:** `src/llm/fast_fragmenter.py`

**FunÃ§Ã£o:** Divide queries grandes em fragmentos menores

**EstratÃ©gias:**
- `COLUMN_GROUPS`: Divide por colunas (<50% mencionadas na query)
- `ROW_SEGMENTS`: Divide por linhas (query menciona "todas as colunas")
- `HYBRID`: Divide colunas E linhas (dataset muito grande)

**Performance:** ~1ms vs ~400ms (LLM-based)

```python
from src.llm.fast_fragmenter import fragment_query_fast

needs_frag, fragments, reason = fragment_query_fast(
    query="CorrelaÃ§Ã£o entre todas features",
    df=df,
    token_budget=TokenBudget(max_tokens_per_request=6000)
)

# Resultado:
# needs_frag = True
# fragments = [QueryFragment(...), QueryFragment(...), ...]
# reason = "Dataset grande - dividir por segmentos de linhas"
```

---

### 2. SupabaseMemoryManager
**Arquivo:** `src/memory/supabase_memory.py`

**FunÃ§Ã£o:** Gerencia cache e histÃ³rico de queries

**Tabelas:**
- `agent_sessions`: SessÃµes de usuÃ¡rio
- `agent_conversations`: HistÃ³rico de queries/respostas
- `agent_context`: Cache e contexto (ContextType.CACHE)

**TTL:** 24 horas (configurÃ¡vel)

```python
# Buscar cache
cached = await memory_manager.get_context(
    session_id=session_id,
    context_type=ContextType.CACHE,
    context_key=cache_key
)

# Armazenar resultado
await memory_manager.save_context(
    session_id=session_id,
    context_type=ContextType.CACHE,
    context_key=cache_key,
    context_data={'result': result},
    expires_at=datetime.now() + timedelta(hours=24)
)
```

---

### 3. LLMManager (AbstraÃ§Ã£o)
**Arquivo:** `src/llm/manager.py`

**FunÃ§Ã£o:** Camada de abstraÃ§Ã£o para mÃºltiplos LLMs

**Providers:**
- Google Gemini (padrÃ£o)
- OpenAI GPT-4
- Groq (Llama)
- Fallback automÃ¡tico

```python
from src.llm.manager import get_llm_manager

llm_manager = get_llm_manager()

# Obter LLM com configuraÃ§Ã£o
llm = llm_manager.get_llm(provider='google', temperature=0.2)

# Usar (async)
response = await llm.ainvoke("Analise este dataset...")
answer = response.content
```

---

## ğŸ“Š DecisÃ£o de EstratÃ©gia

### Algoritmo de DecisÃ£o

```python
def _decide_strategy(query, analysis, existing_chunks, force_csv):
    # 1. Force CSV?
    if force_csv:
        return 'csv_fragmented'
    
    # 2. Calcular cobertura dos chunks
    covered_aspects = identify_covered_aspects(existing_chunks)
    required_aspects = get_required_aspects(analysis['category'])
    
    coverage = len(covered âˆ© required) / len(required)
    
    # 3. DecisÃ£o baseada em cobertura
    if coverage >= 0.8 AND len(existing_chunks) >= 3:
        return 'rag_only'  # âœ… Chunks suficientes
    
    # 4. Query pequena?
    if len(query.split()) < 50:
        return 'csv_fallback'  # CSV sem fragmentar
    
    # 5. Query grande
    return 'csv_fragmented'  # Requer fragmentaÃ§Ã£o
```

### Mapeamento de Aspectos

**Chunks â†’ Aspectos Cobertos:**

| Chunk Type | Aspectos Cobertos |
|------------|------------------|
| `metadata_types` | structure, schema, data_types |
| `metadata_distribution` | distribution, statistics_basic |
| `metadata_central_variability` | statistics, central_tendency |
| `metadata_frequency_outliers` | outliers, anomalies |
| `metadata_correlations` | correlation, relationships |
| `metadata_patterns_clusters` | patterns, trends |

**Query â†’ Aspectos NecessÃ¡rios:**

| Categoria Query | Aspectos NecessÃ¡rios |
|----------------|---------------------|
| `statistics` | statistics, distribution |
| `correlation` | correlation |
| `outliers` | outliers, anomalies |
| `patterns` | patterns, trends |
| `structure` | structure, schema |

---

## ğŸ” Fallback Guiado (Evita RedundÃ¢ncia)

### Problema Original
âŒ Processador antigo gerava chunks duplicados:
- CSV carregado â†’ anÃ¡lise completa â†’ chunks gerados
- Chunks sobrepunham com os 6 originais
- MemÃ³ria desperdiÃ§ada

### SoluÃ§Ã£o V2
âœ… Usa chunks existentes como **guia**:

```python
# 1. Identificar o que JÃ EXISTE
covered = identify_covered_aspects(existing_chunks)
# covered = {'statistics', 'distribution', 'structure'}

# 2. Identificar o que FALTA (gaps)
required = get_required_aspects(query_category)
# required = {'statistics', 'outliers'}

gaps = required - covered
# gaps = {'outliers'}

# 3. Carregar CSV APENAS para gaps
if gaps:
    df = load_csv(source_id)
    csv_analysis = perform_focused_analysis(df, gaps)
    # Analisa SOMENTE outliers
    
    # 4. Gerar chunks COMPLEMENTARES
    new_chunks = generate_complementary_chunks(csv_analysis, covered)
    # Gera chunk 'complementary_outliers' (Ãºnico, nÃ£o duplicado)
```

### BenefÃ­cios
- ğŸ“‰ **Reduz armazenamento:** NÃ£o duplica anÃ¡lises
- âš¡ **Mais rÃ¡pido:** AnÃ¡lise focada (nÃ£o completa)
- ğŸ¯ **Preciso:** Preenche apenas gaps reais

---

## ğŸ›¡ï¸ Garantias de Token Budget

### Limite GROQ: 6000 TPM

**TokenBudgetController:**
```python
class TokenBudget:
    max_tokens_per_request = 6000
    reserved_tokens = 500  # Para prompt/resposta
    safety_margin = 200     # Buffer de seguranÃ§a
    
    available_tokens = 6000 - 500 - 200 = 5300
```

**ValidaÃ§Ã£o de Fragmentos:**
```python
def validate_fragment(fragment):
    estimated_tokens = estimate_tokens(fragment)
    
    if estimated_tokens > budget.available_tokens:
        raise ValueError("Fragmento excede limite!")
    
    return True
```

**CÃ¡lculo de Tokens:**
```python
# Regra aproximada: 4 caracteres = 1 token
def estimate_tokens(text):
    return len(text) // 4

# Para DataFrame:
def estimate_dataframe_tokens(df):
    # Colunas
    col_tokens = len(df.columns) * 10
    
    # CÃ©lulas
    cell_tokens = df.shape[0] * df.shape[1] * 5
    
    return col_tokens + cell_tokens
```

**CÃ¡lculo de MÃ¡ximo de Linhas:**
```python
def calculate_max_rows(df, available_tokens):
    cols = len(df.columns)
    
    # Cada linha â‰ˆ cols * 5 tokens
    tokens_per_row = cols * 5
    
    max_rows = available_tokens // tokens_per_row
    
    return max(1, max_rows)  # MÃ­nimo 1 linha

# Exemplo:
# df com 25 colunas
# available_tokens = 5300
# tokens_per_row = 25 * 5 = 125
# max_rows = 5300 // 125 = 42 linhas por fragmento
```

---

## ğŸ“ Logging Detalhado

### NÃ­veis de Log

```python
# INFO: Eventos principais
logger.info("ğŸ” Processando query: {query[:100]}...")
logger.info("ğŸ“Š AnÃ¡lise: COMPLEX | Categoria: correlation")
logger.info("âœ… EstratÃ©gia: csv_fragmented")

# DEBUG: Detalhes internos
logger.debug("ğŸ“¦ CSV do cache: creditcard_abc123")
logger.debug("ğŸ” Buscando cache: a3f2b1c9...")

# WARNING: SituaÃ§Ãµes anormais
logger.warning("âš ï¸ Nenhum chunk encontrado, fallback para CSV")
logger.warning("âš ï¸ Erro ao buscar cache: timeout")

# ERROR: Erros recuperÃ¡veis
logger.error("âŒ Erro ao carregar CSV: FileNotFoundError")
logger.error("âŒ Erro LLM: rate limit exceeded")
```

### Auditoria Completa

Cada processamento gera log estruturado:

```
ğŸ“¥ INÃCIO - Query: CorrelaÃ§Ã£o entre Amount e Time...
   Source: creditcard_abc123 | Session: sess_20250120_123456
   
ğŸ” Buscando cache: a3f2b1c9d4e5f6...
ğŸ“Š AnÃ¡lise: COMPLEX | Categoria: correlation | Requer CSV: True
ğŸ“¦ Chunks encontrados: 4
   - metadata_types
   - metadata_distribution
   - metadata_central_variability
   - metadata_correlations

ğŸ“Š Cobertura: 75.0% (3/4 aspectos)
ğŸ¯ EstratÃ©gia: csv_fallback | RazÃ£o: Chunks cobrem 75% dos aspectos
ğŸ“Š Aspectos cobertos: {'statistics', 'distribution', 'structure'}
âš ï¸ Gaps identificados: {'correlation_detailed'}

ğŸ“‚ Carregando CSV para preencher gaps...
âœ… CSV carregado: (5000, 28)
ğŸ¯ AnÃ¡lise focada nos gaps: ['correlation_detailed']
ğŸ†• Gerando chunks complementares (evitando duplicaÃ§Ã£o)
âœ… 1 chunks complementares armazenados

ğŸ¤– LLM recomenda: csv_fallback
âœ… SUCESSO - Tempo: 4.52s | EstratÃ©gia: csv_fallback
ğŸ’¾ Resultado armazenado em cache (TTL: 24h)
```

---

## ğŸ§ª Testes de IntegraÃ§Ã£o

**Arquivo:** `test_hybrid_processor_v2_integration.py`

### CenÃ¡rios Testados

1. **RAG ONLY:** Query simples com chunks suficientes
2. **CSV FALLBACK:** Query complexa, complementa com CSV
3. **CSV FRAGMENTED:** Dataset grande, requer fragmentaÃ§Ã£o
4. **CACHE:** Segunda execuÃ§Ã£o usa cache

### Executar Testes

```bash
# Testes de integraÃ§Ã£o
python test_hybrid_processor_v2_integration.py

# Testes rÃ¡pidos de fragmentaÃ§Ã£o
python test_fast_fragmentation.py
```

### MÃ©tricas Esperadas

| Teste | Tempo | EstratÃ©gia | Cache |
|-------|-------|-----------|-------|
| RAG ONLY | ~2s | rag_only | âŒ |
| CSV FALLBACK | ~5s | csv_fallback | âŒ |
| CSV FRAGMENTED | ~15s | csv_fragmented | âŒ |
| CACHE (2Âª exec) | ~0.5s | N/A | âœ… |

---

## ğŸ“š Exemplos de Uso

### Exemplo 1: Query Simples (RAG ONLY)

```python
from src.agent.hybrid_query_processor_v2 import HybridQueryProcessorV2
from src.embeddings.vector_store import VectorStore
from src.embeddings.generator import EmbeddingGenerator

# Setup
processor = HybridQueryProcessorV2(
    vector_store=VectorStore(),
    embedding_generator=EmbeddingGenerator(),
    agent_name="my_agent"
)

# Processar
result = await processor.process_query(
    query="Qual a mÃ©dia da coluna Amount?",
    source_id="creditcard_abc123"
)

# Resultado
print(result['strategy_decision']['strategy'])  # â†’ 'rag_only'
print(result['answer'])  # â†’ "A mÃ©dia da coluna Amount Ã© 88.35..."
print(result['processing_time_seconds'])  # â†’ 2.1
```

### Exemplo 2: Query Complexa (CSV FALLBACK)

```python
result = await processor.process_query(
    query="Identifique outliers na coluna Amount usando mÃ©todo IQR",
    source_id="creditcard_abc123"
)

# Resultado
print(result['strategy_decision']['strategy'])  # â†’ 'csv_fallback'
print(result['covered_aspects'])  # â†’ ['statistics', 'distribution']
print(result['gaps_filled'])  # â†’ ['outliers']
print(result['new_chunks_generated'])  # â†’ 1
print(result['csv_accessed'])  # â†’ True
```

### Exemplo 3: Dataset Grande (CSV FRAGMENTED)

```python
result = await processor.process_query(
    query="""Analise correlaÃ§Ã£o entre Amount, Time e todas features V1-V28,
             identificando padrÃµes de fraude e outliers significativos""",
    source_id="creditcard_large_5000rows",
    force_csv=True  # ForÃ§ar para garantir fragmentaÃ§Ã£o
)

# Resultado
print(result['strategy_decision']['strategy'])  # â†’ 'csv_fragmented'
print(result['fragments_count'])  # â†’ 12
print(result['fragments_success'])  # â†’ 12
print(result['fragmentation_reason'])  # â†’ "Query complexa, fragmentaÃ§Ã£o necessÃ¡ria"
```

### Exemplo 4: Usar Cache

```python
# Primeira execuÃ§Ã£o
result1 = await processor.process_query(
    query="MÃ©dia de Amount",
    source_id="creditcard_abc123"
)

print(result1['from_cache'])  # â†’ False
print(result1['processing_time_seconds'])  # â†’ 3.2

# Segunda execuÃ§Ã£o (mesma query)
result2 = await processor.process_query(
    query="MÃ©dia de Amount",
    source_id="creditcard_abc123",
    session_id=result1['session_id']  # IMPORTANTE: mesma sessÃ£o
)

print(result2['from_cache'])  # â†’ True
print(result2['processing_time_seconds'])  # â†’ 0.5 (6x mais rÃ¡pido!)
```

---

## ğŸš€ PrÃ³ximos Passos

### Melhorias Planejadas

1. **FragmentaÃ§Ã£o Paralela:**
   - Processar fragmentos em paralelo (asyncio)
   - Reduzir tempo de ~15s para ~5s

2. **Cache Inteligente:**
   - Cache por similaridade semÃ¢ntica (nÃ£o apenas hash exato)
   - "MÃ©dia de Amount" vs "Amount mÃ©dio" â†’ mesmo cache

3. **EstratÃ©gia TIME_WINDOWS:**
   - Implementar fragmentaÃ§Ã£o temporal
   - Para queries como "AnÃ¡lise mensal de fraudes"

4. **Fallback LLM AutomÃ¡tico:**
   - Se heurÃ­stica tem confidence <0.7, usar LLM
   - Melhor de dois mundos: velocidade + precisÃ£o

5. **MÃ©tricas de Performance:**
   - Dashboard com mÃ©tricas de cache hit/miss
   - Tempo mÃ©dio por estratÃ©gia
   - Uso de tokens por query

---

## ğŸ“– ReferÃªncias

- **FastQueryFragmenter:** `src/llm/fast_fragmenter.py`
- **SimpleQueryAggregator:** `src/llm/simple_aggregator.py`
- **SupabaseMemoryManager:** `src/memory/supabase_memory.py`
- **LLMManager:** `src/llm/manager.py`
- **QueryAnalyzer:** `src/agent/query_analyzer.py`
- **Testes RÃ¡pidos:** `test_fast_fragmentation.py`
- **Testes IntegraÃ§Ã£o:** `test_hybrid_processor_v2_integration.py`

---

## ğŸ‘¥ Contribuindo

Ao contribuir para este mÃ³dulo:

1. âœ… Manter logging detalhado em TODAS as funÃ§Ãµes principais
2. âœ… Usar camada de abstraÃ§Ã£o LLM (nÃ£o chamar APIs diretamente)
3. âœ… Validar token budget em novos fragmentadores
4. âœ… Adicionar testes para novos cenÃ¡rios
5. âœ… Documentar decisÃµes tÃ©cnicas no cÃ³digo

---

**VersÃ£o:** 2.0  
**Data:** 2025-01-20  
**Autor:** AI Minds Group
