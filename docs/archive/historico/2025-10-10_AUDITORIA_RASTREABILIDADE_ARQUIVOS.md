# üîç Auditoria T√©cnica: Rastreabilidade de Arquivos CSV entre Frontend e Backend

**Data:** 2025-10-10  
**Sistema:** EDA AI Minds - Sistema Multiagente  
**Objetivo:** Investigar identifica√ß√£o e rastreabilidade de arquivos CSV desde o upload at√© os embeddings

---

## üìã Executive Summary

### Conclus√£o Principal
**‚ùå N√ÉO EXISTE** identificador √∫nico persistido no backend que correlacione com o `id` gerado pelo frontend (`csv_20251010_134412`).

### Situa√ß√£o Atual
O sistema armazena **apenas o path do arquivo** no campo `metadata.source`:
```json
{
  "source": "data\\processando\\creditcard.csv"
}
```

### Impacto
- ‚ùå Imposs√≠vel correlacionar embeddings com arquivos espec√≠ficos enviados pelo frontend
- ‚ùå Sem rastreabilidade entre m√∫ltiplos uploads do mesmo arquivo
- ‚ùå Sem hist√≥rico de vers√µes ou diferentes uploads
- ‚ùå Dificuldade para deletar/atualizar embeddings de um arquivo espec√≠fico

---

## 1Ô∏è‚É£ An√°lise Completa da Cadeia de Ingest√£o

### üîÑ Fluxo de Dados Identificado

```
FRONTEND                    BACKEND                         SUPABASE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê                         ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. Gera ID √∫nico            
   csv_20251010_134412
         ‚îÇ
         ‚îú‚îÄ> Upload via API ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> 2. Recebe arquivo CSV
         ‚îÇ                              ‚îÇ
         ‚îÇ                              ‚îú‚îÄ> GoogleDriveClient.download()
         ‚îÇ                              ‚îÇ
         ‚îÇ                              ‚îú‚îÄ> DataIngestor.ingest_csv()
         ‚îÇ                              ‚îÇ   ou
         ‚îÇ                              ‚îÇ   RAGAgent.ingest_csv_file()
         ‚îÇ                              ‚îÇ
         ‚îÇ                              ‚îî‚îÄ> Chunking + Embeddings
         ‚îÇ                                      ‚îÇ
         ‚îÇ                                      ‚îî‚îÄ> VectorStore.store_embeddings()
         ‚îÇ                                              ‚îÇ
         ‚îî‚îÄ ID PERDIDO! ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> 3. Persiste embeddings
                                                           ‚îî‚îÄ> metadata: {"source": "path"}
                                                               ‚ùå SEM ID DO FRONTEND
```

---

## 2Ô∏è‚É£ An√°lise de Campos e Estruturas de Dados

### 2.1 Schema da Tabela `embeddings` (Supabase)

**Arquivo:** `migrations/0002_schema.sql`

```sql
create table if not exists public.embeddings (
    id uuid primary key default gen_random_uuid(),      -- UUID gerado pelo Supabase
    chunk_text text not null,                           -- Conte√∫do textual do chunk
    embedding vector(1536) not null,                    -- Vetor 1536D (OpenAI dims)
    metadata jsonb default '{}'::jsonb,                 -- Metadados em formato JSON
    created_at timestamp with time zone default now()
);
```

**Campos Dispon√≠veis:**
- ‚úÖ `id` - UUID √∫nico do embedding (gerado pelo Supabase)
- ‚úÖ `chunk_text` - Texto do chunk
- ‚úÖ `embedding` - Vetor de embeddings
- ‚ö†Ô∏è `metadata` - **Campo JSONB onde o ID do frontend DEVE ser armazenado**
- ‚úÖ `created_at` - Timestamp de cria√ß√£o

---

### 2.2 Estrutura do Campo `metadata` (Atual)

#### üìÅ DataIngestor (Simplificado - 2 chunks)

**Arquivo:** `src/agent/data_ingestor.py` (Linha 186)

```python
self.supabase.table('embeddings').insert({
    'chunk_text': chunk,
    'embedding': embedding,
    'metadata': {'source': csv_path}  # ‚ùå APENAS O PATH DO ARQUIVO
}).execute()
```

**Metadata Resultante:**
```json
{
  "source": "data\\processando\\creditcard.csv"
}
```

**‚ùå Problemas:**
- Sem identificador √∫nico do arquivo
- Sem informa√ß√µes sobre upload
- Sem rastreabilidade de vers√µes

---

#### üìä RAGAgent (Completo - 633 chunks)

**Arquivo:** `src/embeddings/vector_store.py` (Linhas 168-179)

```python
metadata = {
    "provider": result.provider.value,           # Ex: "sentence_transformer"
    "model": result.model,                       # Ex: "all-MiniLM-L6-v2"
    "dimensions": result.dimensions,             # Ex: 384
    "raw_dimensions": result.raw_dimensions,     
    "processing_time": result.processing_time,   # Tempo de processamento
    "source_type": source_type,                  # Ex: "csv"
    "created_at": datetime.now().isoformat()     # Timestamp ISO
}

# Adicionar metadados do chunk se dispon√≠veis
if result.chunk_metadata:
    metadata.update(result.chunk_metadata)
```

**Metadados do Chunk** (`ChunkMetadata` - `src/embeddings/chunker.py` linhas 27-38):

```python
@dataclass
class ChunkMetadata:
    """Metadados de um chunk."""
    source: str                      # ‚ùå Apenas o path do arquivo
    chunk_index: int                 # √çndice do chunk
    strategy: ChunkStrategy          # Estrat√©gia de chunking (CSV_ROW, FIXED_SIZE)
    char_count: int                  # Contagem de caracteres
    word_count: int                  # Contagem de palavras
    start_position: int              # Posi√ß√£o inicial no texto
    end_position: int                # Posi√ß√£o final no texto
    overlap_with_previous: int = 0   # Overlap com chunk anterior
    additional_info: Dict[str, Any] = None  # ‚ö†Ô∏è CAMPO EXTENS√çVEL!
```

**Metadata Consolidada (Exemplo Real):**
```json
{
  "provider": "sentence_transformer",
  "model": "all-MiniLM-L6-v2",
  "dimensions": 384,
  "raw_dimensions": 384,
  "processing_time": 0.045,
  "source_type": "csv",
  "created_at": "2025-10-10T12:40:39.176000",
  "source": "data/creditcard.csv",       // ‚ùå Apenas path
  "chunk_index": 0,
  "strategy": "csv_row",
  "char_count": 1024,
  "word_count": 187,
  "start_position": 0,
  "end_position": 1024,
  "overlap_with_previous": 0,
  "additional_info": {                   // ‚ö†Ô∏è CAMPO EXTENS√çVEL!
    "chunk_type": "metadata_types",
    "topic": "data_types_structure"
  }
}
```

**‚úÖ Campo Extens√≠vel Identificado:**
- `additional_info: Dict[str, Any]` - **Pode receber o ID do frontend!**

---

### 2.3 An√°lise do Fluxo `interface_interativa.py`

**Arquivo:** `interface_interativa.py` (Linhas 170-178)

```python
# Gerar session_id √∫nico para esta sess√£o de chat
session_id = str(uuid4())
safe_print(f"üîë Sess√£o iniciada: {session_id[:8]}...\n")

# INTEGRA√á√ÉO: Executar ingest√£o do dataset antes de inicializar orchestrador
safe_print("üßπ Limpando base vetorial e carregando dataset...")
from src.agent.data_ingestor import DataIngestor
ingestor = DataIngestor()
ingestor.ingest_csv('data/creditcard.csv')  # ‚ùå SEM ID DE ARQUIVO
safe_print("‚úÖ Dataset creditcard.csv carregado e base vetorial atualizada!\n")
```

**‚ùå Observa√ß√£o Cr√≠tica:**
- `interface_interativa.py` gera `session_id` (UUID da sess√£o de chat)
- **N√ÉO gera** `file_id` √∫nico para o arquivo CSV
- Chama `ingestor.ingest_csv()` **sem passar identificador**

---

### 2.4 An√°lise do Fluxo `auto_ingest_service.py`

**Arquivo:** `src/services/auto_ingest_service.py` (Linhas 265-280)

```python
# 2. Executa ingest√£o usando DataIngestor
logger.info("  ‚Üí Executando ingest√£o no Supabase (DataIngestor)...")

# Executar ingest√£o (limpa base + analisa + chunking + embeddings)
self.data_ingestor.ingest_csv(str(download_path))  # ‚ùå SEM ID DE ARQUIVO
logger.info("  ‚úÖ Ingest√£o conclu√≠da com sucesso")
```

**‚ùå Observa√ß√£o:**
- Recebe arquivo do Google Drive
- **N√ÉO gera** identificador √∫nico
- Chama `ingest_csv()` apenas com path

---

## 3Ô∏è‚É£ Resposta √†s Quest√µes da Auditoria

### 3.1 Existe l√≥gica de cria√ß√£o de identificador no backend?

**‚ùå N√ÉO.** O backend **n√£o cria** identificador pr√≥prio para arquivos CSV.

**Evid√™ncias:**
1. `DataIngestor.ingest_csv()` recebe apenas `csv_path: str`
2. `RAGAgent.ingest_csv_file()` recebe `file_path` e `source_id`, mas `source_id` n√£o √© usado como ID √∫nico rastre√°vel
3. Nenhum UUID ou hash √© gerado para o arquivo

---

### 3.2 Algum campo armazena identificador correlacion√°vel com o frontend?

**‚ùå N√ÉO.** Nenhum campo atual permite correla√ß√£o direta.

**Campos Analisados:**
- ‚úÖ `embeddings.id` - UUID do embedding (gerado pelo Supabase, n√£o relacionado ao arquivo)
- ‚ùå `embeddings.chunk_text` - Apenas conte√∫do textual
- ‚ö†Ô∏è `embeddings.metadata` - **Campo JSON extens√≠vel, mas atualmente s√≥ tem path**
- ‚ùå `embeddings.created_at` - Timestamp de cria√ß√£o

---

### 3.3 Sugest√£o de Extens√£o: Onde adicionar o ID do frontend?

### ‚úÖ SOLU√á√ÉO RECOMENDADA

**Campo Alvo:** `embeddings.metadata` (JSONB)

**Proposta de Estrutura:**
```json
{
  // ====== IDENTIFICA√á√ÉO DO ARQUIVO (NOVO) ======
  "file_id": "csv_20251010_134412",              // ‚úÖ ID do frontend
  "file_name": "creditcard.csv",                 // ‚úÖ Nome original
  "file_hash": "sha256:abc123...",               // ‚úÖ Hash para deduplica√ß√£o
  "upload_timestamp": "2025-10-10T13:44:12Z",    // ‚úÖ Timestamp do upload
  "upload_source": "google_drive",               // ‚úÖ Origem (drive/local/api)
  
  // ====== METADADOS DE PROCESSAMENTO (EXISTENTES) ======
  "provider": "sentence_transformer",
  "model": "all-MiniLM-L6-v2",
  "dimensions": 384,
  "processing_time": 0.045,
  "source_type": "csv",
  "created_at": "2025-10-10T12:40:39.176000",
  
  // ====== METADADOS DO CHUNK (EXISTENTES) ======
  "source": "data/creditcard.csv",               // Path local
  "chunk_index": 0,
  "strategy": "csv_row",
  "char_count": 1024,
  "word_count": 187,
  "additional_info": {
    "chunk_type": "metadata_types",
    "topic": "data_types_structure"
  }
}
```

---

## 4Ô∏è‚É£ Implementa√ß√£o Recomendada

### 4.1 Onde Gerar o ID?

#### ‚úÖ Op√ß√£o 1: Gerar no Frontend (Recomendado)

**Vantagens:**
- ‚úÖ Frontend j√° gera IDs (`csv_20251010_134412`)
- ‚úÖ ID dispon√≠vel desde o in√≠cio do upload
- ‚úÖ Facilita tracking na UI
- ‚úÖ Permite retry com mesmo ID

**Fluxo:**
```typescript
// Frontend (TypeScript/JavaScript)
const fileId = `csv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

// Upload via API
await uploadCSV({
  file: csvFile,
  fileId: fileId,          // ‚úÖ Envia ID para backend
  fileName: csvFile.name,
  uploadSource: 'web_ui'
});
```

---

#### ‚ö†Ô∏è Op√ß√£o 2: Gerar no Backend

**Vantagens:**
- ‚úÖ Backend controla unicidade
- ‚úÖ Pode usar hash do conte√∫do

**Desvantagens:**
- ‚ùå Frontend n√£o conhece o ID imediatamente
- ‚ùå Requer API para retornar o ID gerado

**Fluxo:**
```python
# Backend (Python)
import hashlib
from datetime import datetime

def generate_file_id(file_path: str, content: bytes = None) -> str:
    """Gera ID √∫nico para arquivo CSV."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if content:
        # Usar hash do conte√∫do para deduplica√ß√£o
        file_hash = hashlib.sha256(content).hexdigest()[:12]
        return f"csv_{timestamp}_{file_hash}"
    else:
        # Fallback para UUID
        import uuid
        return f"csv_{timestamp}_{uuid.uuid4().hex[:12]}"
```

---

### 4.2 Modifica√ß√µes no C√≥digo Backend

#### üìù Modifica√ß√£o 1: `DataIngestor.ingest_csv()`

**Arquivo:** `src/agent/data_ingestor.py`

**Estado Atual (Linha 173-189):**
```python
def ingest_csv(self, csv_path):
    self.clean_vector_db()
    logger.info(f"Processando CSV: {csv_path}")
    md_text = self.analyze_csv(csv_path)
    chunks = self.chunk_text(md_text)
    logger.info(f"Gerando embeddings e inserindo {len(chunks)} chunks...")
    for chunk in chunks:
        embedding = generate_embedding(chunk)
        self.supabase.table('embeddings').insert({
            'chunk_text': chunk,
            'embedding': embedding,
            'metadata': {'source': csv_path}  # ‚ùå PROBLEMA AQUI
        }).execute()
    logger.info("Ingest√£o conclu√≠da com sucesso.")
```

**‚úÖ Proposta de Modifica√ß√£o:**
```python
def ingest_csv(self, 
               csv_path: str, 
               file_id: Optional[str] = None,
               file_name: Optional[str] = None,
               upload_source: str = "local",
               file_hash: Optional[str] = None):
    """Ingesta CSV com rastreabilidade completa.
    
    Args:
        csv_path: Path do arquivo CSV
        file_id: ID √∫nico do frontend (ex: csv_20251010_134412)
        file_name: Nome original do arquivo
        upload_source: Origem do upload (google_drive, web_ui, local)
        file_hash: Hash SHA256 do conte√∫do (opcional)
    """
    self.clean_vector_db()
    logger.info(f"Processando CSV: {csv_path} (ID: {file_id})")
    
    # Gerar file_id se n√£o fornecido
    if not file_id:
        from datetime import datetime
        import uuid
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_id = f"csv_{timestamp}_{uuid.uuid4().hex[:8]}"
        logger.warning(f"file_id n√£o fornecido, gerando: {file_id}")
    
    # Extrair nome do arquivo se n√£o fornecido
    if not file_name:
        from pathlib import Path
        file_name = Path(csv_path).name
    
    # Calcular hash se n√£o fornecido
    if not file_hash:
        import hashlib
        with open(csv_path, 'rb') as f:
            file_hash = f"sha256:{hashlib.sha256(f.read()).hexdigest()[:16]}"
    
    md_text = self.analyze_csv(csv_path)
    chunks = self.chunk_text(md_text)
    
    logger.info(f"Gerando embeddings e inserindo {len(chunks)} chunks...")
    upload_timestamp = datetime.now().isoformat()
    
    for chunk in chunks:
        embedding = generate_embedding(chunk)
        
        # ‚úÖ METADATA ENRIQUECIDA COM RASTREABILIDADE
        metadata = {
            # Identifica√ß√£o do arquivo
            'file_id': file_id,
            'file_name': file_name,
            'file_hash': file_hash,
            'upload_timestamp': upload_timestamp,
            'upload_source': upload_source,
            
            # Path local (mantido para compatibilidade)
            'source': csv_path,
            
            # Tipo de processamento
            'ingestor_type': 'DataIngestor',
            'ingestor_version': '1.0-deprecated'
        }
        
        self.supabase.table('embeddings').insert({
            'chunk_text': chunk,
            'embedding': embedding,
            'metadata': metadata
        }).execute()
    
    logger.info(f"Ingest√£o conclu√≠da: {len(chunks)} chunks com file_id={file_id}")
```

---

#### üìù Modifica√ß√£o 2: `RAGAgent.ingest_csv_file()`

**Arquivo:** `src/agent/rag_agent.py`

**Estado Atual (Linhas 179-216):**
```python
def ingest_csv_file(self,
                   file_path: str,
                   source_id: str,
                   encoding: str = "utf-8") -> Dict[str, Any]:
    """Ingesta arquivo CSV usando estrat√©gia otimizada CSV_ROW."""
    # ... (c√≥digo atual sem file_id)
```

**‚úÖ Proposta de Modifica√ß√£o:**
```python
def ingest_csv_file(self,
                   file_path: str,
                   source_id: str,
                   encoding: str = "utf-8",
                   file_id: Optional[str] = None,
                   file_name: Optional[str] = None,
                   upload_source: str = "local",
                   file_hash: Optional[str] = None) -> Dict[str, Any]:
    """Ingesta arquivo CSV usando estrat√©gia otimizada CSV_ROW com rastreabilidade.
    
    Args:
        file_path: Path do arquivo CSV
        source_id: Identificador da fonte (usado como prefixo)
        encoding: Encoding do arquivo
        file_id: ID √∫nico do frontend (ex: csv_20251010_134412)
        file_name: Nome original do arquivo
        upload_source: Origem do upload (google_drive, web_ui, local)
        file_hash: Hash SHA256 do conte√∫do (opcional)
    """
    # Gerar file_id se n√£o fornecido
    if not file_id:
        from datetime import datetime
        import uuid
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        file_id = f"csv_{timestamp}_{uuid.uuid4().hex[:8]}"
        self.logger.warning(f"file_id n√£o fornecido, gerando: {file_id}")
    
    # Extrair nome do arquivo se n√£o fornecido
    if not file_name:
        from pathlib import Path
        file_name = Path(file_path).name
    
    # Calcular hash se n√£o fornecido
    if not file_hash:
        import hashlib
        with open(file_path, 'rb') as f:
            file_hash = f"sha256:{hashlib.sha256(f.read()).hexdigest()[:16]}"
    
    self.logger.info(f"üîÑ Ingest√£o CSV: {file_path} (file_id={file_id})")
    
    # ... (resto do c√≥digo de ingest√£o)
    
    # Ao criar chunks, adicionar metadados do arquivo
    # Modificar ChunkMetadata.additional_info
    for chunk in chunks:
        if chunk.metadata.additional_info is None:
            chunk.metadata.additional_info = {}
        
        # ‚úÖ ADICIONAR RASTREABILIDADE
        chunk.metadata.additional_info.update({
            'file_id': file_id,
            'file_name': file_name,
            'file_hash': file_hash,
            'upload_timestamp': datetime.now().isoformat(),
            'upload_source': upload_source
        })
    
    # ... (continuar com embedding e armazenamento)
```

---

#### üìù Modifica√ß√£o 3: `auto_ingest_service.py`

**Arquivo:** `src/services/auto_ingest_service.py` (Linhas 265-280)

**Estado Atual:**
```python
# Executar ingest√£o (limpa base + analisa + chunking + embeddings)
self.data_ingestor.ingest_csv(str(download_path))
```

**‚úÖ Proposta de Modifica√ß√£o:**
```python
# Gerar file_id √∫nico para este arquivo
from datetime import datetime
import hashlib

file_name = file['name']
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

# Calcular hash do arquivo para deduplica√ß√£o
with open(download_path, 'rb') as f:
    content_hash = hashlib.sha256(f.read()).hexdigest()[:12]

file_id = f"csv_{timestamp}_{content_hash}"

logger.info(f"  üìå file_id gerado: {file_id}")

# Executar ingest√£o com rastreabilidade
self.data_ingestor.ingest_csv(
    csv_path=str(download_path),
    file_id=file_id,
    file_name=file_name,
    upload_source="google_drive",
    file_hash=f"sha256:{content_hash}"
)
```

---

#### üìù Modifica√ß√£o 4: Endpoint da API (se houver upload direto)

**Arquivo Hipot√©tico:** `src/api/upload_endpoint.py`

```python
from fastapi import APIRouter, UploadFile, File, Form
from typing import Optional

router = APIRouter()

@router.post("/api/upload-csv")
async def upload_csv(
    file: UploadFile = File(...),
    file_id: Optional[str] = Form(None),  # ‚úÖ Recebe ID do frontend
    upload_source: str = Form("web_ui")
):
    """Endpoint para upload de CSV com rastreabilidade."""
    
    # Salvar arquivo temporariamente
    temp_path = f"data/processando/{file.filename}"
    content = await file.read()
    
    with open(temp_path, 'wb') as f:
        f.write(content)
    
    # Gerar file_id se n√£o fornecido pelo frontend
    if not file_id:
        from datetime import datetime
        import hashlib
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        content_hash = hashlib.sha256(content).hexdigest()[:12]
        file_id = f"csv_{timestamp}_{content_hash}"
    
    # Calcular hash
    import hashlib
    file_hash = f"sha256:{hashlib.sha256(content).hexdigest()[:16]}"
    
    # Executar ingest√£o
    from src.agent.data_ingestor import DataIngestor
    ingestor = DataIngestor()
    
    ingestor.ingest_csv(
        csv_path=temp_path,
        file_id=file_id,
        file_name=file.filename,
        upload_source=upload_source,
        file_hash=file_hash
    )
    
    return {
        "status": "success",
        "file_id": file_id,          # ‚úÖ Retorna ID para o frontend
        "file_name": file.filename,
        "file_hash": file_hash,
        "message": "CSV ingerido com sucesso"
    }
```

---

### 4.3 Consultas e Opera√ß√µes com `file_id`

#### üîç Buscar Embeddings por `file_id`

```python
def get_embeddings_by_file_id(file_id: str) -> List[Dict]:
    """Busca todos embeddings de um arquivo espec√≠fico."""
    from src.vectorstore.supabase_client import supabase
    
    response = supabase.table('embeddings') \
        .select('*') \
        .filter('metadata->>file_id', 'eq', file_id) \
        .execute()
    
    return response.data
```

#### üóëÔ∏è Deletar Embeddings por `file_id`

```python
def delete_embeddings_by_file_id(file_id: str) -> int:
    """Deleta todos embeddings de um arquivo espec√≠fico."""
    from src.vectorstore.supabase_client import supabase
    
    # Buscar IDs dos embeddings
    response = supabase.table('embeddings') \
        .select('id') \
        .filter('metadata->>file_id', 'eq', file_id) \
        .execute()
    
    embedding_ids = [row['id'] for row in response.data]
    
    # Deletar em batch
    for embedding_id in embedding_ids:
        supabase.table('embeddings').delete().eq('id', embedding_id).execute()
    
    return len(embedding_ids)
```

#### üìä Listar Arquivos Ingeridos

```python
def list_ingested_files() -> List[Dict]:
    """Lista todos arquivos CSV ingeridos com estat√≠sticas."""
    from src.vectorstore.supabase_client import supabase
    
    # Buscar embeddings √∫nicos por file_id
    response = supabase.table('embeddings') \
        .select('metadata') \
        .execute()
    
    files = {}
    for row in response.data:
        metadata = row['metadata']
        file_id = metadata.get('file_id')
        
        if file_id and file_id not in files:
            files[file_id] = {
                'file_id': file_id,
                'file_name': metadata.get('file_name'),
                'file_hash': metadata.get('file_hash'),
                'upload_timestamp': metadata.get('upload_timestamp'),
                'upload_source': metadata.get('upload_source'),
                'chunk_count': 0
            }
        
        if file_id:
            files[file_id]['chunk_count'] += 1
    
    return list(files.values())
```

---

## 5Ô∏è‚É£ Exemplos Pr√°ticos de Uso

### Exemplo 1: Upload pelo Frontend com ID

```typescript
// Frontend (React/TypeScript)
async function handleCSVUpload(file: File) {
  // Gerar ID √∫nico
  const fileId = `csv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  
  const formData = new FormData();
  formData.append('file', file);
  formData.append('file_id', fileId);
  formData.append('upload_source', 'web_ui');
  
  const response = await fetch('/api/upload-csv', {
    method: 'POST',
    body: formData
  });
  
  const result = await response.json();
  
  console.log('Arquivo ingerido:', result.file_id);
  
  // Salvar file_id no estado da aplica√ß√£o
  setUploadedFiles(prev => [...prev, {
    id: result.file_id,
    name: result.file_name,
    hash: result.file_hash,
    uploadedAt: new Date()
  }]);
}
```

### Exemplo 2: Consulta de Embeddings por Arquivo

```python
# Backend - Consultar embeddings de arquivo espec√≠fico
from src.vectorstore.supabase_client import supabase

file_id = "csv_20251010_134412"

# Buscar todos chunks deste arquivo
response = supabase.table('embeddings') \
    .select('chunk_text, metadata') \
    .filter('metadata->>file_id', 'eq', file_id) \
    .execute()

print(f"Encontrados {len(response.data)} chunks para file_id={file_id}")

for chunk in response.data:
    print(f"Chunk {chunk['metadata']['chunk_index']}: {chunk['chunk_text'][:100]}...")
```

### Exemplo 3: Deletar Arquivo e Seus Embeddings

```python
# Backend - Deletar todos embeddings de um arquivo
from src.vectorstore.supabase_client import supabase

file_id = "csv_20251010_134412"

# Contar embeddings antes
count_before = supabase.table('embeddings') \
    .select('id', count='exact') \
    .filter('metadata->>file_id', 'eq', file_id) \
    .execute()

print(f"Encontrados {count_before.count} embeddings para deletar")

# Deletar (PostgreSQL suporta delete com filtro JSONB)
delete_response = supabase.table('embeddings') \
    .delete() \
    .filter('metadata->>file_id', 'eq', file_id) \
    .execute()

print(f"Deletados com sucesso")
```

---

## 6Ô∏è‚É£ Benef√≠cios da Implementa√ß√£o

### ‚úÖ Rastreabilidade Total
- Correla√ß√£o direta entre frontend e backend
- Hist√≥rico completo de uploads
- Auditoria de processamentos

### ‚úÖ Gest√£o de Arquivos
- Deletar embeddings de arquivos espec√≠ficos
- Atualizar vers√µes sem conflitos
- Detectar duplicatas via hash

### ‚úÖ Debugging e Monitoramento
- Identificar origem de embeddings problem√°ticos
- Logs mais informativos
- M√©tricas por arquivo

### ‚úÖ Features Futuras
- Versionamento de arquivos
- Compara√ß√£o entre vers√µes
- Rollback de ingest√µes

---

## 7Ô∏è‚É£ Checklist de Implementa√ß√£o

### Fase 1: Backend (Obrigat√≥rio)
- [ ] Modificar `DataIngestor.ingest_csv()` para aceitar `file_id`
- [ ] Modificar `RAGAgent.ingest_csv_file()` para aceitar `file_id`
- [ ] Atualizar `auto_ingest_service.py` para gerar `file_id`
- [ ] Criar fun√ß√µes auxiliares (get/delete by file_id)
- [ ] Atualizar testes unit√°rios

### Fase 2: API (se houver upload direto)
- [ ] Criar/atualizar endpoint `/api/upload-csv`
- [ ] Adicionar par√¢metro `file_id` no endpoint
- [ ] Retornar `file_id` na resposta
- [ ] Documentar API (Swagger/OpenAPI)

### Fase 3: Frontend (se houver)
- [ ] Gerar `file_id` antes do upload
- [ ] Enviar `file_id` na requisi√ß√£o
- [ ] Armazenar `file_id` no estado local
- [ ] Implementar listagem de arquivos
- [ ] Implementar delete de arquivos

### Fase 4: Documenta√ß√£o
- [ ] Atualizar README com novo fluxo
- [ ] Documentar estrutura do metadata
- [ ] Criar guia de migra√ß√£o para dados existentes
- [ ] Adicionar exemplos de uso

---

## 8Ô∏è‚É£ Migra√ß√£o de Dados Existentes

### Problema: Embeddings sem `file_id`

Embeddings j√° armazenados **n√£o t√™m** `file_id`. Como migrar?

### Op√ß√£o 1: Inferir file_id a partir do path

```python
from src.vectorstore.supabase_client import supabase
from datetime import datetime
import hashlib

# Buscar todos embeddings sem file_id
response = supabase.table('embeddings') \
    .select('id, metadata') \
    .execute()

for row in response.data:
    metadata = row['metadata']
    
    # Se j√° tem file_id, pular
    if 'file_id' in metadata:
        continue
    
    # Inferir file_id a partir do source path
    source_path = metadata.get('source', '')
    
    if source_path:
        # Gerar file_id baseado no path + timestamp
        path_hash = hashlib.md5(source_path.encode()).hexdigest()[:8]
        file_id = f"csv_migrated_{path_hash}"
        
        # Atualizar metadata
        metadata['file_id'] = file_id
        metadata['file_name'] = source_path.split('/')[-1]
        metadata['upload_source'] = 'migration'
        metadata['migration_timestamp'] = datetime.now().isoformat()
        
        # Atualizar no banco
        supabase.table('embeddings') \
            .update({'metadata': metadata}) \
            .eq('id', row['id']) \
            .execute()
        
        print(f"Migrado: {row['id']} -> {file_id}")
```

### Op√ß√£o 2: Agrupar por created_at

```python
# Agrupar embeddings por created_at pr√≥ximos (mesmo upload)
from datetime import timedelta

response = supabase.table('embeddings') \
    .select('id, created_at, metadata') \
    .order('created_at') \
    .execute()

# Agrupar por janelas de 1 minuto
groups = {}
for row in response.data:
    created_at = datetime.fromisoformat(row['created_at'])
    # Truncar para minuto
    minute_key = created_at.replace(second=0, microsecond=0)
    
    if minute_key not in groups:
        groups[minute_key] = []
    
    groups[minute_key].append(row)

# Atribuir file_id por grupo
for minute_key, rows in groups.items():
    file_id = f"csv_batch_{minute_key.strftime('%Y%m%d_%H%M')}"
    
    for row in rows:
        metadata = row['metadata']
        metadata['file_id'] = file_id
        metadata['upload_source'] = 'migration_batch'
        
        supabase.table('embeddings') \
            .update({'metadata': metadata}) \
            .eq('id', row['id']) \
            .execute()
```

---

## 9Ô∏è‚É£ Conclus√µes e Recomenda√ß√µes

### ‚ùå Situa√ß√£o Atual: Cr√≠tica
- **Sem rastreabilidade** entre frontend e backend
- **Imposs√≠vel** correlacionar embeddings com uploads
- **Dif√≠cil** deletar ou atualizar dados de arquivo espec√≠fico

### ‚úÖ Solu√ß√£o Proposta: Robusta e Escal√°vel

1. **Campo `file_id` no metadata** - Simples e eficaz
2. **Gera√ß√£o no frontend** - Rastreamento desde o in√≠cio
3. **Backend compat√≠vel** - Aceita e propaga o ID
4. **Fun√ß√µes auxiliares** - Get/Delete by file_id

### üéØ Prioridades de Implementa√ß√£o

1. **Alta Prioridade (Imediato)**
   - Modificar `DataIngestor` e `RAGAgent` para aceitar `file_id`
   - Atualizar `auto_ingest_service.py`

2. **M√©dia Prioridade (Curto Prazo)**
   - Criar endpoint de upload com `file_id`
   - Implementar fun√ß√µes de consulta/delete

3. **Baixa Prioridade (Longo Prazo)**
   - Migrar dados existentes
   - Dashboard de arquivos ingeridos

### üìö Refer√™ncias de C√≥digo

- `src/agent/data_ingestor.py` - Ingest√£o simplificada
- `src/agent/rag_agent.py` - Ingest√£o completa
- `src/embeddings/vector_store.py` - Persist√™ncia
- `src/embeddings/chunker.py` - Estrutura de metadados
- `src/services/auto_ingest_service.py` - Auto-ingest√£o
- `migrations/0002_schema.sql` - Schema do banco

---

**Auditoria conclu√≠da em:** 2025-10-10  
**Autor:** GitHub Copilot (GPT-4.1)  
**Status:** ‚úÖ An√°lise Completa com Proposta de Solu√ß√£o
