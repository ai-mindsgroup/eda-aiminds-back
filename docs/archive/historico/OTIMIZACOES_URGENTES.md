# OtimizaÃ§Ãµes Urgentes - HybridQueryProcessorV2

**Data:** 2025-10-21  
**Status:** ğŸ”§ CRÃTICO - Requer aÃ§Ã£o imediata

---

## ğŸš¨ Problemas Identificados

### 1. **Rate Limit GROQ (CRÃTICO)**

**Erro:**
```
Error code: 413 - Request too large for model `llama-3.1-8b-instant`
Limit 6000 TPM, Requested 6582 tokens
```

**Impacto:**
- Sistema falha em 100% das queries com dataset creditcard (284K linhas)
- Primeira query jÃ¡ estourou o limite

**Causa Raiz:**
O sistema estÃ¡ enviando:
1. Query original
2. Todo o DataFrame (284K linhas) como contexto
3. AnÃ¡lise + metadados

Total: **6582 tokens > 6000 TPM**

**SoluÃ§Ã£o Implementada (TEMPORÃRIA):**
```python
# Desabilitado anÃ¡lise LLM adicional para reduzir consumo
# linha 118-126 de hybrid_query_processor_v2.py
```

**SoluÃ§Ã£o Definitiva (RECOMENDADA):**

#### OpÃ§Ã£o A: FragmentaÃ§Ã£o Inteligente de Contexto
```python
def _prepare_context_for_llm(self, df, query, max_tokens=4500):
    """
    Limita contexto para caber no budget GROQ.
    """
    # 1. EstatÃ­sticas resumidas (500 tokens)
    stats = {
        'shape': df.shape,
        'columns': list(df.columns),
        'dtypes': df.dtypes.to_dict(),
        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2
    }
    
    # 2. Amostra estratÃ©gica (2000 tokens)
    if len(df) > 1000:
        # Head + tail + amostra aleatÃ³ria
        sample = pd.concat([
            df.head(50),
            df.sample(min(100, len(df))),
            df.tail(50)
        ])
    else:
        sample = df
    
    # 3. Converter para JSON compacto
    context = {
        'stats': stats,
        'sample': sample.to_dict(orient='records')[:100],  # MÃ¡ximo 100 registros
        'query': query
    }
    
    # 4. Validar tamanho
    context_tokens = len(str(context)) // 4  # Estimativa: 4 chars/token
    if context_tokens > max_tokens:
        # Reduzir amostra
        context['sample'] = context['sample'][:50]
    
    return context
```

#### OpÃ§Ã£o B: Usar Fast Query Fragmenter SEMPRE
```python
# No _process_with_csv_fallback, linha 547:
if csv_shape[0] > 10000 or csv_shape[1] > 20:
    # ForÃ§ar fragmentaÃ§Ã£o para datasets grandes
    return await self._process_with_csv_fragmented(
        query=query,
        source_id=source_id,
        existing_chunks=existing_chunks,
        covered_aspects=covered_aspects
    )
```

#### OpÃ§Ã£o C: Upgrade GROQ Plan
```
Dev Tier:
- 30,000 TPM (5x o limite atual)
- $0.05 per 1M tokens
- Sem rate limits agressivos
```

---

### 2. **Teste RAG ONLY Falha**

**Erro:**
```
âŒ Teste 1 falhou: CSV nÃ£o deve ser acessado em RAG ONLY
```

**Causa:**
- Cobertura 0% (nenhum chunk encontrado)
- Sistema decide por `csv_fallback` corretamente
- **Teste estÃ¡ errado**, nÃ£o o cÃ³digo

**SoluÃ§Ã£o:**
O teste precisa POPULAR chunks antes de rodar:

```python
async def test_rag_only_strategy():
    """RAG ONLY - quando chunks cobrem â‰¥80%"""
    
    # 1. Criar chunks ANTES de testar
    chunks = [
        {
            'content': 'DistribuiÃ§Ã£o Amount: mÃ©dia 88.35, std 250.12...',
            'metadata': {'aspect': 'distribution', 'source': 'creditcard_test123'}
        },
        {
            'content': 'EstatÃ­sticas Amount: min 0, max 25691.16...',
            'metadata': {'aspect': 'statistics', 'source': 'creditcard_test123'}
        },
        {
            'content': 'AnÃ¡lise temporal: 48 horas de transaÃ§Ãµes...',
            'metadata': {'aspect': 'time_analysis', 'source': 'creditcard_test123'}
        }
    ]
    
    # 2. Armazenar chunks
    for chunk in chunks:
        await processor.vector_store.store_embeddings([chunk])
    
    # 3. AGORA testar - deve usar RAG ONLY
    result = await processor.process_query(
        query="Qual a distribuiÃ§Ã£o de valores do dataset?",
        source_id="creditcard_test123"
    )
    
    assert result['strategy'] == 'rag_only', "Deve usar RAG quando hÃ¡ cobertura"
    assert not result['csv_accessed'], "CSV nÃ£o deve ser acessado"
```

---

### 3. **Cache Timezone Issue (CORRIGIDO âœ…)**

**Erro:**
```
can't compare offset-naive and offset-aware datetimes
```

**CorreÃ§Ã£o Aplicada:**
```python
# Linha 226-232
if expires_at and expires_at.tzinfo:
    now = now.replace(tzinfo=expires_at.tzinfo)
```

**Status:** âœ… RESOLVIDO

---

## ğŸ“Š MÃ©tricas Atuais

### Performance dos Testes
```
âœ… CSV FALLBACK: 5.00s (OK)
âœ… CSV FRAGMENTED: 3.57s (OK)
âœ… CACHE: 4.54s primeira exec (OK)
âŒ RAG ONLY: 10.36s + falha (teste incorreto)
```

### Token Usage
```
Query simples:     ~500 tokens âœ…
Query + stats:     ~2000 tokens âœ…
Query + CSV full:  6582 tokens âŒ ESTOURA LIMITE
```

### Taxa de Sucesso
```
75% (3/4) - Limitado pelo problema de rate limit
```

---

## âœ… AÃ§Ãµes PrioritÃ¡rias

### 1. URGENTE (Hoje)
- [ ] Implementar `_prepare_context_for_llm()` com limite 4500 tokens
- [ ] ForÃ§ar fragmentaÃ§Ã£o para df com >10K linhas
- [ ] Corrigir `test_rag_only_strategy` para popular chunks

### 2. ALTA (Esta Semana)
- [ ] Implementar estratÃ©gia adaptativa de sampling baseada em query
- [ ] Adicionar mÃ©tricas de token usage em logs
- [ ] Configurar alertas quando uso > 80% do limite

### 3. MÃ‰DIA (PrÃ³ximo Sprint)
- [ ] Avaliar upgrade para GROQ Dev Tier
- [ ] Implementar cache de estatÃ­sticas do dataset
- [ ] Otimizar serializaÃ§Ã£o JSON para reduzir tokens

---

## ğŸ”§ CÃ³digo de CorreÃ§Ã£o RÃ¡pida

### Patch TemporÃ¡rio (Aplicar AGORA)

```python
# Em hybrid_query_processor_v2.py, linha 547:
async def _process_with_csv_fallback(self, ...):
    # ... cÃ³digo existente ...
    
    # âš ï¸ PROTEÃ‡ÃƒO CONTRA RATE LIMIT
    csv_shape = await self._get_csv_shape(source_id)
    estimated_tokens = (csv_shape[0] * csv_shape[1]) // 100
    
    if estimated_tokens > 4000:  # Limite seguro: 4000 < 6000
        self.logger.warning(f"âš ï¸ CSV muito grande ({estimated_tokens} tokens estimados)")
        self.logger.warning("ğŸ”„ Redirecionando para estratÃ©gia fragmentada")
        
        return await self._process_with_csv_fragmented(
            query=query,
            source_id=source_id,
            existing_chunks=existing_chunks,
            covered_aspects=covered_aspects
        )
    
    # Prosseguir com fallback normal apenas se cabe no budget
    csv_loader = self._load_csv(source_id)
    # ... resto do cÃ³digo ...
```

---

## ğŸ“ˆ Resultados Esperados PÃ³s-CorreÃ§Ã£o

### Token Usage
```
Query simples:     ~500 tokens âœ…
Query + stats:     ~1500 tokens âœ… (reduzido de 2000)
Query + sample:    ~3500 tokens âœ… (reduzido de 6582)
```

### Performance
```
RAG ONLY:          ~2s (com chunks populados)
CSV FALLBACK:      ~5s (dataset <10K linhas)
CSV FRAGMENTED:    ~15s (dataset >10K linhas)
CACHE HIT:         ~0.5s
```

### Taxa de Sucesso Esperada
```
100% (4/4) com correÃ§Ãµes aplicadas
```

---

## ğŸ“š ReferÃªncias

- [GROQ Rate Limits](https://console.groq.com/docs/rate-limits)
- [Token Estimation Best Practices](https://platform.openai.com/tokenizer)
- [FastQueryFragmenter Implementation](src/agent/fast_query_fragmenter.py)
- [HybridQueryProcessorV2 Architecture](docs/HYBRID_PROCESSOR_V2_ARCHITECTURE.md)

---

**Ãšltima atualizaÃ§Ã£o:** 2025-10-21 01:30 BRT  
**ResponsÃ¡vel:** GitHub Copilot (GPT-4.1)  
**Status:** ğŸ”´ CRÃTICO - Aguardando implementaÃ§Ã£o das correÃ§Ãµes
