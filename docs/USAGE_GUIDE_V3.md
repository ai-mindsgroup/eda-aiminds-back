# Guia de Uso R√°pido - Arquitetura V3.0

**Data:** 16 de outubro de 2025  
**Vers√£o:** 3.0.0  

---

## üöÄ IN√çCIO R√ÅPIDO

### Instala√ß√£o

```powershell
# Clonar reposit√≥rio
git clone <repo-url>
cd eda-aiminds-i2a2-rb

# Criar ambiente virtual
python -m venv .venv
.venv\Scripts\Activate.ps1

# Instalar depend√™ncias
pip install -r requirements.txt
```

### Configura√ß√£o

```powershell
# Copiar arquivo de configura√ß√£o
cp configs/.env.example configs/.env

# Editar configs/.env com suas credenciais
```

**Vari√°veis obrigat√≥rias:**
```env
SUPABASE_URL=https://seu-projeto.supabase.co
SUPABASE_KEY=sua-chave-anon
OPENAI_API_KEY=sk-...
GOOGLE_API_KEY=... # opcional
```

---

## üìù EXEMPLOS DE USO

### 1. Classifica√ß√£o de Inten√ß√£o Simples

```python
from langchain_openai import ChatOpenAI
from src.analysis.intent_classifier import IntentClassifier

# Inicializar LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# Criar classificador
classifier = IntentClassifier(llm)

# Classificar query
result = classifier.classify("Qual a dispers√£o dos dados?")

print(f"Inten√ß√£o: {result.primary_intent}")  # STATISTICAL
print(f"Confian√ßa: {result.confidence}")     # 0.90
print(f"Explica√ß√£o: {result.reasoning}")
```

**Output esperado:**
```
Inten√ß√£o: AnalysisIntent.STATISTICAL
Confian√ßa: 0.90
Explica√ß√£o: 'Dispers√£o' √© sin√¥nimo de 'variabilidade', que se refere a m√©tricas como desvio padr√£o, vari√¢ncia e coeficiente de varia√ß√£o.
```

---

### 2. An√°lise Estat√≠stica B√°sica

```python
import pandas as pd
from src.analysis.statistical_analyzer import StatisticalAnalyzer

# Carregar dados
df = pd.read_csv('data/creditcard.csv')

# Criar analyzer
analyzer = StatisticalAnalyzer()

# Executar an√°lise
result = analyzer.analyze(df, columns=['Amount', 'Time'])

# Exibir relat√≥rio
print(result.to_markdown())
```

**Output esperado:**
```markdown
# An√°lise Estat√≠stica Descritiva

## Resumo Geral
| M√©trica | Amount | Time |
|---------|--------|------|
| M√©dia   | 88.35  | 94813.86 |
| Mediana | 22.00  | 84692.00 |
| Moda    | 1.00   | 82800.00 |

## Variabilidade
| M√©trica | Amount | Time |
|---------|--------|------|
| Desvio Padr√£o | 250.12 | 47488.15 |
| Vari√¢ncia     | 62,560 | 2,255,123,456 |
| CV (%)        | 283.0  | 50.1 |
...
```

---

### 3. An√°lise de Frequ√™ncia

```python
from src.analysis.frequency_analyzer import FrequencyAnalyzer

# Criar analyzer
analyzer = FrequencyAnalyzer()

# Executar an√°lise
result = analyzer.analyze(df, columns=['Class'], top_n=5)

# Exibir valores mais frequentes
print("Valores mais frequentes:")
for col, freqs in result.frequency_distributions.items():
    print(f"\n{col}:")
    for value, count in freqs.items():
        print(f"  {value}: {count} ocorr√™ncias")

# Exibir estat√≠sticas de moda
print("\nEstat√≠sticas de Moda:")
print(result.mode_stats)
```

**Output esperado:**
```
Valores mais frequentes:

Class:
  0: 284315 ocorr√™ncias
  1: 492 ocorr√™ncias

Estat√≠sticas de Moda:
{'Class': {'mode_value': 0, 'mode_frequency': 284315, 'mode_percentage': 99.83}}
```

---

### 4. Clustering com KMeans

```python
from src.analysis.clustering_analyzer import ClusteringAnalyzer

# Criar analyzer
analyzer = ClusteringAnalyzer()

# Executar clustering
result = analyzer.analyze(
    df, 
    n_clusters=3, 
    method='kmeans',
    features=['V1', 'V2', 'V3', 'V4', 'V5']
)

# Exibir m√©tricas de qualidade
print(f"Silhouette Score: {result.quality_metrics['silhouette_score']:.3f}")
print(f"Davies-Bouldin Index: {result.quality_metrics['davies_bouldin_index']:.3f}")

# Exibir distribui√ß√£o de clusters
print("\nDistribui√ß√£o de pontos:")
for cluster_id, count in result.cluster_distribution.items():
    print(f"Cluster {cluster_id}: {count} pontos")
```

**Output esperado:**
```
Silhouette Score: 0.723
Davies-Bouldin Index: 0.845

Distribui√ß√£o de pontos:
Cluster 0: 95000 pontos
Cluster 1: 120000 pontos
Cluster 2: 69807 pontos
```

---

### 5. Orquestra√ß√£o Completa (Recomendado)

```python
from langchain_openai import ChatOpenAI
from src.analysis.orchestrator import AnalysisOrchestrator
import pandas as pd

# Inicializar LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# Criar orquestrador
orchestrator = AnalysisOrchestrator(llm)

# Carregar dados
df = pd.read_csv('data/creditcard.csv')

# Executar an√°lise orquestrada
result = orchestrator.orchestrate(
    query="Mostre intervalo E variabilidade dos dados",
    df=df
)

# Exibir relat√≥rio completo
print(result.to_markdown())
```

**Output esperado:**
```markdown
# An√°lise Inteligente de Dados

## Classifica√ß√£o de Inten√ß√£o
- **Inten√ß√£o Principal:** STATISTICAL
- **Confian√ßa:** 0.92
- **Justificativa:** Query solicita 'intervalo' (range/amplitude) e 'variabilidade' (dispers√£o/desvio padr√£o), ambos relacionados a an√°lise estat√≠stica descritiva.

## An√°lises Executadas
1. ‚úÖ An√°lise Estat√≠stica

---

## An√°lise Estat√≠stica Descritiva

### M√©tricas de Variabilidade
| Vari√°vel | Desvio Padr√£o | Vari√¢ncia | CV (%) | IQR |
|----------|---------------|-----------|--------|-----|
| Amount   | 250.12        | 62,560.14 | 283.0  | 42.00 |
| V1       | 1.958         | 3.832     | 415.2  | 3.12 |
...

### M√©tricas de Posi√ß√£o (Intervalo)
| Vari√°vel | M√≠n    | Q1     | Mediana | Q3     | M√°x     | Range |
|----------|--------|--------|---------|--------|---------|-------|
| Amount   | 0.00   | 5.60   | 22.00   | 77.17  | 25,691  | 25,691|
...

## Interpreta√ß√£o Integrada

As vari√°veis apresentam alta variabilidade, especialmente Amount (CV = 283%), indicando grande dispers√£o dos valores em rela√ß√£o √† m√©dia. O intervalo de Amount √© muito amplo (0 a 25,691), com 50% dos dados concentrados entre 5.60 e 77.17 (IQR = 42.00).

Recomenda√ß√µes:
1. Considerar transforma√ß√µes (log) para Amount devido √† alta assimetria
2. Investigar outliers em valores extremos
3. Segmentar an√°lises por faixas de valores
```

---

## üéØ CASOS DE USO COMUNS

### Caso 1: "Qual a m√©dia das transa√ß√µes?"

```python
result = orchestrator.orchestrate("Qual a m√©dia das transa√ß√µes?", df)
```

**Resposta autom√°tica:**
- Classifica como `STATISTICAL`
- Executa `StatisticalAnalyzer`
- Retorna m√©dia de todas vari√°veis num√©ricas
- Destaca m√©trica `mean` no relat√≥rio

---

### Caso 2: "Mostre os valores mais comuns"

```python
result = orchestrator.orchestrate("Mostre os valores mais comuns", df)
```

**Resposta autom√°tica:**
- Classifica como `FREQUENCY`
- Executa `FrequencyAnalyzer`
- Retorna top 10 valores mais frequentes
- Calcula estat√≠sticas de moda

---

### Caso 3: "Agrupe os dados em clusters"

```python
result = orchestrator.orchestrate("Agrupe os dados em clusters", df)
```

**Resposta autom√°tica:**
- Classifica como `CLUSTERING`
- Executa `ClusteringAnalyzer` com KMeans (default)
- Retorna distribui√ß√£o de clusters
- Calcula m√©tricas de qualidade

---

### Caso 4: Query Mista - "Qual a dispers√£o E valores raros?"

```python
result = orchestrator.orchestrate(
    "Qual a dispers√£o E valores raros?", 
    df
)
```

**Resposta autom√°tica:**
- Classifica como `STATISTICAL + FREQUENCY`
- Executa **ambos** analyzers em paralelo
- Combina resultados via LLM
- Retorna relat√≥rio integrado com:
  - M√©tricas de variabilidade (dispers√£o)
  - An√°lise de valores raros (< 1% frequ√™ncia)

---

## üîß CONFIGURA√á√ïES AVAN√áADAS

### Personalizar Classificador de Inten√ß√£o

```python
from src.analysis.intent_classifier import IntentClassifier

classifier = IntentClassifier(
    llm=llm,
    logger=custom_logger  # Opcional
)

# Classificar com contexto adicional
result = classifier.classify(
    query="Mostre tend√™ncias",
    context={
        'available_columns': ['Time', 'Amount', 'Class'],
        'has_temporal_data': True
    }
)
```

---

### Personalizar Analyzer Estat√≠stico

```python
from src.analysis.statistical_analyzer import StatisticalAnalyzer

analyzer = StatisticalAnalyzer(logger=custom_logger)

# Analisar apenas colunas espec√≠ficas
result = analyzer.analyze(
    df=df,
    columns=['Amount', 'Time']  # S√≥ estas colunas
)

# Exportar para dict
data = result.to_dict()

# Exportar para markdown
markdown = result.to_markdown()
```

---

### Personalizar Clustering

```python
from src.analysis.clustering_analyzer import ClusteringAnalyzer

analyzer = ClusteringAnalyzer()

# KMeans com 5 clusters
result = analyzer.analyze(
    df=df,
    n_clusters=5,
    method='kmeans',
    features=['V1', 'V2', 'V3']
)

# DBSCAN com par√¢metros customizados
result = analyzer.analyze(
    df=df,
    method='dbscan',
    eps=0.5,
    min_samples=10,
    features=['Amount', 'Time']
)

# Clustering hier√°rquico
result = analyzer.analyze(
    df=df,
    n_clusters=4,
    method='hierarchical',
    features=df.select_dtypes(include=['number']).columns.tolist()
)
```

---

### Orquestrador com Contexto

```python
from src.analysis.orchestrator import AnalysisOrchestrator

orchestrator = AnalysisOrchestrator(
    llm=llm,
    logger=custom_logger
)

# Orquestrar com contexto adicional
result = orchestrator.orchestrate(
    query="Analise as fraudes",
    df=df,
    context={
        'target_column': 'Class',
        'fraud_class': 1,
        'previous_queries': ['Qual a propor√ß√£o de fraudes?']
    }
)
```

---

## üß™ TESTANDO SUA IMPLEMENTA√á√ÉO

### Teste R√°pido de Classifica√ß√£o

```python
from langchain_openai import ChatOpenAI
from src.analysis.intent_classifier import IntentClassifier

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
classifier = IntentClassifier(llm)

# Teste com sin√¥nimos
queries = [
    "Qual a dispers√£o?",
    "Mostre a variabilidade",
    "Calcule o desvio padr√£o",
    "Qual o spread dos dados?"
]

for query in queries:
    result = classifier.classify(query)
    print(f"{query} ‚Üí {result.primary_intent}")
```

**Resultado esperado:** Todos devem classificar como `STATISTICAL`

---

### Teste de Orquestra√ß√£o End-to-End

```python
import pandas as pd
from langchain_openai import ChatOpenAI
from src.analysis.orchestrator import AnalysisOrchestrator

# Setup
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
orchestrator = AnalysisOrchestrator(llm)
df = pd.DataFrame({
    'A': [1, 2, 3, 4, 5],
    'B': [10, 20, 30, 40, 50]
})

# Executar
result = orchestrator.orchestrate("Qual a m√©dia?", df)

# Validar
assert result.intent_classification.primary_intent.value == 'statistical'
assert 'statistical' in result.execution_order
assert result.combined_results is not None

print("‚úÖ Teste passou!")
```

---

## üìä MONITORAMENTO E LOGGING

### Habilitar Logs Detalhados

```python
import logging
from src.utils.logging_config import get_logger

# Configurar n√≠vel de log
logging.basicConfig(level=logging.DEBUG)

# Usar logger em seus m√≥dulos
logger = get_logger(__name__)

# Logs ser√£o exibidos automaticamente:
# INFO: Classificando inten√ß√£o da query: 'Qual a m√©dia?'
# DEBUG: Inten√ß√£o classificada: STATISTICAL (confidence: 0.92)
# INFO: Executando StatisticalAnalyzer...
# INFO: An√°lise estat√≠stica conclu√≠da
```

---

### Exportar Resultados para Arquivo

```python
# Exportar resultado para Markdown
with open('outputs/relatorio.md', 'w', encoding='utf-8') as f:
    f.write(result.to_markdown())

# Exportar resultado para JSON
import json

with open('outputs/resultado.json', 'w', encoding='utf-8') as f:
    json.dump(result.to_dict(), f, indent=2)
```

---

## ‚ö†Ô∏è TROUBLESHOOTING

### Problema: "LLM n√£o classifica corretamente"

**Solu√ß√£o:**
- Verifique temperatura do LLM (deve ser baixa: 0.1-0.3)
- Confirme que est√° usando modelo poderoso (gpt-4o, gpt-4o-mini, gemini-1.5)
- Adicione contexto na classifica√ß√£o

```python
result = classifier.classify(
    query=query,
    context={'domain': 'fraud detection', 'available_analyses': ['statistical', 'clustering']}
)
```

---

### Problema: "Erro ao calcular m√©tricas estat√≠sticas"

**Solu√ß√£o:**
- Confirme que DataFrame tem colunas num√©ricas
- Verifique valores nulos/infinitos

```python
# Pr√©-processar dados
df_clean = df.select_dtypes(include=['number']).fillna(0)
result = analyzer.analyze(df_clean)
```

---

### Problema: "Clustering retorna apenas 1 cluster"

**Solu√ß√£o:**
- Ajuste par√¢metros do algoritmo
- Normalize/padronize features antes

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = pd.DataFrame(
    scaler.fit_transform(df[features]),
    columns=features
)

result = analyzer.analyze(df_scaled, n_clusters=3, method='kmeans')
```

---

## üìö PR√ìXIMOS PASSOS

1. **Integrar com RAGDataAgent** - usar orquestrador no fluxo principal
2. **Adicionar mais analyzers** - correla√ß√£o, outliers, compara√ß√£o
3. **Visualiza√ß√£o autom√°tica** - gr√°ficos baseados em tipo de an√°lise
4. **Cache de resultados** - evitar recomputa√ß√£o de an√°lises repetidas
5. **Testes automatizados** - garantir qualidade cont√≠nua

---

## üîó REFER√äNCIAS

- **Documenta√ß√£o Completa:** `docs/ARCHITECTURE_V3.md`
- **Diagramas de Fluxo:** `docs/ARCHITECTURE_FLOW.md`
- **Exemplo V3.0:** `examples/rag_data_agent_v3_proposal.py`
- **Auditoria V2.0:** `docs/2025-10-16_relatorio-auditoria-tecnica-refatoracao.md`

---

**Guia criado por:** EDA AI Minds Team  
**√öltima atualiza√ß√£o:** 16 de outubro de 2025  
**Vers√£o m√≠nima Python:** 3.10+  
**Depend√™ncias:** LangChain 0.2+, pandas 2.0+, scikit-learn 1.3+
