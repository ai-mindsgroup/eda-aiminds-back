# ROTEIRO COMPLETO DE CORRE√á√ÉO - SISTEMA EDA AI MINDS

**Data:** 19/10/2025  
**Objetivo:** Corrigir sistema para responder corretamente as 17 perguntas do curso, com foco especial na Pergunta 01 (tipos de dados)  
**Prazo:** 1.5 dias (m√°ximo)  
**Status Atual:** Etapa 01 (80% conclu√≠da) - Etapas 02-05 (pendentes)

---

## üìã CONTEXTO CONSOLIDADO

### **PROBLEMA CENTRAL**

Sistema n√£o responde corretamente "Quais s√£o os tipos de dados (num√©ricos, categ√≥ricos)?" porque:

1. **Embeddings n√£o cont√™m metadados estruturados** do dataset (tipos de colunas, shape, sem√¢ntica)
2. **Chunks cont√™m apenas an√°lises descritivas** (m√©dia, mediana, desvio) - n√£o tipos (int64, float64)
3. **Fluxo de an√°lise decide tipo de an√°lise ANTES de interpretar a pergunta** ‚Üí detecta "Time" como temporal ‚Üí executa TemporalAnalyzer
4. **LLM n√£o processa dados reais do DataFrame** durante fallback ‚Üí respostas gen√©ricas/inventadas

### **O QUE J√Å FOI FEITO (ETAPA 01 - 80%)**

‚úÖ Camada de abstra√ß√£o LLM auditada e documentada  
‚úÖ GROQ funcionando via `LangChainLLMManager`  
‚úÖ `RAGDataAgent._init_langchain_llm()` refatorado (40‚Üí26 linhas, -35%)  
‚úÖ Testes unit√°rios criados e passando  
‚úÖ Documenta√ß√£o t√©cnica completa (1,110 linhas)  
‚ö†Ô∏è **FALTA:** Valida√ß√£o em interface interativa e API (teste manual)

### **ESTRUTURA DE DADOS SUPABASE (AUDITADA)**

**Tabela `embeddings`:**
```sql
- id UUID
- chunk_text TEXT           -- An√°lises descritivas em Markdown
- embedding VECTOR(384)
- metadata JSONB            -- {"source", "chunk_index", "ingestion_id"}
- created_at TIMESTAMP
```

**GAP Cr√≠tico:** Metadados JSONB N√ÉO cont√™m informa√ß√µes estruturadas do dataset (tipos, shape, sem√¢ntica das colunas).

---

## üéØ ARQUITETURA DA SOLU√á√ÉO

### **PRINC√çPIOS FUNDAMENTAIS**

1. ‚úÖ **LLM-First:** LLM deve SEMPRE processar dados reais, nunca respostas hardcoded
2. ‚úÖ **Din√¢mico:** Sem hardcoding de nomes/n√∫meros de colunas - usar `df.dtypes`, `df.shape`
3. ‚úÖ **Intent-Driven:** Classificar inten√ß√£o da pergunta ANTES de decidir tipo de an√°lise
4. ‚úÖ **Metadata-Rich:** Persistir metadados estruturados durante ingest√£o para consultas r√°pidas
5. ‚úÖ **Abstra√ß√£o LLM:** Respeitar camada existente (`LangChainLLMManager`)

### **FLUXO IDEAL DE QUERY**

```
User Query: "Quais s√£o os tipos de dados?"
    ‚Üì
IntentClassifier (LLM-based) ‚Üí intent = "data_types"
    ‚Üì
IF intent IN ["data_types", "structure", "summary"]:
    ‚Üì
    Buscar chunk especial "dataset_metadata" (JSONB estruturado)
    ‚Üì
    IF encontrado:
        ‚Üí Parsear JSON com tipos/shape/sem√¢ntica
        ‚Üí LLM gera resposta humanizada a partir dos metadados
    ‚Üì
    ELSE:
        ‚Üí Fallback: Carregar CSV completo
        ‚Üí Extrair df.dtypes, df.shape, detectar sem√¢ntica
        ‚Üí LLM processa dados reais ‚Üí resposta humanizada
ELSE IF intent == "temporal":
    ‚Üí Verificar se existem colunas temporais
    ‚Üí Executar TemporalAnalyzer
ELSE:
    ‚Üí RAG normal (busca chunks relevantes)
```

---

## üìÖ ROTEIRO DE EXECU√á√ÉO - 5 ETAPAS

---

### **ETAPA 01: VALIDA√á√ÉO DA INFRAESTRUTURA LLM** ‚ö†Ô∏è 80% CONCLU√çDA

**Status:** LLM funcional, mas falta valida√ß√£o nos pontos de entrada reais

#### **Checklist:**

- [x] Auditar camada de abstra√ß√£o LLM (`LangChainLLMManager`)
- [x] Confirmar suporte GROQ (1¬™ prioridade)
- [x] Refatorar `RAGDataAgent._init_langchain_llm()` para usar abstra√ß√£o
- [x] Criar testes unit√°rios (valida√ß√£o isolada do LLM)
- [x] Gerar documenta√ß√£o t√©cnica
- [ ] **PENDENTE:** Testar interface interativa (`setup_and_run_interface_interativa.py`)
- [ ] **PENDENTE:** Testar API (endpoint de query)
- [ ] **PENDENTE:** Documentar logs de execu√ß√£o real

#### **A√ß√µes Imediatas:**

```bash
# 1. Testar interface interativa
python scripts/setup_and_run_interface_interativa.py

# Perguntar: "Quais s√£o os tipos de dados?"
# Verificar log: "‚úÖ LLM inicializado via abstra√ß√£o: GROQ"

# 2. Se API dispon√≠vel, testar endpoint
# POST /query {"query": "Quais s√£o os tipos de dados?"}
```

**Crit√©rio de Sucesso:**
- ‚úÖ Logs confirmam GROQ via abstra√ß√£o em interface e API
- ‚úÖ Nenhum erro de inicializa√ß√£o LLM
- ‚úÖ Sistema responde (mesmo que incorretamente - ser√° corrigido nas pr√≥ximas etapas)

**Tempo Estimado:** 30 minutos

---

### **ETAPA 02: ENRIQUECER PROCESSO DE INGEST√ÉO**

**Objetivo:** Adicionar chunk especial com metadados estruturados do dataset

#### **Checklist:**

- [ ] Localizar m√≥dulo de ingest√£o (`src/ingestion/` ou similar)
- [ ] Adicionar fun√ß√£o `extract_dataset_metadata(df)`:
  ```python
  def extract_dataset_metadata(df):
      return {
          "dataset_name": "creditcard.csv",
          "shape": {"rows": len(df), "cols": len(df.columns)},
          "columns_metadata": {
              col: {
                  "dtype": str(df[col].dtype),
                  "semantic_type": detect_semantic_type(df[col]),
                  "null_count": int(df[col].isnull().sum()),
                  "unique_values": int(df[col].nunique()),
                  "is_categorical_binary": (df[col].nunique() == 2)
              } for col in df.columns
          }
      }
  ```
- [ ] Implementar `detect_semantic_type(series)`:
  ```python
  def detect_semantic_type(series):
      if pd.api.types.is_numeric_dtype(series):
          if series.nunique() == 2:
              return "categorical_binary"
          elif series.name.lower() in ["time", "timestamp", "date"]:
              return "temporal"
          else:
              return "numeric"
      elif pd.api.types.is_object_dtype(series):
          return "categorical"
      else:
          return "other"
  ```
- [ ] Modificar fluxo de chunking para inserir chunk especial (√≠ndice -1 ou 0):
  ```python
  metadata_chunk = {
      "chunk_text": json.dumps(dataset_metadata, ensure_ascii=False, indent=2),
      "metadata": {
          "chunk_type": "dataset_metadata",  # TAG ESPECIAL
          "source": csv_path,
          "created_at": datetime.now().isoformat()
      }
  }
  # Inserir PRIMEIRO (antes dos chunks de an√°lise)
  ```
- [ ] Testar ingest√£o com novo CSV (ex: creditcard_500lines.csv)
- [ ] Validar no Supabase:
  ```sql
  SELECT chunk_text, metadata 
  FROM embeddings 
  WHERE metadata->>'chunk_type' = 'dataset_metadata';
  ```

**Crit√©rio de Sucesso:**
- ‚úÖ Chunk especial criado e persistido
- ‚úÖ Metadados JSONB cont√™m tipos de TODAS as colunas
- ‚úÖ Tag `chunk_type = "dataset_metadata"` presente

**Tempo Estimado:** 2-3 horas

---

### **ETAPA 03: CORRIGIR FLUXO DE INTERPRETA√á√ÉO DE PERGUNTAS**

**Objetivo:** Classificar intent ANTES de decidir tipo de an√°lise

#### **Checklist:**

- [ ] Auditar `IntentClassifier` atual:
  - Localiza√ß√£o: `src/agent/intent_classifier.py` (ou similar)
  - Confirmar que usa LLM (n√£o keywords hardcoded)
- [ ] Se IntentClassifier falha, corrigir:
  ```python
  # Prompt para LLM classificar intent
  prompt = f"""
  Classifique a inten√ß√£o da pergunta do usu√°rio em UMA das categorias:
  - data_types: Pergunta sobre tipos de dados, estrutura, colunas
  - statistical: M√©dia, mediana, desvio padr√£o, distribui√ß√£o
  - temporal: Padr√µes temporais, tend√™ncias, s√©ries temporais
  - correlation: Correla√ß√µes entre vari√°veis
  - outliers: Detec√ß√£o de outliers, valores at√≠picos
  - general: Perguntas gerais sobre o dataset
  
  Pergunta: {user_query}
  
  Responda APENAS com a categoria (ex: "data_types").
  """
  intent = llm.invoke(prompt).strip().lower()
  ```
- [ ] Modificar `RAGDataAgent._analisar_completo_csv()`:
  ```python
  # ANTES (errado)
  temporal_cols = detect_temporal_columns(df)
  if temporal_cols:
      return run_temporal_analysis()  # ‚ùå
  
  # DEPOIS (correto)
  intent = classify_intent(user_query)  # ‚úÖ PRIMEIRO
  
  if intent == "data_types":
      return answer_data_types_question(df, user_query)
  elif intent == "temporal" and has_temporal_columns(df):
      return run_temporal_analysis(df, user_query)
  elif intent == "statistical":
      return run_statistical_analysis(df, user_query)
  # ...
  ```
- [ ] Implementar `answer_data_types_question(df, query)`:
  ```python
  def answer_data_types_question(df, query):
      # Buscar chunk de metadados primeiro
      metadata_chunk = get_metadata_chunk_from_embeddings()
      
      if metadata_chunk:
          dataset_info = json.loads(metadata_chunk["chunk_text"])
      else:
          # Fallback: extrair ao vivo
          dataset_info = extract_dataset_metadata(df)
      
      # LLM processa metadados estruturados
      prompt = f"""
      Informa√ß√µes estruturadas do dataset:
      {json.dumps(dataset_info, indent=2, ensure_ascii=False)}
      
      Pergunta do usu√°rio: {query}
      
      Responda de forma completa, humanizada e precisa, citando TODAS as colunas relevantes.
      """
      return llm.invoke(prompt)
  ```
- [ ] Testar com Pergunta 01:
  ```bash
  python scripts/setup_and_run_interface_interativa.py
  # "Quais s√£o os tipos de dados (num√©ricos, categ√≥ricos)?"
  ```

**Crit√©rio de Sucesso:**
- ‚úÖ Intent classificado corretamente: "data_types"
- ‚úÖ Sistema busca chunk de metadados (n√£o executa TemporalAnalyzer)
- ‚úÖ Resposta lista TODAS as 31 colunas com tipos corretos
- ‚úÖ Coluna "Class" identificada como categ√≥rica bin√°ria

**Tempo Estimado:** 3-4 horas

---

### **ETAPA 04: VALIDA√á√ÉO COMPLETA**

**Objetivo:** Testar as 17 perguntas e corrigir gaps remanescentes

#### **Checklist:**

- [ ] Executar suite de testes:
  ```bash
  python tests/test_17_perguntas_v4.py
  ```
- [ ] Analisar resultados:
  - Quantas passaram? (meta: 15+/17)
  - Quais falharam e por qu√™?
- [ ] Para cada falha, identificar root cause:
  - Intent mal classificado?
  - Metadados insuficientes?
  - LLM gerando resposta incompleta?
  - Falta de gera√ß√£o de gr√°ficos?
- [ ] Corrigir problemas espec√≠ficos:
  - Ajustar prompts de classifica√ß√£o
  - Enriquecer metadados (se necess√°rio)
  - Corrigir gera√ß√£o de visualiza√ß√µes (Pergunta 02)
- [ ] Re-executar testes at√© atingir 90%+ de sucesso

**Crit√©rio de Sucesso:**
- ‚úÖ 15+/17 perguntas passam (88%+)
- ‚úÖ Pergunta 01 passa com 100% de precis√£o
- ‚úÖ Pergunta 02 gera gr√°ficos corretamente
- ‚úÖ Nenhuma resposta cont√©m dados inventados

**Tempo Estimado:** 4-5 horas

---

### **ETAPA 05: INTEGRA√á√ÉO E DEPLOYMENT**

**Objetivo:** Garantir que corre√ß√µes funcionam em todos os pontos de entrada

#### **Checklist:**

- [ ] Validar interface interativa (teste manual completo)
- [ ] Validar API (se dispon√≠vel):
  - Testar endpoint `/query`
  - Verificar CORS, autentica√ß√£o, rate limiting
- [ ] Atualizar documenta√ß√£o:
  - README com instru√ß√µes de uso
  - Documenta√ß√£o da API
  - Guia de troubleshooting
- [ ] Limpar c√≥digo:
  - Remover m√≥dulos tempor√°rios (rag_data_agent_v4.py, test_*.py antigos)
  - Consolidar em c√≥digo principal
- [ ] Preparar para entrega:
  - Relat√≥rio executivo de melhorias
  - Comparativo antes/depois
  - M√©tricas de qualidade

**Crit√©rio de Sucesso:**
- ‚úÖ Interface interativa funciona perfeitamente
- ‚úÖ API (se dispon√≠vel) responde corretamente
- ‚úÖ Documenta√ß√£o atualizada e clara
- ‚úÖ Sistema pronto para avalia√ß√£o do professor

**Tempo Estimado:** 2-3 horas

---

## üìä RESUMO EXECUTIVO - CHECKLIST GERAL

### **ETAPA 01: INFRAESTRUTURA LLM** ‚ö†Ô∏è 80%

- [x] Auditar e documentar camada de abstra√ß√£o
- [x] Refatorar RAGDataAgent para usar abstra√ß√£o
- [x] Criar testes unit√°rios
- [ ] **Testar interface interativa**
- [ ] **Testar API**

### **ETAPA 02: ENRIQUECER INGEST√ÉO** üî¥ PENDENTE

- [ ] Implementar `extract_dataset_metadata()`
- [ ] Implementar `detect_semantic_type()`
- [ ] Adicionar chunk especial de metadados
- [ ] Testar nova ingest√£o

### **ETAPA 03: CORRIGIR FLUXO** üî¥ PENDENTE

- [ ] Auditar IntentClassifier
- [ ] Classificar intent ANTES de an√°lise
- [ ] Implementar `answer_data_types_question()`
- [ ] Testar Pergunta 01

### **ETAPA 04: VALIDA√á√ÉO** üî¥ PENDENTE

- [ ] Executar teste das 17 perguntas
- [ ] Corrigir falhas identificadas
- [ ] Re-executar at√© 90%+ sucesso

### **ETAPA 05: INTEGRA√á√ÉO** üî¥ PENDENTE

- [ ] Validar interface e API
- [ ] Atualizar documenta√ß√£o
- [ ] Limpar c√≥digo tempor√°rio
- [ ] Preparar entrega

---

## ‚è±Ô∏è CRONOGRAMA ESTIMADO (1.5 DIAS)

| Etapa | Tempo | Prioridade | Status |
|-------|-------|------------|--------|
| Etapa 01 (valida√ß√£o final) | 0.5h | üî¥ CR√çTICA | 80% |
| Etapa 02 (ingest√£o) | 3h | üî¥ CR√çTICA | 0% |
| Etapa 03 (fluxo) | 4h | üî¥ CR√çTICA | 0% |
| Etapa 04 (valida√ß√£o) | 5h | üü° ALTA | 0% |
| Etapa 05 (integra√ß√£o) | 3h | üü¢ M√âDIA | 0% |
| **TOTAL** | **15.5h** | **~1.5 dias** | **16%** |

---

## üö® RISCOS E MITIGA√á√ïES

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| IntentClassifier falha constantemente | M√©dia | Alto | Usar classifica√ß√£o por keywords temporariamente |
| Embeddings antigos incompat√≠veis | Baixa | Alto | Reprocessar CSV completo |
| LLM gera respostas incompletas | M√©dia | M√©dio | Ajustar prompts e temperature |
| Prazo insuficiente | Alta | Alto | Priorizar Etapas 02-03, adiar 05 se necess√°rio |

---

## üéØ CRIT√âRIOS DE SUCESSO FINAL

1. ‚úÖ **Pergunta 01 respondida corretamente:** Lista todas as 31 colunas, tipos corretos (int64, float64), identifica "Class" como categ√≥rica bin√°ria
2. ‚úÖ **Pergunta 02 gera gr√°ficos:** Histogramas criados e referenciados na resposta
3. ‚úÖ **15+/17 perguntas passam** no teste automatizado
4. ‚úÖ **Interface interativa funciona** perfeitamente
5. ‚úÖ **Nenhuma resposta cont√©m dados inventados** (10 colunas A1-A10, valores falsos)

---

## üìù PR√ìXIMA A√á√ÉO IMEDIATA

**COMPLETAR ETAPA 01 (30 minutos):**

```bash
# 1. Testar interface interativa
python scripts/setup_and_run_interface_interativa.py

# 2. Fazer pergunta e capturar logs
# Pergunta: "Quais s√£o os tipos de dados?"

# 3. Verificar:
# - Log confirma: "‚úÖ LLM inicializado via abstra√ß√£o: GROQ"
# - Sistema responde (mesmo que incorreto - ser√° corrigido)

# 4. Documentar resultado e prosseguir para Etapa 02
```

**Ap√≥s completar Etapa 01, informar aqui para avan√ßar para Etapa 02.**
