"""Agente Orquestrador Central para coordenar sistema multiagente.

Este agente √© respons√°vel por:
- Receber consultas dos usu√°rios
- Determinar qual(is) agente(s) especializado(s) utilizar
- Coordenar m√∫ltiplos agentes quando necess√°rio
- Combinar respostas de diferentes agentes
- Manter contexto da conversa√ß√£o
- Fornecer interface √∫nica para o sistema completo
"""
from __future__ import annotations
import sys
import os
from pathlib import Path

# Adiciona o diret√≥rio raiz do projeto ao PYTHONPATH
root_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(root_dir))

import re
from typing import Any, Dict, List, Optional, Union, Tuple
from dataclasses import dataclass
from enum import Enum

from src.agent.base_agent import BaseAgent, AgentError
from src.agent.rag_data_agent import RAGDataAgent  # Agente RAG puro sem keywords hardcoded
from src.data.data_processor import DataProcessor

# Import condicional do RAGAgent (pode falhar se Supabase n√£o configurado)
try:
    from src.agent.rag_agent import RAGAgent
    RAG_AGENT_AVAILABLE = True
except ImportError as e:
    RAG_AGENT_AVAILABLE = False
    RAGAgent = None
    print(f"‚ö†Ô∏è RAGAgent n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    RAG_AGENT_AVAILABLE = False  
    RAGAgent = None
    print(f"‚ö†Ô∏è RAGAgent n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do cliente Supabase para verifica√ß√£o de dados
try:
    from src.vectorstore.supabase_client import supabase
    SUPABASE_CLIENT_AVAILABLE = True
except ImportError as e:
    SUPABASE_CLIENT_AVAILABLE = False
    supabase = None
    print(f"‚ö†Ô∏è Cliente Supabase n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    SUPABASE_CLIENT_AVAILABLE = False  
    supabase = None
    print(f"‚ö†Ô∏è Cliente Supabase n√£o dispon√≠vel: {str(e)[:100]}...")

# Import da ferramenta de an√°lise Python
try:
    from src.tools.python_analyzer import python_analyzer
    PYTHON_ANALYZER_AVAILABLE = True
except ImportError as e:
    PYTHON_ANALYZER_AVAILABLE = False
    python_analyzer = None
    print(f"‚ö†Ô∏è Python Analyzer n√£o dispon√≠vel: {str(e)[:100]}...")

# Import dos guardrails de valida√ß√£o
try:
    from src.tools.guardrails import statistics_guardrails
    GUARDRAILS_AVAILABLE = True
except ImportError as e:
    GUARDRAILS_AVAILABLE = False
    statistics_guardrails = None
    print(f"‚ö†Ô∏è Guardrails n√£o dispon√≠vel: {str(e)[:100]}...")# Import do LLM Manager (camada de abstra√ß√£o para m√∫ltiplos provedores)
try:
    from src.llm.manager import get_llm_manager, LLMManager, LLMConfig
    LLM_MANAGER_AVAILABLE = True
except ImportError as e:
    LLM_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è LLM Manager n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    LLM_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è LLM Manager n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do sistema de prompts
try:
    from src.prompts.manager import get_prompt_manager, AgentRole
    PROMPT_MANAGER_AVAILABLE = True
except ImportError as e:
    PROMPT_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è Prompt Manager n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    PROMPT_MANAGER_AVAILABLE = False
    print(f"‚ö†Ô∏è Prompt Manager n√£o dispon√≠vel: {str(e)[:100]}...")

# Import do Roteador Sem√¢ntico para classifica√ß√£o inteligente de inten√ß√µes
try:
    from src.router.semantic_router import SemanticRouter
    SEMANTIC_ROUTER_AVAILABLE = True
except ImportError as e:
    SEMANTIC_ROUTER_AVAILABLE = False
    print(f"‚ö†Ô∏è Semantic Router n√£o dispon√≠vel: {str(e)[:100]}...")
except RuntimeError as e:
    SEMANTIC_ROUTER_AVAILABLE = False
    print(f"‚ö†Ô∏è Semantic Router n√£o dispon√≠vel: {str(e)[:100]}...")


class QueryType(Enum):
    """Tipos de consultas que o orquestrador pode processar."""
    CSV_ANALYSIS = "csv_analysis"      # An√°lise de dados CSV
    RAG_SEARCH = "rag_search"          # Busca sem√¢ntica/contextual
    DATA_LOADING = "data_loading"      # Carregamento de dados
    LLM_ANALYSIS = "llm_analysis"      # An√°lise via LLM (Google Gemini)
    HYBRID = "hybrid"                  # M√∫ltiplos agentes necess√°rios
    GENERAL = "general"                # Consulta geral/conversacional
    UNKNOWN = "unknown"                # Tipo n√£o identificado


@dataclass
class AgentTask:
    """Representa uma tarefa para um agente espec√≠fico."""
    agent_name: str
    query: str
    context: Optional[Dict[str, Any]] = None
    priority: int = 1  # 1=alta, 2=m√©dia, 3=baixa


@dataclass
class OrchestratorResponse:
    """Resposta consolidada do orquestrador."""
    content: str
    query_type: QueryType
    agents_used: List[str]
    metadata: Dict[str, Any]
    success: bool = True
    error: Optional[str] = None


class OrchestratorAgent(BaseAgent):
    """Agente central que coordena todos os agentes especializados."""
    
    def __init__(self, 
                 enable_csv_agent: bool = True,
                 enable_rag_agent: bool = True,
                 enable_llm_manager: bool = True,
                 enable_data_processor: bool = True):
        """Inicializa o orquestrador com agentes especializados.
        
        Args:
            enable_csv_agent: Habilitar agente de an√°lise CSV
            enable_rag_agent: Habilitar agente RAG
            enable_llm_manager: Habilitar LLM Manager (camada de abstra√ß√£o para m√∫ltiplos LLMs)
            enable_data_processor: Habilitar processador de dados
        """
        super().__init__(
            name="orchestrator",
            description="Coordenador central do sistema multiagente de IA para an√°lise de dados",
            enable_memory=True  # Habilita sistema de mem√≥ria
        )
        
        # Inicializar agentes especializados
        self.agents = {}
        # Palavras-chave para detec√ß√£o de visualiza√ß√µes (usado para setar flags)
        self._viz_keywords = {
            'histogram': ['histograma', 'histogram', 'histograms', 'distribui√ß√£o', 'distribuicao', 'distribuicoes', 'distributions'],
            'bar': ['barras', 'bar', 'barplot', 'bar chart', 'gr√°fico', 'grafico'],
            'scatter': ['scatter', 'dispers√£o', 'dispersao', 'scatterplot'],
            'box': ['boxplot', 'box plot', 'box'],
        }
        
        # MIGRA√á√ÉO: conversation_history e current_data_context agora s√£o persistentes
        # Mant√©m compatibilidade tempor√°ria para transi√ß√£o gradual
        self.conversation_history = []  # DEPRECIADO - usar mem√≥ria Supabase
        self.current_data_context = {}  # DEPRECIADO - usar mem√≥ria Supabase
        
        # Inicializar LLM Manager (camada de abstra√ß√£o)
        self.llm_manager = None
        
        # Inicializar Prompt Manager
        self.prompt_manager = None
        if PROMPT_MANAGER_AVAILABLE:
            try:
                self.prompt_manager = get_prompt_manager()
                self.logger.info("‚úÖ Prompt Manager inicializado")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Falha ao inicializar Prompt Manager: {str(e)}")
        
        # Inicializar agentes com tratamento de erro gracioso
        initialization_errors = []
        
        # CSV Agent (sempre dispon√≠vel - sem depend√™ncias externas)
        # ATUALIZADO: Usa RAGDataAgent que implementa busca vetorial pura
        if enable_csv_agent:
            try:
                self.agents["csv"] = RAGDataAgent()
                self.logger.info("‚úÖ Agente RAG Data (CSV) inicializado - busca vetorial pura")
            except Exception as e:
                error_msg = f"RAG Data Agent: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        
        # RAG Agent (requer Supabase configurado)
        if enable_rag_agent and RAG_AGENT_AVAILABLE:
            try:
                self.agents["rag"] = RAGAgent()
                self.logger.info("‚úÖ Agente RAG inicializado")
            except Exception as e:
                error_msg = f"RAG Agent: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        elif enable_rag_agent and not RAG_AGENT_AVAILABLE:
            error_msg = "RAG Agent: Depend√™ncias n√£o dispon√≠veis (Supabase n√£o configurado)"
            initialization_errors.append(error_msg)
            self.logger.warning(f"‚ö†Ô∏è {error_msg}")

        # LLM Manager (camada de abstra√ß√£o para m√∫ltiplos provedores)
        if enable_llm_manager and LLM_MANAGER_AVAILABLE:
            try:
                self.llm_manager = get_llm_manager()
                self.logger.info("‚úÖ LLM Manager inicializado")
                
                # Adicionar informa√ß√µes do provedor ativo
                status = self.llm_manager.get_status()
                active_provider = status.get("active_provider", "unknown")
                self.logger.info(f"ü§ñ Provedor LLM ativo: {active_provider}")
                
            except Exception as e:
                error_msg = f"LLM Manager: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        elif enable_llm_manager and not LLM_MANAGER_AVAILABLE:
            error_msg = "LLM Manager: Depend√™ncias n√£o dispon√≠veis"
            initialization_errors.append(error_msg)
            self.logger.warning(f"‚ö†Ô∏è {error_msg}")
        
        # Data Processor (sempre dispon√≠vel - sem depend√™ncias externas)  
        if enable_data_processor:
            try:
                self.data_processor = DataProcessor(caller_agent='orchestrator_agent')
                self.logger.info("‚úÖ Data Processor inicializado")
            except Exception as e:
                error_msg = f"Data Processor: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
                self.data_processor = None
        else:
            self.data_processor = None
        
        # Semantic Router (para classifica√ß√£o inteligente de inten√ß√µes via embeddings)
        if SEMANTIC_ROUTER_AVAILABLE:
            try:
                self.semantic_router = SemanticRouter()
                self.logger.info("‚úÖ Semantic Router inicializado (classifica√ß√£o via embeddings)")
                # Removido: use_semantic_routing obsoleto
            except Exception as e:
                error_msg = f"Semantic Router: {str(e)}"
                initialization_errors.append(error_msg)
                self.logger.warning(f"‚ö†Ô∏è {error_msg}")
                self.semantic_router = None
                # Removido: use_semantic_routing obsoleto
        else:
            self.semantic_router = None
            # Removido: use_semantic_routing obsoleto
            self.logger.warning("‚ö†Ô∏è Semantic Router n√£o dispon√≠vel, usando roteamento est√°tico")
        
        # Log do resultado da inicializa√ß√£o
        if self.agents or self.data_processor:
            self.logger.info(f"üöÄ Orquestrador inicializado com {len(self.agents)} agentes")
            if initialization_errors:
                self.logger.warning(f"‚ö†Ô∏è {len(initialization_errors)} componentes falharam na inicializa√ß√£o")
        else:
            self.logger.error("‚ùå Nenhum agente foi inicializado com sucesso")
            if initialization_errors:
                raise AgentError(
                    self.name, 
                    f"Falha na inicializa√ß√£o de todos os componentes: {'; '.join(initialization_errors)}"
                )
    
    def _detect_visualization_type(self, query: str) -> Optional[str]:
        """Detecta se a query solicita algum tipo de visualiza√ß√£o.

        Retorna o tipo identificado (ex: 'histogram', 'bar') ou None.
        M√©todo simples baseado em palavras-chave; mant√©m baixo custo e alta
        previsibilidade.
        """
        if not query:
            return None
        q = query.lower()
        for vtype, keywords in self._viz_keywords.items():
            for kw in keywords:
                if kw in q:
                    return vtype
        return None
    
    def _check_embeddings_data_availability(self) -> bool:
        """Verifica se existem dados na tabela embeddings (CONFORMIDADE)."""
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            return False
        
        try:
            result = supabase.table('embeddings').select('id').limit(1).execute()
            has_data = bool(result.data)
            
            if has_data:
                self.logger.info("‚úÖ Dados encontrados na tabela embeddings")
            else:
                self.logger.warning("‚ö†Ô∏è Nenhum dado encontrado na tabela embeddings")
            
            return has_data
        except Exception as e:
            self.logger.error(f"Erro ao verificar dados embeddings: {str(e)}")
            return False
    
    def _ensure_embeddings_compliance(self) -> bool:
        """Garante conformidade com regra embeddings-only.
        
        Returns:
            True se dados est√£o dispon√≠veis via embeddings
        """
        if self._check_embeddings_data_availability():
            return True
        
        self.logger.error("‚ö†Ô∏è VIOLA√á√ÉO DE CONFORMIDADE: Dados n√£o dispon√≠veis via embeddings!")
        self.logger.error("‚ö†Ô∏è Sistema deve funcionar APENAS com dados da tabela embeddings!")
        return False
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta determinando agente(s) apropriado(s).
        
        ‚ö†Ô∏è CONFORMIDADE: Prioriza dados da tabela embeddings.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional (file_path, dados, configura√ß√µes)
        
        Returns:
            Resposta consolidada do sistema
        """
        self.logger.info(f"üéØ Processando consulta: '{query[:50]}...'")
        
        # Verificar conformidade com embeddings-only
        if not self._ensure_embeddings_compliance():
            return {
                'success': False,
                'error': 'Dados n√£o dispon√≠veis via embeddings. Sistema em conformidade apenas com dados indexados.',
                'message': 'Por favor, certifique-se de que os dados foram adequadamente indexados na tabela embeddings.',
                'suggestion': 'Execute o processo de ingest√£o para indexar os dados primeiro.'
            }
        
        try:
            # 1. Adicionar √† hist√≥ria da conversa (compatibilidade)
            self.conversation_history.append({
                "type": "user_query",
                "query": query,
                "timestamp": self._get_timestamp(),
                "context": context
            })
            
            # 2. Analisar tipo da consulta
            query_type = self._classify_query(query, context)
            self.logger.info(f"üìù Tipo de consulta identificado: {query_type.value}")
            
            # 3. Processar baseado no tipo
            if query_type == QueryType.CSV_ANALYSIS:
                result = self._handle_csv_analysis(query, context)
            elif query_type == QueryType.RAG_SEARCH:
                result = self._handle_rag_search(query, context)
            elif query_type == QueryType.DATA_LOADING:
                result = self._handle_data_loading(query, context)
            elif query_type == QueryType.LLM_ANALYSIS:
                result = self._handle_llm_analysis(query, context)
            elif query_type == QueryType.HYBRID:
                result = self._handle_hybrid_query(query, context)
            elif query_type == QueryType.GENERAL:
                result = self._handle_general_query(query, context)
            else:
                result = self._handle_unknown_query(query, context)
            
            # 4. Adicionar √† hist√≥ria
            self.conversation_history.append({
                "type": "system_response",
                "response": result,
                "timestamp": self._get_timestamp()
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no processamento da consulta: {str(e)}",
                metadata={
                    "error": True,
                    "query_type": "error",
                    "agents_used": []
                }
            )
    
    def _check_data_availability(self) -> bool:
        """Verifica se h√° dados dispon√≠veis na base de dados.
        
        Returns:
            True se h√° dados carregados, False caso contr√°rio
        """
        # 1. Verificar contexto em mem√≥ria primeiro (mais r√°pido)
        if self.current_data_context.get("csv_loaded", False):
            self.logger.debug("‚úÖ Dados encontrados no contexto em mem√≥ria")
            return True
        
        # 2. Verificar dados na base de dados Supabase
        if SUPABASE_CLIENT_AVAILABLE and supabase:
            try:
                # Verificar se h√° dados na tabela embeddings
                result = supabase.table('embeddings').select('id').limit(1).execute()
                if result.data and len(result.data) > 0:
                    self.logger.debug("‚úÖ Dados encontrados na tabela embeddings")
                    # Atualizar contexto em mem√≥ria para pr√≥ximas consultas
                    self.current_data_context["csv_loaded"] = True
                    self.current_data_context["data_source"] = "database_embeddings"
                    return True
                
                # Verificar se h√° dados na tabela chunks
                result = supabase.table('chunks').select('id').limit(1).execute()
                if result.data and len(result.data) > 0:
                    self.logger.debug("‚úÖ Dados encontrados na tabela chunks")
                    # Atualizar contexto em mem√≥ria para pr√≥ximas consultas
                    self.current_data_context["csv_loaded"] = True
                    self.current_data_context["data_source"] = "database_chunks"
                    return True
                
                self.logger.debug("‚ùå Nenhum dado encontrado nas tabelas da base de dados")
                return False
                
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Erro ao verificar dados na base: {str(e)}")
                return False
        else:
            self.logger.debug("‚ö†Ô∏è Cliente Supabase n√£o dispon√≠vel")
            return False
    
    def _detect_visualization_need(self, query: str) -> Optional[str]:
        """
        Detecta se a query do usu√°rio requer visualiza√ß√£o gr√°fica.
        
        Args:
            query: Pergunta do usu√°rio
            
        Returns:
            Tipo de gr√°fico necess√°rio ou None
        """
        try:
            from src.tools.graph_generator import detect_visualization_need
            viz_type = detect_visualization_need(query)
            if viz_type:
                self.logger.info(f"üé® Visualiza√ß√£o detectada: {viz_type}")
            return viz_type
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erro ao detectar visualiza√ß√£o: {e}")
            return None
    
    def _retrieve_data_context_from_supabase(self) -> Optional[Dict[str, Any]]:
        """Recupera contexto de dados armazenados no Supabase.
        
        Returns:
            Dicion√°rio com informa√ß√µes sobre os dados ou None se n√£o conseguir recuperar
        """
        if not SUPABASE_CLIENT_AVAILABLE or not supabase:
            return None
            
        try:
            # CORRE√á√ÉO: Recuperar dados da tabela embeddings (n√£o chunks)
            embeddings_result = supabase.table('embeddings').select('chunk_text, metadata').limit(10).execute()
            
            if not embeddings_result.data:
                self.logger.debug("‚ùå Nenhum embedding encontrado para an√°lise")
                return None
            
            # Analisar chunk_text para extrair informa√ß√µes sobre a estrutura dos dados
            total_embeddings = len(embeddings_result.data)
            sample_chunks = []
            columns_found = set()
            dataset_info = {}
            
            for embedding in embeddings_result.data:
                chunk_text = embedding.get('chunk_text', '')
                metadata = embedding.get('metadata', {})
                
                # Coletar amostra dos chunks para an√°lise
                if chunk_text:
                    sample_chunks.append(chunk_text[:200])  # Primeiros 200 caracteres
                
                # Extrair informa√ß√µes gen√©ricas dos chunks sobre dataset
                # Detectar nome do arquivo CSV
                import re
                csv_match = re.search(r'([\w-]+\.csv)', chunk_text)
                if csv_match:
                    dataset_info['dataset_name'] = csv_match.group(1)
                
                # Sistema gen√©rico - sem detec√ß√£o espec√≠fica de tipo
                dataset_info['type'] = 'general'
                
                # Tentar extrair informa√ß√µes de colunas dos chunks
                if 'colunas:' in chunk_text.lower() or 'columns:' in chunk_text.lower():
                    # Procurar por padr√µes de colunas no texto
                    import re
                    col_patterns = re.findall(r'\b[A-Za-z_][A-Za-z0-9_]*\b', chunk_text)
                    for pattern in col_patterns:
                        if len(pattern) > 2 and not pattern.lower() in ['dataset', 'chunk', 'transacoes', 'linhas']:
                            columns_found.add(pattern)
            
            # Construir contexto baseado nos dados encontrados
            context = {
                'csv_loaded': True,
                'data_source': 'database_embeddings',
                'csv_analysis': f"Dados encontrados na base vetorial: {total_embeddings} embeddings dispon√≠veis."
            }
            
            if dataset_info.get('dataset_name'):
                context['file_path'] = dataset_info['dataset_name']
                context['csv_analysis'] += f" Dataset: {dataset_info['dataset_name']}"
                
                # üîß SISTEMA GEN√âRICO: Calcular estat√≠sticas reais para QUALQUER CSV
                if PYTHON_ANALYZER_AVAILABLE and python_analyzer:
                    try:
                        self.logger.info("üî¢ Calculando estat√≠sticas reais com Python Analyzer...")
                        real_stats = python_analyzer.calculate_real_statistics("all")
                        
                        if "error" not in real_stats:
                            # Usar estat√≠sticas reais ao inv√©s de estimativas
                            context['csv_analysis'] += f"\n\nüìä ESTAT√çSTICAS REAIS (do chunk_text parseado):"
                            context['csv_analysis'] += f"\n- Total de registros: {real_stats['total_records']:,}"
                            context['csv_analysis'] += f"\n- Total de colunas: {real_stats['total_columns']}"
                            
                            if 'tipos_dados' in real_stats:
                                tipos = real_stats['tipos_dados']
                                # ‚úÖ INFORMA√á√ÉO ESTRUTURADA GEN√âRICA DAS COLUNAS (funciona com qualquer CSV)
                                context['csv_analysis'] += f"\n\nüìã COLUNAS RECONSTRU√çDAS DA TABELA EMBEDDINGS (chunk_text parseado):"
                                context['csv_analysis'] += f"\n- Colunas totais: {real_stats['total_columns']}"
                                context['csv_analysis'] += f"\n- Lista completa de colunas: {real_stats['columns']}"
                                context['csv_analysis'] += f"\n\nüìä TIPOS DE DADOS (baseado em dtypes reais do DataFrame parseado):"
                                context['csv_analysis'] += f"\n- Num√©ricas ({tipos['total_numericos']}): {tipos['numericos']}"
                                context['csv_analysis'] += f"\n- Categ√≥ricas ({tipos['total_categoricos']}): {tipos['categoricos']}"
                                if tipos.get('datetime'):
                                    context['csv_analysis'] += f"\n- Temporais ({tipos['total_datetime']}): {tipos['datetime']}"
                                
                                context['columns_summary'] = f"Num√©ricos: {', '.join(tipos['numericos'][:5])}{'...' if len(tipos['numericos']) > 5 else ''} ({tipos['total_numericos']} colunas), Categ√≥ricos: {', '.join(tipos['categoricos'])}"
                            
                            # Sistema gen√©rico - estat√≠sticas j√° inclu√≠das em real_stats
                            # Sem l√≥gica espec√≠fica por tipo de dataset
                            
                            self.logger.info("‚úÖ Estat√≠sticas reais calculadas com sucesso")
                        else:
                            self.logger.warning(f"‚ö†Ô∏è Erro no Python Analyzer: {real_stats.get('error')}")
                            # N√£o h√° fallback com colunas hardcoded - sistema deve funcionar genericamente
                    
                    except Exception as e:
                        self.logger.error(f"‚ùå Erro ao calcular estat√≠sticas reais: {str(e)}")
                        # Sem fallback hardcoded - sistema gen√©rico
                        context['csv_analysis'] += "\n\n‚ö†Ô∏è N√£o foi poss√≠vel calcular estat√≠sticas detalhadas"
                else:
                    # Python Analyzer n√£o dispon√≠vel
                    self.logger.warning("‚ö†Ô∏è Python Analyzer n√£o dispon√≠vel")
                    context['csv_analysis'] += "\n\n‚ö†Ô∏è Python Analyzer n√£o configurado"
            
            if columns_found:
                context['csv_analysis'] += f" Colunas identificadas: {', '.join(list(columns_found)[:10])}"
            
            # Tentar recuperar uma amostra dos dados reais usando RAG
            if "rag" in self.agents:
                try:
                    sample_query = "tipos dados colunas num√©ricos categ√≥ricos"  # Query mais espec√≠fica e curta
                    rag_result = self.agents["rag"].process(sample_query, {})
                    if rag_result and not rag_result.get("metadata", {}).get("error", False):
                        # Adicionar informa√ß√µes do RAG ao contexto (LIMITADO)
                        rag_content = rag_result.get("content", "")
                        if rag_content and len(rag_content) > 50:  # Se temos conte√∫do significativo
                            # LIMITA√á√ÉO: Usar apenas os primeiros 300 caracteres para evitar token overflow
                            context['csv_analysis'] += f"\n\nInforma√ß√µes dos dados:\n{rag_content[:300]}..."
                            self.logger.info("‚úÖ Contexto enriquecido com dados do RAG (resumido)")
                except Exception as e:
                    self.logger.debug(f"‚ö†Ô∏è Erro ao recuperar amostra via RAG: {str(e)}")
                    # Sistema gen√©rico - sem informa√ß√µes hardcoded
                    context['csv_analysis'] += "\n\n‚úÖ Dados carregados do banco vetorial"
            
            return context
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao recuperar contexto do Supabase: {str(e)}")
            return None
    
    def _classify_query(self, query: str, context: Optional[Dict[str, Any]]) -> QueryType:
        """Classifica o tipo de consulta usando roteamento sem√¢ntico ou est√°tico.
        
        FLUXO DE DECIS√ÉO:
        1. Se Semantic Router dispon√≠vel: usa classifica√ß√£o via embeddings e consulta vetorial
        2. Fallback: usa matching est√°tico por palavras-chave
        3. Logging: registra decis√£o e rota escolhida
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
        
        Returns:
            Tipo da consulta identificado
        """
        query_lower = query.lower()
        
        # ========================================
        # ETAPA 1: TENTATIVA DE ROTEAMENTO SEM√ÇNTICO
        # ========================================
        # Removido: use_semantic_routing obsoleto
        if self.semantic_router:
            try:
                self.logger.info("üß† Usando roteamento sem√¢ntico via embeddings...")
                
                # Chamar o roteador sem√¢ntico para classificar inten√ß√£o
                routing_result = self.semantic_router.route(query)
                
                # Log da decis√£o do roteador
                self.logger.info(f"üìç Roteamento sem√¢ntico: {routing_result}")
                
                # Mapear categoria sem√¢ntica para QueryType
                route = routing_result.get('route', 'unknown')
                confidence = routing_result.get('confidence', 0.0)
                
                # PONTO DE DECIS√ÉO 1: Verificar se classifica√ß√£o tem confian√ßa suficiente
                if confidence >= 0.7:  # Threshold de confian√ßa
                    self.logger.info(f"‚úÖ Classifica√ß√£o sem√¢ntica com alta confian√ßa ({confidence:.2f})")
                    
                    # Mapear rota sem√¢ntica para QueryType (sistema gen√©rico)
                    route_mapping = {
                        'statistical_analysis': QueryType.CSV_ANALYSIS,
                        # 'fraud_detection' removido - sistema gen√©rico sem rotas espec√≠ficas
                        'data_visualization': QueryType.CSV_ANALYSIS,
                        'contextual_embedding': QueryType.RAG_SEARCH,
                        'data_loading': QueryType.DATA_LOADING,
                        'llm_generic': QueryType.LLM_ANALYSIS,
                        'unknown': None  # Fallback para matching est√°tico
                    }
                    
                    query_type = route_mapping.get(route)
                    
                    if query_type:
                        self.logger.info(f"üéØ Rota sem√¢ntica mapeada: {route} ‚Üí {query_type.value}")
                        return query_type
                    else:
                        self.logger.warning(f"‚ö†Ô∏è Rota sem√¢ntica '{route}' n√£o mapeada, usando fallback")
                else:
                    self.logger.warning(f"‚ö†Ô∏è Confian√ßa baixa ({confidence:.2f}), usando fallback est√°tico")
                    
            except Exception as e:
                self.logger.error(f"‚ùå Erro no roteamento sem√¢ntico: {str(e)}")
                self.logger.info("üîÑ Fallback para roteamento est√°tico")
        
        # ========================================
        # ETAPA 2: FALLBACK - ROTEAMENTO EST√ÅTICO
        # ========================================
        self.logger.info("üìã Usando roteamento est√°tico por palavras-chave...")
        
        # Verificar se √© solicita√ß√£o de visualiza√ß√£o
        viz_type = self._detect_visualization_need(query)
        if viz_type:
            self.logger.info(f"üìä Visualiza√ß√£o detectada: {viz_type}")
            # Adicionar flag ao contexto para processamento posterior
            if context is None:
                context = {}
            context['visualization_requested'] = viz_type
        
        # Palavras-chave para cada tipo de consulta
        csv_keywords = [
            'csv', 'tabela', 'dados', 'an√°lise', 'estat√≠stica', 'correla√ß√£o',
            'gr√°fico', 'plot', 'visualiza√ß√£o', 'resumo', 'describe', 'dataset',
            'colunas', 'linhas', 'm√©dia', 'mediana', 'fraude', 'outlier',
            'tipos de dados', 'num√©ricos', 'categ√≥ricos', 'distribui√ß√£o',
            'intervalo', 'm√≠nimo', 'm√°ximo', 'min', 'max', 'range', 'amplitude',
            'vari√¢ncia', 'desvio', 'percentil', 'quartil', 'valores',
            'vari√°vel', 'vari√°veis', 'features', 'atributos', 'estat√≠sticas',
            'padr√£o', 'padr√µes', 'tend√™ncia', 'tend√™ncias', 'temporal', 'temporais',
            'tempo', 's√©rie', 's√©ries', 'comportamento', 'anomalia', 'an√¥malo',
            'frequente', 'frequentes', 'frequ√™ncia', 'comum', 'raro', 'raros',
            'moda', 'contagem', 'count', 'value_counts', 'top', 'bottom',
            'cluster', 'clusters', 'agrupamento', 'agrupamentos', 'grupos',
            'kmeans', 'k-means', 'dbscan', 'hier√°rquico', 'hierarquico',
            'segmenta√ß√£o', 'segmentacao'
        ]
        
        rag_keywords = [
            'buscar', 'procurar', 'encontrar', 'pesquisar', 'consultar',
            'conhecimento', 'base', 'documento', 'texto', 'similar',
            'contexto', 'embedding', 'sem√¢ntica', 'retrieval'
        ]
        
        data_keywords = [
            'carregar', 'upload', 'importar', 'abrir', 'arquivo',
            'dados sint√©ticos', 'gerar dados', 'criar dados', 'load'
        ]
        
        llm_keywords = [
            'explicar', 'explique', 'interpretar', 'interprete', 'insight', 'insights', 
            'conclus√£o', 'conclus√µes', 'recomenda√ß√£o', 'recomenda√ß√µes', 'recomende',
            'sugest√£o', 'sugest√µes', 'sugira', 'opini√£o', 'an√°lise detalhada', 
            'relat√≥rio', 'sum√°rio', 'resume', 'resumo detalhado', 
            'previs√£o', 'hip√≥tese', 'teoria', 'tire', 'conclua',
            'avalie', 'considere', 'entenda', 'compreenda', 'descoberta',
            'descobrimentos', 'suspeito',
            'detalhado', 'profundo', 'aprofunde', 'discuta', 'comente', 'o que',
            'quais', 'como', 'por que', 'porque'
        ]
        
        general_keywords = [
            'ol√°', 'oi', 'ajuda', 'como', 'o que', 'qual', 'quando',
            'onde', 'por que', 'definir', 'status', 'sistema'
        ]
        
        # Verificar contexto de arquivo
        has_file_context = context and 'file_path' in context
        
        # CORRE√á√ÉO: Verificar se h√° dados carregados no Supabase
        has_supabase_data = self._check_data_availability()
        
        # Classificar baseado em palavras-chave e contexto
        csv_score = sum(1 for kw in csv_keywords if kw in query_lower)
        rag_score = sum(1 for kw in rag_keywords if kw in query_lower)
        data_score = sum(1 for kw in data_keywords if kw in query_lower)
        llm_score = sum(3 for kw in llm_keywords if kw in query_lower)  # Peso triplicado para LLM
        general_score = sum(1 for kw in general_keywords if kw in query_lower)
        
        # PRIORIDADE: Se h√° visualiza√ß√£o detectada, sempre usar CSV_ANALYSIS
        # porque apenas o EmbeddingsAnalysisAgent tem o m√©todo _handle_visualization_query
        if viz_type and has_supabase_data:
            self.logger.info("üé® Redirecionando para CSV analysis (visualiza√ß√£o solicitada)")
            return QueryType.CSV_ANALYSIS

        # CORRE√á√ÉO ABSOLUTA: Se a query cont√©m termos de intervalo, m√≠nimo, m√°ximo, range, amplitude, SEMPRE usar CSV_ANALYSIS
        interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
        if any(term in query_lower for term in interval_terms):
            self.logger.info("üîí For√ßando roteamento para CSV analysis por conter termos de intervalo/m√≠nimo/m√°ximo/range/amplitude")
            return QueryType.CSV_ANALYSIS

        # PRIORIDADE 2: Se h√° dados no Supabase E score CSV alto, usar CSV_ANALYSIS (RAGDataAgent)
        # Isso permite perguntas sobre estat√≠sticas, intervalos, distribui√ß√£o irem para o RAGDataAgent
        if has_supabase_data and csv_score >= 2:
            self.logger.info("üìä Redirecionando para CSV analysis (dados no Supabase + an√°lise estat√≠stica detectada)")
            return QueryType.CSV_ANALYSIS
        
        # Adicionar peso do contexto
        if has_file_context:
            if any(ext in str(context.get('file_path', '')).lower() for ext in ['.csv', '.xlsx', '.json']):
                csv_score += 1
        
        # Verificar se precisa de m√∫ltiplos agentes
        scores = [csv_score, rag_score, data_score, llm_score]
        high_scores = [s for s in scores if s >= 2]
        
        # Se LLM tem score alto, priorizar sobre hybrid
        if llm_score >= 3:
            return QueryType.LLM_ANALYSIS
        
        if len(high_scores) >= 2:
            return QueryType.HYBRID
        
        # Determinar tipo baseado na maior pontua√ß√£o
        max_score = max(csv_score, rag_score, data_score, llm_score, general_score)
        
        if max_score == 0:
            return QueryType.UNKNOWN
        elif max_score == csv_score:
            return QueryType.CSV_ANALYSIS
        elif max_score == rag_score:
            return QueryType.RAG_SEARCH
        elif max_score == data_score:
            return QueryType.DATA_LOADING
        elif max_score == llm_score:
            return QueryType.LLM_ANALYSIS
        else:
            return QueryType.GENERAL
    
    def _handle_csv_analysis(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Delega an√°lise CSV para o agente especializado.
        
        LOGGING: Registra decis√£o de roteamento e agente utilizado.
        """
        if "csv" not in self.agents:
            return self._build_response(
                "‚ùå Agente de an√°lise CSV n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        # Log da decis√£o de delega√ß√£o
        self.logger.info("üìä Delegando para agente CSV (EmbeddingsAnalysisAgent)")
        self.logger.info(f"üîç Query: '{query[:80]}...'")
        
        # Preparar contexto para o agente CSV
        csv_context = context or {}
        
        # Se h√° dados carregados no orquestrador, passar para o agente
        if self.current_data_context:
            csv_context.update(self.current_data_context)
            self.logger.debug(f"üì¶ Contexto de dados atual: {list(self.current_data_context.keys())}")
        
        # Executar processamento no agente especializado (s√≠ncrono)
        try:
            result = self.agents["csv"].process(query, csv_context)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente CSV: {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente CSV: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )
        
        # Log do resultado
        if result.get("metadata", {}).get("error"):
            self.logger.error(f"‚ùå Erro no agente CSV: {result.get('response', 'Erro desconhecido')}")
        else:
            self.logger.info("‚úÖ An√°lise CSV conclu√≠da com sucesso")
        
        # Atualizar contexto se dados foram carregados
        if result.get("metadata") and not result["metadata"].get("error"):
            self.current_data_context.update(result["metadata"])
        
        return self._enhance_response(result, ["embeddings_analyzer"])
    
    def _handle_rag_search(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Delega busca sem√¢ntica para o agente RAG."""
        if "rag" not in self.agents:
            return self._build_response(
                "‚ùå Agente RAG n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("üîç Delegando para agente RAG")
        
        try:
            # Garantir que o contexto inclua ingestion_id/source_id do dataset ativo
            context = context or {}
            # Buscar do contexto atual do orquestrador se dispon√≠vel
            ingestion_id = self.current_data_context.get('ingestion_id')
            source_id = self.current_data_context.get('source_id')
            if ingestion_id:
                context['ingestion_id'] = ingestion_id
            if source_id:
                context['source_id'] = source_id
            result = self.agents["rag"].process(query, context)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente RAG: {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente RAG: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        return self._enhance_response(result, ["rag"])

    # ========================================================================
    # VERS√ÉO ASS√çNCRONA DOS HANDLERS (USADA POR process_with_persistent_memory)
    # ========================================================================
    async def _handle_csv_analysis_async(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Vers√£o async que aguarda agentes ass√≠ncronos quando necess√°rio."""
        if "csv" not in self.agents:
            return self._build_response(
                "‚ùå Agente de an√°lise CSV n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )

        self.logger.info("üìä Delegando para agente CSV (RAGDataAgent) [async]")
        # Para queries de intervalo, limpar contexto para evitar polui√ß√£o por hist√≥rico/mem√≥ria
        interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
        query_lower = query.lower()
        if any(term in query_lower for term in interval_terms):
            csv_context = {}  # contexto limpo, sem mem√≥ria/hist√≥rico
            self.logger.info("üßπ Contexto limpo aplicado para consulta de intervalo/min/max/range/amplitude")
        else:
            csv_context = context or {}
            if self.current_data_context:
                csv_context.update(self.current_data_context)
        
        # ‚úÖ CORRE√á√ÉO: Detectar solicita√ß√£o de visualiza√ß√£o e setar flag no contexto
        viz_type = self._detect_visualization_type(query)
        if viz_type:
            csv_context['visualization_requested'] = True
            csv_context['visualization_type'] = viz_type
            self.logger.info(f"üìä Flag de visualiza√ß√£o setada: {viz_type}")

        try:
            # RAGDataAgent.process() √© async e requer session_id opcional
            session_id = csv_context.get('session_id') or self._current_session_id
            result = await self.agents["csv"].process(query, csv_context, session_id=session_id)
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente CSV (async): {e}", exc_info=True)
            return self._build_response(
                f"‚ùå Erro ao executar agente CSV: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        if result.get("metadata", {}).get("error"):
            self.logger.error(f"‚ùå Erro no agente CSV: {result.get('response', result.get('content', 'Erro desconhecido'))}")
        else:
            self.logger.info("‚úÖ An√°lise CSV conclu√≠da com sucesso [async]")

        if result.get("metadata") and not result["metadata"].get("error"):
            self.current_data_context.update(result["metadata"])

        return self._enhance_response(result, ["rag_data_agent"])

    async def _handle_rag_search_async(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        if "rag" not in self.agents:
            return self._build_response(
                "‚ùå Agente RAG n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )

        self.logger.info("üîç Delegando para agente RAG [async]")
        try:
            result_candidate = self.agents["rag"].process(query, context)
            import inspect
            if inspect.isawaitable(result_candidate):
                result = await result_candidate
            else:
                result = result_candidate
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao executar agente RAG (async): {e}")
            return self._build_response(
                f"‚ùå Erro ao executar agente RAG: {str(e)}",
                metadata={"error": True, "agents_used": []}
            )

        return self._enhance_response(result, ["rag"])

    async def _process_async(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Vers√£o ass√≠ncrona de process() utilizada por process_with_persistent_memory."""
        self.logger.info(f"üéØ [async] Processando consulta: '{query[:50]}...'")

        # Verificar conformidade com embeddings-only
        if not self._ensure_embeddings_compliance():
            return {
                'success': False,
                'error': 'Dados n√£o dispon√≠veis via embeddings. Sistema em conformidade apenas com dados indexados.',
                'message': 'Por favor, certifique-se de que os dados foram adequadamente indexados na tabela embeddings.',
                'suggestion': 'Execute o processo de ingest√£o para indexar os dados primeiro.'
            }

        try:
            # Adicionar √† hist√≥ria compatibilidade
            self.conversation_history.append({
                "type": "user_query",
                "query": query,
                "timestamp": self._get_timestamp(),
                "context": context
            })

            query_type = self._classify_query(query, context)
            self.logger.info(f"üìù [async] Tipo de consulta identificado: {query_type.value}")

            # Processar baseado no tipo (usar vers√µes async quando dispon√≠vel)
            if query_type == QueryType.CSV_ANALYSIS:
                result = await self._handle_csv_analysis_async(query, context)
            elif query_type == QueryType.RAG_SEARCH:
                result = await self._handle_rag_search_async(query, context)
            elif query_type == QueryType.DATA_LOADING:
                result = self._handle_data_loading(query, context)
            elif query_type == QueryType.LLM_ANALYSIS:
                # LLM analysis pode chamar agentes sync/async internamente
                # Reusar implementa√ß√£o s√≠ncrona e permitir que ela chame agentes sync
                result = self._handle_llm_analysis(query, context)
            elif query_type == QueryType.HYBRID:
                result = self._handle_hybrid_query(query, context)
            elif query_type == QueryType.GENERAL:
                result = self._handle_general_query(query, context)
            else:
                result = self._handle_unknown_query(query, context)

            # Adicionar resposta ao hist√≥rico
            self.conversation_history.append({
                "type": "system_response",
                "response": result,
                "timestamp": self._get_timestamp()
            })

            return result

        except Exception as e:
            self.logger.error(f"Erro no processamento async: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no processamento da consulta: {str(e)}",
                metadata={"error": True, "query_type": "error", "agents_used": []}
            )
    
    def _handle_data_loading(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa carregamento de dados."""
        if not self.data_processor:
            return self._build_response(
                "‚ùå Sistema de carregamento de dados n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("üìÅ Processando carregamento de dados")
        
        try:
            # ‚ö†Ô∏è CONFORMIDADE: OrchestratorAgent N√ÉO deve carregar CSV para consultas
            # Este m√©todo deve ser usado apenas para ingest√£o inicial
            self.logger.warning("üö® ATEN√á√ÉO: OrchestratorAgent realizando carregamento de dados!")
            self.logger.warning("üö® Consultas devem usar APENAS a tabela embeddings!")
            
            # Verificar se foi fornecido um arquivo
            if context and 'file_path' in context:
                file_path = context['file_path']
                
                # Carregar dados usando DataProcessor (que deve validar autoriza√ß√£o)
                result = self.data_processor.load_from_file(file_path)
                
                if not result.get('error'):
                    # Armazenar contexto dos dados carregados
                    self.current_data_context = {
                        'file_path': file_path,
                        'data_info': result.get('data_info', {}),
                        'quality_report': result.get('quality_report', {})
                    }
                    
                    # Criar resposta informativa
                    data_info = result.get('data_info', {})
                    quality_report = result.get('quality_report', {})
                    
                    response = f"""‚úÖ **Dados Carregados com Sucesso**

üìÑ **Arquivo:** {file_path}
üìä **Dimens√µes:** {data_info.get('rows', 0):,} linhas √ó {data_info.get('columns', 0)} colunas
‚≠ê **Qualidade:** {quality_report.get('overall_score', 0):.1f}/100

**Pr√≥ximos passos dispon√≠veis:**
‚Ä¢ An√°lise explorat√≥ria: "fa√ßa um resumo dos dados"
‚Ä¢ Correla√ß√µes: "mostre as correla√ß√µes"  
‚Ä¢ Visualiza√ß√µes: "crie gr√°ficos dos dados"
‚Ä¢ Busca sem√¢ntica: "busque informa√ß√µes sobre fraude"
"""
                    
                    return self._build_response(
                        response,
                        metadata={
                            "agents_used": ["data_processor"],
                            "data_loaded": True,
                            "file_path": file_path,
                            "data_info": data_info,
                            "quality_report": quality_report
                        }
                    )
                else:
                    return self._build_response(
                        f"‚ùå Erro ao carregar dados: {result.get('error', 'Erro desconhecido')}",
                        metadata={"error": True, "agents_used": ["data_processor"]}
                    )
            
            else:
                # Instru√ß√µes de como carregar dados
                response = """üìÅ **Como Carregar Dados**

Para carregar dados, use:
```
context = {"file_path": "caminho/para/seu/arquivo.csv"}
```

**Formatos suportados:**
‚Ä¢ CSV (.csv)
‚Ä¢ Excel (.xlsx) - *em desenvolvimento*
‚Ä¢ JSON (.json) - *em desenvolvimento*

**Dados sint√©ticos dispon√≠veis:**
‚Ä¢ Detec√ß√£o de fraude
‚Ä¢ Dados de vendas  
‚Ä¢ Dados de clientes
‚Ä¢ Dados gen√©ricos
"""
                
                return self._build_response(
                    response,
                    metadata={"agents_used": [], "instructions": True}
                )
        
        except Exception as e:
            self.logger.error(f"Erro no carregamento: {str(e)}")
            return self._build_response(
                f"‚ùå Erro no carregamento de dados: {str(e)}",
                metadata={"error": True, "agents_used": ["data_processor"]}
            )

    def _handle_llm_analysis(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas atrav√©s do LLM Manager com verifica√ß√£o de base de dados."""
        if not self.llm_manager:
            return self._build_response(
                "‚ùå LLM Manager n√£o est√° dispon√≠vel",
                metadata={"error": True, "agents_used": []}
            )
        
        self.logger.info("ü§ñ Delegando para LLM Manager")
        
        # 1. VERIFICA√á√ÉO OBRIGAT√ìRIA: Identificar se consulta requer dados espec√≠ficos
        data_specific_keywords = [
            'tipos de dados', 'colunas', 'vari√°veis', 'estat√≠sticas', 'resumo',
            'distribui√ß√£o', 'correla√ß√£o', 'missing', 'nulos', 'formato',
            'csv', 'arquivo', 'dataset', 'base de dados', 'planilha'
        ]
        
        needs_data_analysis = any(keyword in query.lower() for keyword in data_specific_keywords)
        
        # 2. VERIFICAR ESTADO DOS DADOS
        has_loaded_data = self._check_data_availability()
        has_file_context = bool(context and context.get("file_path"))
        
        self.logger.info(f"üìä An√°lise necess√°ria: {needs_data_analysis}, Dados carregados: {has_loaded_data}, Arquivo no contexto: {has_file_context}")
        
        # 3. L√ìGICA DE DECIS√ÉO BASEADA NO ESTADO
        if needs_data_analysis and not has_loaded_data and not has_file_context:
            # Caso 1: Precisa de dados espec√≠ficos mas n√£o h√° nada carregado
            return self._build_response(
                """‚ùì **Base de Dados Necess√°ria**
                
Sua pergunta requer an√°lise de dados espec√≠ficos, mas n√£o h√° nenhuma base de dados carregada no momento.

**Op√ß√µes dispon√≠veis:**

üî∏ **An√°lise espec√≠fica**: Carregue um arquivo CSV primeiro:
   ‚Ä¢ "carregar arquivo dados.csv"
   ‚Ä¢ "analisar arquivo /caminho/para/arquivo.csv"

üî∏ **Resposta gen√©rica**: Se deseja uma explica√ß√£o geral sobre o conceito, reformule sua pergunta:
   ‚Ä¢ "o que s√£o tipos de dados em geral?"
   ‚Ä¢ "explique conceitos b√°sicos de an√°lise de dados"

**Como posso te ajudar?**""",
                metadata={
                    "error": False, 
                    "agents_used": ["llm_manager"],
                    "requires_data": True,
                    "data_available": False
                }
            )
        
        elif needs_data_analysis and not has_loaded_data and has_file_context:
            # Caso 2: Precisa de dados, tem arquivo no contexto, mas n√£o carregou ainda
            self.logger.info("üîÑ Carregando dados automaticamente para an√°lise espec√≠fica...")
            
            # Tentar carregar dados usando agente CSV
            if "csv" in self.agents:
                try:
                    load_query = f"carregar e analisar estrutura b√°sica"
                    csv_result = self.agents["csv"].process(load_query, context)
                    
                    if csv_result and not csv_result.get("metadata", {}).get("error", False):
                        # Extrair informa√ß√µes do CSV e atualizar contexto
                        self._update_data_context_from_csv_result(csv_result, context)
                        self.logger.info("‚úÖ Dados carregados automaticamente")
                    else:
                        return self._build_response(
                            f"‚ùå N√£o foi poss√≠vel carregar o arquivo: {csv_result.get('content', 'Erro desconhecido')}",
                            metadata={"error": True, "agents_used": ["csv"]}
                        )
                except Exception as e:
                    return self._build_response(
                        f"‚ùå Erro ao carregar arquivo: {str(e)}",
                        metadata={"error": True, "agents_used": ["csv"]}
                    )
            else:
                return self._build_response(
                    "‚ùå Agente CSV n√£o dispon√≠vel para carregar dados",
                    metadata={"error": True, "agents_used": []}
                )
        
        # 4. PREPARAR CONTEXTO PARA LLM
        llm_context = context.copy() if context else {}
        
        # Adicionar dados carregados se dispon√≠veis
        if self.current_data_context:
            llm_context.update(self.current_data_context)
        
        # üîÑ REDIRECIONAMENTO PARA RAG: Se precisa de an√°lise de dados e h√° embeddings no Supabase
        if needs_data_analysis and has_loaded_data:
            self.logger.info("üîÑ Redirecionando para LLM analysis (dados no Supabase detectados)")
            
            # Verificar se deve usar RAG para interpreta√ß√£o sem√¢ntica dos chunks
            if "rag" in self.agents:
                try:
                    # Enriquecer contexto com an√°lise sem√¢ntica via RAG
                    self.logger.info("üìö Usando RAG para interpreta√ß√£o sem√¢ntica dos chunks...")
                    rag_result = self.agents["rag"].process(query, {"include_context": True, "max_results": 5})
                    
                    if rag_result and not rag_result.get("metadata", {}).get("error"):
                        # Adicionar contexto RAG ao LLM context
                        llm_context["rag_context"] = rag_result.get("content", "")
                        llm_context["rag_sources"] = rag_result.get("metadata", {}).get("sources", [])
                        self.logger.info("‚úÖ Contexto enriquecido com dados do RAG (resumido)")
                    else:
                        self.logger.warning("‚ö†Ô∏è RAG n√£o retornou resultados, continuando sem contexto RAG")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao usar RAG: {str(e)}, continuando sem contexto RAG")
            
            # NOVA FUNCIONALIDADE: Recuperar dados do Supabase quando necess√°rio
            if not llm_context.get("csv_analysis"):
                self.logger.info("üîç Recuperando dados da base Supabase para an√°lise...")
                try:
                    # Recuperar informa√ß√µes sobre os dados armazenados
                    supabase_data_context = self._retrieve_data_context_from_supabase()
                    if supabase_data_context:
                        llm_context.update(supabase_data_context)
                        self.logger.info("‚úÖ Contexto de dados recuperado do Supabase")
                    else:
                        self.logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel recuperar contexto de dados do Supabase")
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao recuperar dados do Supabase: {str(e)}")
        
        # 5. CONSTRUIR PROMPT CONTEXTUALIZADO
        prompt = self._build_llm_prompt(query, llm_context, needs_data_analysis)
        
        try:
            # 6. CHAMAR LLM MANAGER com configura√ß√£o otimizada
            config = LLMConfig(temperature=0.2, max_tokens=512)  # Reduzir tokens de resposta
            response = self.llm_manager.chat(prompt, config)
            
            if not response.success:
                raise RuntimeError(response.error)
            
            # 7. APLICAR GUARDRAILS DE VALIDA√á√ÉO
            if GUARDRAILS_AVAILABLE and statistics_guardrails and needs_data_analysis:
                validation_result = statistics_guardrails.validate_response(response.content, llm_context)
                
                if not validation_result.is_valid and validation_result.confidence_score < 0.7:
                    self.logger.warning(f"‚ö†Ô∏è Resposta falhol na valida√ß√£o (score: {validation_result.confidence_score:.2f})")
                    self.logger.warning(f"Issues detectados: {', '.join(validation_result.issues[:3])}")
                    
                    # Se h√° valores corrigidos, tentar nova consulta com corre√ß√µes
                    if validation_result.corrected_values and len(validation_result.issues) <= 3:
                        correction_prompt = statistics_guardrails.generate_correction_prompt(validation_result)
                        
                        # Adicionar corre√ß√µes ao contexto
                        corrected_context = llm_context.copy()
                        corrected_context['correction_prompt'] = correction_prompt
                        
                        # Tentar novamente com corre√ß√µes
                        self.logger.info("üîÑ Tentando nova consulta com corre√ß√µes...")
                        corrected_prompt = self._build_llm_prompt(query, corrected_context, needs_data_analysis)
                        
                        try:
                            config = LLMConfig(temperature=0.1, max_tokens=512)  # Temperatura mais baixa para precis√£o
                            corrected_response = self.llm_manager.chat(corrected_prompt, config)
                            
                            if corrected_response.success:
                                response = corrected_response
                                self.logger.info("‚úÖ Resposta corrigida gerada com sucesso")
                        except Exception as e:
                            self.logger.warning(f"‚ö†Ô∏è Falha na corre√ß√£o autom√°tica: {str(e)}")
                
                elif validation_result.confidence_score >= 0.7:
                    self.logger.info(f"‚úÖ Resposta aprovada pelos guardrails (score: {validation_result.confidence_score:.2f})")
            
            # 8. CONSTRUIR RESPOSTA COM METADADOS CORRETOS
            result = {
                "content": response.content,
                "metadata": {
                    "provider": response.provider.value,
                    "model": response.model,
                    "processing_time": response.processing_time,
                    "tokens_used": response.tokens_used,
                    "data_analysis": needs_data_analysis,
                    "data_loaded": bool(self.current_data_context.get("csv_loaded", False))
                }
            }
            
            # 8. REGISTRAR AGENTES USADOS CORRETAMENTE
            agents_used = ["llm_manager"]
            if needs_data_analysis and self.current_data_context.get("csv_loaded"):
                agents_used.append("embeddings_analyzer")  # Agente de an√°lise via embeddings
            
            return self._enhance_response(result, agents_used)
            
        except Exception as e:
            self.logger.error(f"Erro no LLM Manager: {str(e)}")
            return self._build_response(
                f"‚ùå Erro na an√°lise LLM: {str(e)}",
                metadata={"error": True, "agents_used": ["llm_manager"]}
            )
    
    def _handle_hybrid_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas que requerem m√∫ltiplos agentes."""
        self.logger.info("üîÑ Processando consulta h√≠brida (m√∫ltiplos agentes)")
        
        results = []
        agents_used = []
        
        # Determinar quais agentes s√£o necess√°rios
        query_lower = query.lower()
        
        # CSV + RAG (ex: "analise os dados e busque informa√ß√µes similares")
        if any(kw in query_lower for kw in ['dados', 'csv', 'an√°lise']) and \
           any(kw in query_lower for kw in ['buscar', 'similar', 'contexto']):
            
            # Primeiro: an√°lise CSV se h√° dados
            if "csv" in self.agents and self.current_data_context:
                csv_result = self.agents["csv"].process(query, context)
                results.append(("csv", csv_result))
                agents_used.append("embeddings_analyzer")  # Nome correto do agente
            
            # Segundo: busca RAG
            if "rag" in self.agents:
                rag_result = self.agents["rag"].process(query, context)
                results.append(("rag", rag_result))
                agents_used.append("rag")
        
        # Se nenhum resultado, usar abordagem padr√£o
        if not results:
            return self._handle_csv_analysis(query, context)
        
        # Combinar resultados
        combined_response = self._combine_agent_responses(results)
        
        return self._build_response(
            combined_response,
            metadata={"agents_used": agents_used, "hybrid_query": True}
        )
    
    def _handle_general_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas gerais/conversacionais."""
        self.logger.info("üí¨ Processando consulta geral")
        
        query_lower = query.lower()
        
        # Sauda√ß√µes
        if any(greeting in query_lower for greeting in ['ol√°', 'oi', 'ola']):
            response = """üëã **Ol√°! Sou o Orquestrador do Sistema EDA AI Minds**

Sou o coordenador central que pode te ajudar com:

üîç **An√°lise de Dados CSV**
‚Ä¢ Carregamento e valida√ß√£o de arquivos
‚Ä¢ Estat√≠sticas e correla√ß√µes  
‚Ä¢ Visualiza√ß√µes e insights

üß† **Busca Sem√¢ntica (RAG)**
‚Ä¢ Consultas contextualizadas
‚Ä¢ Base de conhecimento vetorial
‚Ä¢ Respostas inteligentes

**Como posso te ajudar hoje?**
"""
            return self._build_response(response, metadata={"agents_used": [], "greeting": True})
        
        # Status do sistema
        elif any(status in query_lower for status in ['status', 'sistema', 'agentes']):
            return self._get_system_status()
        
        # Ajuda
        elif 'ajuda' in query_lower or 'help' in query_lower:
            return self._get_help_response()
        
        # Usar LLM Manager para resposta geral se dispon√≠vel
        elif self.llm_manager:
            try:
                prompt = self._build_llm_prompt(query, context)
                config = LLMConfig(temperature=0.3, max_tokens=512)  # Mais criativo para consultas gerais
                response = self.llm_manager.chat(prompt, config)
                
                if response.success:
                    result = {"content": response.content}
                    return self._enhance_response(result, ["llm_manager"])
                else:
                    raise RuntimeError(response.error)
                    
            except Exception as e:
                self.logger.warning(f"Erro ao usar LLM Manager para consulta geral: {str(e)}")
                # Fallback para resposta padr√£o
                response = "Desculpe, n√£o consegui processar sua consulta com o LLM. Tente ser mais espec√≠fico ou pergunte sobre an√°lise de dados CSV."
                return self._build_response(response, metadata={"agents_used": [], "fallback": True})
        
        # Resposta padr√£o quando LLM n√£o est√° dispon√≠vel
        else:
            response = """üí≠ **Consulta Geral Recebida**

Como n√£o tenho acesso ao LLM no momento, posso te ajudar especificamente com:

üìä **An√°lise de Dados CSV:**
‚Ä¢ "analise o arquivo dados.csv"
‚Ä¢ "mostre correla√ß√µes"
‚Ä¢ "detecte fraudes"

üîç **Carregamento de Dados:**  
‚Ä¢ "carregue o arquivo X"
‚Ä¢ "valide os dados"

**Tente ser mais espec√≠fico sobre dados ou an√°lises!**
"""
            return self._build_response(response, metadata={"agents_used": [], "general_query": True})
    
    def _handle_unknown_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de tipo desconhecido."""
        self.logger.warning(f"ü§î Consulta de tipo desconhecido: {query[:50]}...")
        
        response = f"""ü§î **N√£o consegui identificar o tipo da sua consulta**

**Sua consulta:** "{query}"

**Posso te ajudar com:**
‚Ä¢ üìä **An√°lise de dados:** "analise o arquivo dados.csv"
‚Ä¢ üîç **Busca sem√¢ntica:** "busque informa√ß√µes sobre fraude"
‚Ä¢ üìÅ **Carregar dados:** use context={{"file_path": "arquivo.csv"}}

**Reformule sua pergunta ou seja mais espec√≠fico sobre o que precisa.**
"""
        
        return self._build_response(response, metadata={"agents_used": [], "unknown_query": True})
    
    def _combine_agent_responses(self, results: List[Tuple[str, Dict[str, Any]]]) -> str:
        """Combina respostas de m√∫ltiplos agentes em uma resposta coesa."""
        if not results:
            return "Nenhum resultado dispon√≠vel."
        
        combined = "üîÑ **Resposta Consolidada de M√∫ltiplos Agentes**\n\n"
        
        for agent_name, result in results:
            agent_display = {
                "csv": "üìä **An√°lise CSV**",
                "rag": "üîç **Busca Sem√¢ntica**"
            }.get(agent_name, f"ü§ñ **{agent_name.upper()}**")
            
            combined += f"{agent_display}\n"
            combined += f"{result.get('content', 'Sem conte√∫do')}\n\n"
            combined += "‚îÄ" * 50 + "\n\n"
        
        return combined.rstrip("‚îÄ\n ")
    
    def _enhance_response(self, agent_result: Dict[str, Any], agents_used: List[str]) -> Dict[str, Any]:
        """Melhora resposta do agente com informa√ß√µes do orquestrador."""
        if not agent_result:
            return self._build_response("Erro: resposta vazia do agente", metadata={"error": True})
        
        # Preservar conte√∫do original
        enhanced = agent_result.copy()
        
        # Adicionar informa√ß√µes do orquestrador
        if "metadata" not in enhanced:
            enhanced["metadata"] = {}
        
        # CORRE√á√ÉO: Registrar agentes usados no n√≠vel principal da metadata
        enhanced["metadata"]["agents_used"] = agents_used
        enhanced["metadata"]["orchestrator"] = {
            "conversation_length": len(self.conversation_history),
            "has_data_context": bool(self.current_data_context)
        }
        
        return enhanced
    
    def _get_system_status(self) -> Dict[str, Any]:
        """Retorna status completo do sistema."""
        status_info = {
            "agents": {},
            "data_context": bool(self.current_data_context),
            "conversation_history": len(self.conversation_history)
        }
        
        # Status dos agentes
        for name, agent in self.agents.items():
            status_info["agents"][name] = {
                "available": True,
                "name": agent.name,
                "description": agent.description
            }
        
        # Status do data processor
        if self.data_processor:
            status_info["data_processor"] = {"available": True}
        
        # Informa√ß√µes sobre dados carregados
        data_info = ""
        if self.current_data_context:
            file_path = self.current_data_context.get('file_path', 'N/A')
            data_info = f"\nüìÅ **Dados Carregados:** {file_path}"
        
        response = f"""‚ö° **Status do Sistema EDA AI Minds**

ü§ñ **Agentes Dispon√≠veis:** {len(self.agents)}
{chr(10).join(f'‚Ä¢ {name.upper()}: {agent.description}' for name, agent in self.agents.items())}

üíæ **Data Processor:** {'‚úÖ Ativo' if self.data_processor else '‚ùå Inativo'}
üí¨ **Hist√≥rico:** {len(self.conversation_history)} intera√ß√µes{data_info}

üöÄ **Sistema Operacional e Pronto!**
"""
        
        return self._build_response(response, metadata=status_info)
    
    def _get_help_response(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes de ajuda completas."""
        help_text = """üìö **Guia de Uso do Sistema EDA AI Minds**

## üîç **Tipos de Consulta**

### üìä **An√°lise de Dados CSV**
```python
# Carregar arquivo
context = {"file_path": "dados.csv"}
query = "carregue os dados"

# An√°lises
"fa√ßa um resumo dos dados"
"mostre as correla√ß√µes"
"analise fraudes"
"crie visualiza√ß√µes"
```

### üß† **Busca Sem√¢ntica (RAG)**
```python
"busque informa√ß√µes sobre detec√ß√£o de fraude"
"encontre dados similares a transa√ß√µes suspeitas"
"qual o contexto sobre an√°lise de risco?"
```

### üìÅ **Carregamento de Dados**
```python
"carregar arquivo CSV"
"importar dados"
"gerar dados sint√©ticos"
```

## üí° **Dicas**
‚Ä¢ Seja espec√≠fico nas consultas
‚Ä¢ Use contexto para fornecer arquivos
‚Ä¢ Combine diferentes tipos de an√°lise
‚Ä¢ Pergunte sobre status do sistema

**Exemplo Completo:**
```python
# 1. Carregar dados
context = {"file_path": "fraude.csv"}
"carregue e analise os dados"

# 2. An√°lise espec√≠fica  
"mostre correla√ß√µes entre valor e fraude"

# 3. Busca contextual
"busque padr√µes similares na base de conhecimento"
```
"""
        
        return self._build_response(help_text, metadata={"help": True, "agents_used": []})
    
    # ========================================================================
    # M√âTODOS DE PROCESSAMENTO COM MEM√ìRIA
    # ========================================================================
    
    async def process_with_persistent_memory(self, query: str, context: Optional[Dict[str, Any]] = None,
                                           session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Processa consulta utilizando sistema de mem√≥ria persistente Supabase.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
            session_id: ID da sess√£o (inicializa se None)
            
        Returns:
            Resposta processada com persist√™ncia de mem√≥ria
        """
        self.logger.info(f"üß† Processando com mem√≥ria persistente: '{query[:50]}...'")
        
        try:
            # 1. Inicializar sess√£o de mem√≥ria se necess√°rio
            if session_id and self.has_memory:
                if not self._current_session_id or self._current_session_id != session_id:
                    await self.init_memory_session(session_id)
            elif not self._current_session_id and self.has_memory:
                session_id = await self.init_memory_session()
            
            # 2. Recuperar contexto de mem√≥ria
            memory_context = {}
            if self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(f"Contexto de mem√≥ria recuperado: {len(memory_context.get('recent_conversations', []))} intera√ß√µes")
                
                # Mescla contexto de mem√≥ria com contexto atual
                if context:
                    context.update({"memory_context": memory_context})
                else:
                    context = {"memory_context": memory_context}
            
            # 3. Verificar cache de an√°lises
            analysis_cache_key = None
            if context and context.get('file_path'):
                analysis_cache_key = f"analysis_{hash(query + str(context.get('file_path')))}"
                cached_result = await self.recall_cached_analysis(analysis_cache_key)
                if cached_result:
                    self.logger.info("üì¶ Resultado recuperado do cache de an√°lises")
                    cached_result['metadata']['from_cache'] = True
                    return cached_result
            
            # 4. Processar consulta usando vers√£o ass√≠ncrona (evita coroutines n√£o aguardadas)
            result = await self._process_async(query, context)
            
            # 5. Salvar intera√ß√£o na mem√≥ria persistente
            if self.has_memory and self._current_session_id:
                await self.remember_interaction(
                    query=query,
                    response=result.get('content', str(result)),
                    metadata=result.get('metadata', {})
                )
                
                # 6. Cachear resultado de an√°lise se aplic√°vel
                if analysis_cache_key and result.get('metadata', {}).get('query_type') in ['csv_analysis', 'llm_analysis']:
                    await self.remember_analysis_result(analysis_cache_key, result, expiry_hours=24)
                
                # 7. Salvar contexto de dados se carregado
                if context and context.get('file_path'):
                    data_context = {
                        'file_path': context['file_path'],
                        'last_query': query,
                        'timestamp': self._get_timestamp()
                    }
                    await self.remember_data_context(data_context, "current_data")
            
            # 8. Adicionar informa√ß√µes de mem√≥ria √† resposta
            if self.has_memory:
                result.setdefault('metadata', {})['session_id'] = self._current_session_id
                result.setdefault('metadata', {})['memory_enabled'] = True
                
                # Estat√≠sticas de mem√≥ria
                memory_stats = await self.get_memory_stats()
                result.setdefault('metadata', {})['memory_stats'] = memory_stats
            
            # 9. Garantir compatibilidade do campo 'content' (RAGDataAgent retorna 'response')
            if 'response' in result and 'content' not in result:
                result['content'] = result['response']
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento com mem√≥ria: {e}", exc_info=True)
            # Fallback para processamento sem mem√≥ria sincronamente
            try:
                return self.process(query, context)
            except Exception:
                return self._build_response(f"‚ùå Erro no processamento com mem√≥ria: {str(e)}", metadata={"error": True})
    
    # ========================================================================
    # M√âTODOS DE GEST√ÉO DE MEM√ìRIA PARA COMPATIBILIDADE
    # ========================================================================
    
    def get_conversation_history(self) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico completo da conversa (compatibilidade).
        
        DEPRECIADO: Use get_persistent_conversation_history() para mem√≥ria Supabase.
        """
        return self.conversation_history.copy()
    
    async def get_persistent_conversation_history(self, limit: int = 50) -> List[Dict[str, Any]]:
        """Retorna hist√≥rico de conversa√ß√£o da mem√≥ria persistente."""
        if not self.has_memory or not self._current_session_id:
            return self.get_conversation_history()  # Fallback
        
        try:
            conversations = await self.recall_conversation()
            return conversations[:limit]
        except Exception as e:
            self.logger.error(f"Erro ao recuperar hist√≥rico persistente: {e}")
            return self.get_conversation_history()  # Fallback
    
    def clear_conversation_history(self) -> Dict[str, Any]:
        """Limpa hist√≥rico da conversa (compatibilidade).
        
        DEPRECIADO: Use clear_persistent_memory() para mem√≥ria Supabase.
        """
        count = len(self.conversation_history)
        self.conversation_history.clear()
        self.logger.info(f"Hist√≥rico limpo: {count} intera√ß√µes removidas")
        
        return self._build_response(
            f"‚úÖ Hist√≥rico limpo: {count} intera√ß√µes removidas",
            metadata={"cleared_count": count}
        )
    
    async def clear_persistent_memory(self) -> Dict[str, Any]:
        """Limpa mem√≥ria persistente da sess√£o atual."""
        if not self.has_memory or not self._current_session_id:
            return self.clear_conversation_history()  # Fallback
        
        try:
            # Implementar limpeza via memory manager se necess√°rio
            # Por enquanto, inicia nova sess√£o
            old_session = self._current_session_id
            await self.init_memory_session()
            
            return self._build_response(
                f"‚úÖ Mem√≥ria persistente limpa. Nova sess√£o: {self._current_session_id}",
                metadata={"old_session": old_session, "new_session": self._current_session_id}
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao limpar mem√≥ria persistente: {e}")
            return self.clear_conversation_history()  # Fallback
    
    def clear_data_context(self) -> Dict[str, Any]:
        """Limpa contexto de dados carregados (compatibilidade).
        
        DEPRECIADO: Use clear_persistent_data_context() para mem√≥ria Supabase.
        """
        if self.current_data_context:
            file_path = self.current_data_context.get('file_path', 'N/A')
            self.current_data_context.clear()
            self.logger.info(f"Contexto de dados limpo: {file_path}")
            
            return self._build_response(
                f"‚úÖ Contexto de dados limpo: {file_path}",
                metadata={"cleared_data": file_path}
            )
        else:
            return self._build_response(
                "‚ÑπÔ∏è Nenhum contexto de dados para limpar",
                metadata={"no_data_context": True}
            )
    
    async def clear_persistent_data_context(self) -> Dict[str, Any]:
        """Limpa contexto de dados da mem√≥ria persistente."""
        if not self.has_memory or not self._current_session_id:
            return self.clear_data_context()  # Fallback
        
        try:
            # Aqui implementar√≠amos limpeza espec√≠fica do contexto de dados
            # Por simplicidade, vamos usar o m√©todo de compatibilidade
            result = self.clear_data_context()
            
            # Tamb√©m limpar do sistema de mem√≥ria se houver implementa√ß√£o espec√≠fica
            self.logger.info("Contexto de dados persistente limpo")
            return result
            
        except Exception as e:
            self.logger.error(f"Erro ao limpar contexto de dados persistente: {e}")
            return self.clear_data_context()  # Fallback
    
    def _update_data_context_from_csv_result(self, csv_result: Dict[str, Any], context: Dict[str, Any]) -> None:
        """Atualiza contexto de dados com resultado da an√°lise CSV."""
        try:
            csv_content = csv_result.get("content", "")
            
            # Extrair informa√ß√µes b√°sicas do resultado CSV
            data_info = {
                "file_path": context.get("file_path", ""),
                "csv_loaded": True,
                "structure_analyzed": True,
                "csv_analysis": csv_content
            }
            
            # Tentar extrair informa√ß√µes espec√≠ficas do conte√∫do
            if "Colunas:" in csv_content:
                # Extrair lista de colunas se dispon√≠vel
                lines = csv_content.split('\n')
                for i, line in enumerate(lines):
                    if "Colunas:" in line and i + 1 < len(lines):
                        columns_info = lines[i + 1].strip()
                        data_info["columns_summary"] = columns_info
                        break
            
            if "Shape:" in csv_content:
                # Extrair informa√ß√µes de shape
                lines = csv_content.split('\n')
                for line in lines:
                    if "Shape:" in line:
                        shape_info = line.replace("Shape:", "").strip()
                        data_info["shape"] = shape_info
                        break
            
            # Atualizar contexto global
            self.current_data_context.update(data_info)
            self.logger.info(f"‚úÖ Contexto de dados atualizado: {data_info['file_path']}")
            
        except Exception as e:
            self.logger.error(f"Erro ao atualizar contexto de dados: {e}")

    def get_available_agents(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre agentes dispon√≠veis."""
        agents_info = {}
        
        for name, agent in self.agents.items():
            agents_info[name] = {
                "name": agent.name,
                "description": agent.description,
                "class": agent.__class__.__name__
            }
        
        response = "ü§ñ **Agentes Dispon√≠veis**\n\n"
        for name, info in agents_info.items():
            response += f"‚Ä¢ **{name.upper()}**: {info['description']}\n"
        
        return self._build_response(response, metadata={"agents": agents_info})

    def _build_llm_prompt(self, query: str, context: Optional[Dict[str, Any]] = None, needs_data_analysis: bool = False) -> Tuple[str, Optional[str]]:
        """Constr√≥i prompt contextualizado para o LLM Manager.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional (dados, hist√≥rico, etc.)
            needs_data_analysis: Se a consulta requer an√°lise de dados espec√≠ficos
            
        Returns:
            Tuple[str, Optional[str]]: (user_prompt, system_prompt)
        """
        prompt_parts = []
        
        # Instru√ß√£o base diferenciada
        if needs_data_analysis and context and context.get("csv_loaded"):
            prompt_parts.append("""Voc√™ √© um assistente especializado em an√°lise de dados CSV.
Responda com base ESPECIFICAMENTE nos dados carregados fornecidos no contexto.
Use portugu√™s brasileiro e seja preciso e detalhado sobre os dados reais.""")
        else:
            prompt_parts.append("""Voc√™ √© um assistente de an√°lise de dados especializado em CSV e an√°lise estat√≠stica.
Responda de forma clara, precisa e √∫til. Use portugu√™s brasileiro.""")
        
        # Adicionar contexto de dados se dispon√≠vel
        if context:
            if 'file_path' in context:
                prompt_parts.append(f"\nüìä ARQUIVO CARREGADO: {context['file_path']}")
            
            if 'csv_analysis' in context:
                prompt_parts.append(f"\nüìà AN√ÅLISE DOS DADOS:\n{context['csv_analysis']}")
                
            if 'columns_summary' in context:
                prompt_parts.append(f"\nüìã COLUNAS: {context['columns_summary']}")
                
            if 'shape' in context:
                prompt_parts.append(f"\nÔøΩ DIMENS√ïES: {context['shape']}")
        
        # Adicionar a consulta do usu√°rio
        prompt_parts.append(f"\n‚ùì CONSULTA DO USU√ÅRIO: {query}")
        
        # Instru√ß√£o final diferenciada
        if needs_data_analysis and context and context.get("csv_loaded"):
            # üîÑ CORRE√á√ÉO CR√çTICA: Sempre analisar dados CSV ESTRUTURADOS reconstru√≠dos da tabela embeddings
            prompt_parts.append("""\nüéØ INSTRU√á√ïES CR√çTICAS PARA AN√ÅLISE DE DADOS CSV (da tabela embeddings):

ÔøΩ CONTEXTO RECEBIDO:
- Voc√™ recebeu DADOS ESTRUTURADOS (DataFrame) reconstru√≠dos da coluna chunk_text da tabela embeddings
- Esses dados foram parseados como CSV e representam as COLUNAS ORIGINAIS do arquivo CSV carregado
- As estat√≠sticas fornecidas (dtypes, describe, info) refletem os DADOS REAIS, n√£o a estrutura da tabela embeddings

üîç COMO ANALISAR:
1. EXAMINE as COLUNAS listadas na se√ß√£o "AN√ÅLISE DOS DADOS"
2. IDENTIFIQUE os TIPOS DE DADOS usando dtypes:
   - **Num√©ricos**: float64, int64, float32, int32, etc.
   - **Categ√≥ricos**: object, category, bool
   - **Temporais**: datetime64, timedelta
   - **Texto**: object (sem padr√£o num√©rico)

3. USE as ESTAT√çSTICAS FORNECIDAS:
   - Para distribui√ß√µes: count, mean, std, min, max, quartis
   - Para valores √∫nicos: nunique(), value_counts()
   - Para tipos: dtypes expl√≠citos

‚ö†Ô∏è REGRAS CR√çTICAS:
- Use APENAS os dtypes fornecidos para classificar tipos de dados
- N√ÉO confunda com colunas da tabela embeddings (id, chunk_text, created_at, embedding)
- N√ÉO interprete palavras soltas ou descri√ß√µes textuais como se fossem colunas
- Se o contexto mostra "Colunas: ['Time', 'V1', ..., 'Amount', 'Class']", essas s√£o as colunas REAIS
- Seja PRECISO: liste EXATAMENTE as colunas fornecidas, com seus tipos REAIS
- Se a informa√ß√£o n√£o est√° no contexto estruturado, diga que n√£o tem acesso a ela""")
        else:
            prompt_parts.append("\nüéØ Forne√ßa uma resposta √∫til e estruturada:")
        
        # Adicionar corre√ß√µes se dispon√≠veis
        if context and 'correction_prompt' in context:
            prompt_parts.append(f"\n{context['correction_prompt']}")
            prompt_parts.append("\nRefa√ßa sua resposta com os valores corretos fornecidos acima.")
        
        return "\n".join(prompt_parts)