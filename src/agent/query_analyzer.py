"""
Query Analyzer - Classificador Inteligente de Complexidade de Perguntas via LLM

Analisa perguntas do usu√°rio usando LLMs para determinar a estrat√©gia de resposta:
- SIMPLE: Usa apenas chunks anal√≠ticos existentes (6 chunks de metadata)
- COMPLEX: Requer acesso ao CSV completo + gera√ß√£o de chunks adicionais

‚ú® REFATORADO: Usa an√°lise sem√¢ntica via LLM ao inv√©s de listas est√°ticas de keywords
‚úÖ FIXED: Retorna objetos QueryAnalysis em vez de dict para evitar AttributeError
"""

from typing import Dict, Literal, Optional, List
from enum import Enum
from dataclasses import dataclass, field, asdict
import json
from src.utils.logging_config import get_logger
from src.llm.manager import get_llm_manager, LLMConfig

logger = get_logger(__name__)


@dataclass
class QueryStrategy:
    """
    Estrat√©gia de execu√ß√£o para uma query.
    
    ‚úÖ COMPATIBILIDADE: Suporta acesso tanto por atributo quanto por dict.
    """
    action: str
    chunks_to_query: List[str] = field(default_factory=list)
    fallback_to_csv: bool = False
    generate_new_chunks: bool = False
    use_chunks_as_guide: bool = False
    csv_operations: List[str] = field(default_factory=list)
    avoid_duplicate_analysis: bool = False
    requires_row_level_data: bool = False
    
    def __getitem__(self, key: str):
        """Permite acesso estilo dict: strategy['action']"""
        return getattr(self, key)
    
    def __setitem__(self, key: str, value):
        """Permite atribui√ß√£o estilo dict: strategy['action'] = 'rag'"""
        setattr(self, key, value)
    
    def get(self, key: str, default=None):
        """Emula dict.get(): strategy.get('action', 'default')"""
        return getattr(self, key, default)
    
    def to_dict(self) -> Dict:
        """Converte para dict para compatibilidade legada"""
        return asdict(self)


@dataclass
class QueryAnalysis:
    """
    Resultado da an√°lise de uma query.
    
    ‚úÖ TIPADO: Objeto estruturado em vez de dict solto.
    Evita erros como: 'dict' object has no attribute 'category'
    
    ‚úÖ COMPATIBILIDADE: Suporta acesso tanto por atributo quanto por dict:
       - analysis.category  ‚Üê Novo (recomendado)
       - analysis['category']  ‚Üê Legado (mant√©m compatibilidade)
    """
    query: str
    complexity: str  # 'simple' ou 'complex'
    category: str    # 'statistics', 'correlation', etc
    strategy: QueryStrategy
    justification: str
    requires_csv: bool
    llm_confidence: float = 0.8
    fallback_used: bool = False
    
    def __getitem__(self, key: str):
        """Permite acesso estilo dict para compatibilidade: analysis['category']"""
        return getattr(self, key)
    
    def __setitem__(self, key: str, value):
        """Permite atribui√ß√£o estilo dict para compatibilidade: analysis['category'] = 'stats'"""
        setattr(self, key, value)
    
    def get(self, key: str, default=None):
        """Emula dict.get() para compatibilidade: analysis.get('category', 'unknown')"""
        return getattr(self, key, default)
    
    def to_dict(self) -> Dict:
        """Converte para dict para compatibilidade com c√≥digo legado"""
        result = asdict(self)
        # Converter estrat√©gia aninhada
        if isinstance(result['strategy'], QueryStrategy):
            result['strategy'] = result['strategy'].to_dict()
        return result
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'QueryAnalysis':
        """Cria QueryAnalysis a partir de dict"""
        # Converter estrat√©gia se for dict
        if 'strategy' in data and isinstance(data['strategy'], dict):
            data['strategy'] = QueryStrategy(**data['strategy'])
        return cls(**data)


class QueryComplexity(str, Enum):
    """N√≠veis de complexidade de consulta"""
    SIMPLE = "simple"      # Responde com chunks existentes
    COMPLEX = "complex"    # Requer CSV completo + an√°lise profunda


class QueryCategory(str, Enum):
    """Categorias de perguntas anal√≠ticas"""
    STRUCTURE = "structure"                    # Estrutura, tipos, dimens√µes
    STATISTICS = "statistics"                  # Estat√≠sticas descritivas
    DISTRIBUTION = "distribution"              # Distribui√ß√µes, histogramas
    CORRELATION = "correlation"                # Correla√ß√µes entre vari√°veis
    OUTLIERS = "outliers"                      # Detec√ß√£o de outliers
    PATTERNS = "patterns"                      # Padr√µes temporais/clusters
    VISUALIZATION = "visualization"            # Gera√ß√£o de gr√°ficos
    CUSTOM_ANALYSIS = "custom_analysis"        # An√°lises customizadas


class QueryAnalyzer:
    """
    Analisador inteligente de queries usando LLMs para classifica√ß√£o sem√¢ntica.
    
    ‚ú® REFATORADO:
    - Usa LLMs para detectar complexidade dinamicamente
    - Elimina depend√™ncia de listas fixas de palavras-chave
    - Adapta-se a varia√ß√µes lingu√≠sticas automaticamente
    - Prioriza uso dos 6 chunks anal√≠ticos antes de fallback
    
    100% din√¢mico - sem hardcoding de datasets ou keywords.
    """
    
    def __init__(self):
        self.logger = get_logger(f"{__name__}.QueryAnalyzer")
        self.llm_manager = get_llm_manager()
        
        # Mapeamento de categorias para chunks dispon√≠veis (metadados)
        self.category_to_chunks = {
            'structure': ['metadata_types'],
            'statistics': ['metadata_distribution', 'metadata_central_variability'],
            'distribution': ['metadata_distribution'],
            'correlation': ['metadata_correlations'],
            'outliers': ['metadata_frequency_outliers'],
            'patterns': ['metadata_patterns_clusters']
        }
        
    def analyze(self, query: str, available_chunks: List[str] = None) -> QueryAnalysis:
        """
        Analisa uma query usando LLM para classifica√ß√£o sem√¢ntica inteligente.
        
        Args:
            query: Pergunta do usu√°rio
            available_chunks: Lista de chunk_types dispon√≠veis (ex: ['metadata_types', 'metadata_distribution'])
        
        Returns:
            QueryAnalysis: Objeto tipado com an√°lise completa
        """
        
        # 1. An√°lise sem√¢ntica via LLM
        llm_analysis = self._analyze_with_llm(query, available_chunks)
        
        # 2. Extrair complexidade e categoria
        complexity = QueryComplexity(llm_analysis.get('complexity', 'simple'))
        category = llm_analysis.get('category', 'unknown')
        
        # 3. Determinar estrat√©gia (retorna QueryStrategy)
        strategy = self._determine_strategy(complexity, category, available_chunks, llm_analysis)
        
        # 4. Gerar justificativa
        justification = llm_analysis.get('reasoning', 'An√°lise autom√°tica via LLM')
        
        # ‚úÖ RETORNAR OBJETO, N√ÉO DICT
        result = QueryAnalysis(
            query=query,
            complexity=complexity.value,
            category=category,
            strategy=strategy,
            justification=justification,
            requires_csv=(complexity == QueryComplexity.COMPLEX),
            llm_confidence=llm_analysis.get('confidence', 0.8),
            fallback_used=llm_analysis.get('fallback_used', False)
        )
        
        self.logger.info(f"üìä Query analisada: {result.complexity.upper()} | Categoria: {result.category}")
        self.logger.debug(f"Estrat√©gia: {result.strategy.action} | Confian√ßa: {result.llm_confidence:.2f}")
        
        return result
    
    def _analyze_with_llm(self, query: str, available_chunks: List[str] = None) -> Dict:
        """
        Usa LLM para an√°lise sem√¢ntica da query.
        
        Retorna classifica√ß√£o inteligente de complexidade e categoria.
        """
        
        chunks_description = self._describe_available_chunks(available_chunks)
        
        prompt = f"""Voc√™ √© um especialista em an√°lise de dados.

Analise a pergunta do usu√°rio e classifique com base NO TIPO DE RESPOSTA ESPERADA.

PERGUNTA: "{query}"

CHUNKS DISPON√çVEIS: {chunks_description}

REGRAS:

üîπ SIMPLE = Resposta √© UM VALOR/CONCEITO/ESTAT√çSTICA AGREGADA
Exemplos:
- "Qual a m√©dia de X?" ‚Üí Resposta: "88.35"
- "Quantas colunas?" ‚Üí Resposta: "31 colunas"
- "H√° correla√ß√£o?" ‚Üí Resposta: "Sim, 0.45"
- "Distribui√ß√£o de X" ‚Üí Resposta: "M√≠n:0, Max:100, M√©dia:50, Q1:25, Q3:75"
- "Histograma de X" ‚Üí Resposta: Descri√ß√£o estat√≠stica da distribui√ß√£o
- "Mostre distribui√ß√£o estat√≠stica" ‚Üí Resposta: Estat√≠sticas agregadas (n√£o linhas individuais)

üîπ COMPLEX = Resposta √© LISTAGEM DE REGISTROS INDIVIDUAIS
Exemplos:
- "Quais transa√ß√µes com X>Y?" ‚Üí Resposta: TABELA com linhas espec√≠ficas
- "Mostre top 10 valores" ‚Üí Resposta: LISTA de 10 registros individuais
- "Liste todas as fraudes" ‚Üí Resposta: Linhas espec√≠ficas do dataset

‚ö†Ô∏è ATEN√á√ÉO: Foque no OUTPUT esperado, N√ÉO no c√°lculo
- "Quais linhas com Amount>1000?" ‚Üí COMPLEX (output = lista de linhas)
- "Qual % de linhas com Amount>1000?" ‚Üí SIMPLE (output = n√∫mero estat√≠stico)

CATEGORIAS:
structure, statistics, distribution, correlation, outliers, patterns, visualization, custom_analysis, unknown

JSON (sem markdown):
{{
    "complexity": "simple" ou "complex",
    "category": "categoria",
    "reasoning": "1 frase focada NO TIPO DE OUTPUT",
    "confidence": 0.0-1.0,
    "requires_row_level_data": booleano
}}

JSON:"""

        try:
            config = LLMConfig(temperature=0.1, max_tokens=300)  # Baixa temperatura para classifica√ß√£o
            response = self.llm_manager.chat(prompt=prompt, config=config)
            
            if not response.success:
                self.logger.warning(f"LLM falhou, usando fallback heur√≠stico: {response.error}")
                return self._fallback_heuristic_analysis(query, available_chunks)
            
            # Parse JSON da resposta
            content = response.content.strip()
            
            # Remover poss√≠veis markers de c√≥digo
            if content.startswith('```'):
                content = content.split('```')[1]
                if content.startswith('json'):
                    content = content[4:]
            
            analysis = json.loads(content)
            
            # Validar campos obrigat√≥rios
            if 'complexity' not in analysis or 'category' not in analysis:
                raise ValueError("Campos obrigat√≥rios ausentes na resposta LLM")
            
            self.logger.debug(f"‚úÖ An√°lise LLM: {analysis['complexity']} | {analysis['category']}")
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise LLM: {e}")
            return self._fallback_heuristic_analysis(query, available_chunks)
    
    
    def _describe_available_chunks(self, available_chunks: List[str] = None) -> str:
        """
        Gera descri√ß√£o leg√≠vel dos chunks dispon√≠veis para o LLM.
        """
        
        chunk_descriptions = {
            'metadata_types': 'Tipos de dados das colunas (num√©rico, categ√≥rico, temporal)',
            'metadata_distribution': 'Distribui√ß√µes estat√≠sticas (min, max, m√©dia, mediana, quartis)',
            'metadata_central_variability': 'Medidas de tend√™ncia central e variabilidade (m√©dia, desvio padr√£o, vari√¢ncia)',
            'metadata_frequency_outliers': 'Detec√ß√£o de outliers e valores at√≠picos',
            'metadata_correlations': 'Correla√ß√µes entre vari√°veis num√©ricas',
            'metadata_patterns_clusters': 'Padr√µes temporais e agrupamentos naturais'
        }
        
        if not available_chunks:
            return "Nenhum chunk dispon√≠vel ainda."
        
        descriptions = []
        for chunk in available_chunks:
            desc = chunk_descriptions.get(chunk, chunk)
            descriptions.append(f"- {chunk}: {desc}")
        
        return "\n".join(descriptions)
    
    def _fallback_heuristic_analysis(self, query: str, available_chunks: List[str] = None) -> Dict:
        """
        Fallback heur√≠stico caso LLM falhe.
        
        Usa regras b√°sicas para classifica√ß√£o quando LLM n√£o est√° dispon√≠vel.
        """
        
        query_lower = query.lower()
        
        # Whitelist de termos estat√≠sticos simples (prioridade m√°xima)
        simple_stats = ['m√©dia', 'mediana', 'correla√ß√£o', 'desvio', 'vari√¢ncia', 'quartis', 'm√≠nimo', 'm√°ximo', 'distribui√ß√£o', 'histograma']
        has_simple_stat = any(stat in query_lower for stat in simple_stats)
        
        # Heur√≠stica: palavras que REALMENTE indicam necessidade de dados linha a linha
        # Removido 'mostre' porque aparece em queries simples como "mostre a distribui√ß√£o"
        complex_indicators = [
            'quais', 'liste', 'filtr', 'espec√≠fic', 'exat', 'precis',
            'detalh', 'linha', 'registro', 'transa√ß', 'acima de', 'abaixo de', 'maior que', 'menor que',
            'todos os', 'todas as'
        ]
        
        # Crit√©rio prim√°rio: se tem termo estat√≠stico simples em query curta/m√©dia, √© SIMPLE
        if has_simple_stat and len(query_lower.split()) <= 15:
            is_complex = False
        else:
            # Verificar indicadores complexos apenas se n√£o for query estat√≠stica simples
            is_complex = any(indicator in query_lower for indicator in complex_indicators)
        
        # Tentar inferir categoria por palavras-chave b√°sicas
        category = 'unknown'
        if any(word in query_lower for word in ['tipo', 'coluna', 'estrutura', 'dimens√£o']):
            category = 'structure'
        elif any(word in query_lower for word in ['m√©dia', 'mediana', 'estat√≠stica', 'm√≠nimo', 'm√°ximo']):
            category = 'statistics'
        elif any(word in query_lower for word in ['distribui√ß√£o', 'intervalo']):
            category = 'distribution'
        elif any(word in query_lower for word in ['correla√ß√£o', 'relacionamento']):
            category = 'correlation'
        elif any(word in query_lower for word in ['outlier', 'anomalia', 'at√≠pico']):
            category = 'outliers'
        elif any(word in query_lower for word in ['padr√£o', 'tend√™ncia', 'cluster']):
            category = 'patterns'
        elif any(word in query_lower for word in ['gr√°fico', 'plot', 'visualiza√ß√£o']):
            category = 'visualization'
        
        complexity_value = 'complex' if is_complex else 'simple'
        
        return {
            'complexity': complexity_value,
            'category': category,
            'reasoning': 'Classifica√ß√£o heur√≠stica (LLM indispon√≠vel)',
            'confidence': 0.6,
            'requires_row_level_data': is_complex,
            'fallback_used': True
        }
    
    def _determine_strategy(self, complexity: QueryComplexity, 
                           category: str,
                           available_chunks: List[str] = None,
                           llm_analysis: Dict = None) -> QueryStrategy:
        """
        Determina estrat√©gia de resposta baseada em complexidade e chunks dispon√≠veis.
        
        REFINAMENTO: Fallback usa chunks existentes como GUIA, n√£o duplica an√°lises.
        
        Returns:
            QueryStrategy: Objeto tipado com estrat√©gia de execu√ß√£o
        """
        
        if complexity == QueryComplexity.SIMPLE:
            # Verificar se temos chunks relevantes
            relevant_chunks = self._get_relevant_chunks(category, available_chunks)
            
            return QueryStrategy(
                action='use_existing_chunks',
                chunks_to_query=relevant_chunks,
                fallback_to_csv=False,
                generate_new_chunks=False,
                use_chunks_as_guide=False  # N√£o precisa, j√° responde direto
            )
        
        else:  # COMPLEX
            # Chunks existentes servem como GUIA para fallback
            requires_row_data = llm_analysis.get('requires_row_level_data', True) if llm_analysis else True
            
            return QueryStrategy(
                action='guided_csv_analysis',  # Fallback guiado
                chunks_to_query=available_chunks or [],  # Chunks como refer√™ncia
                fallback_to_csv=True,
                generate_new_chunks='complementary_only',  # Apenas complementos
                use_chunks_as_guide=True,  # Usa chunks como guia
                csv_operations=self._suggest_csv_operations(category),
                avoid_duplicate_analysis=True,  # N√£o duplica an√°lises existentes
                requires_row_level_data=requires_row_data
            )
    
    def _get_relevant_chunks(self, category: str, 
                            available_chunks: List[str] = None) -> List[str]:
        """
        Mapeia categoria para chunk_types relevantes.
        """
        
        if not category or not available_chunks:
            return available_chunks or []
        
        # Buscar chunks relevantes para a categoria
        relevant = self.category_to_chunks.get(category, [])
        
        # Filtrar apenas chunks que realmente existem
        return [chunk for chunk in relevant if chunk in available_chunks]
    
    def _suggest_csv_operations(self, category: str) -> List[str]:
        """
        Sugere opera√ß√µes necess√°rias no CSV completo baseado na categoria.
        """
        
        operations_map = {
            'visualization': ['load_full_dataframe', 'generate_plot', 'save_visualization'],
            'custom_analysis': ['load_full_dataframe', 'apply_custom_logic', 'generate_insights'],
            'outliers': ['load_full_dataframe', 'detect_outliers_detailed', 'generate_outlier_report'],
            'correlation': ['load_full_dataframe', 'compute_correlations', 'generate_heatmap'],
            'patterns': ['load_full_dataframe', 'analyze_temporal_patterns', 'detect_clusters']
        }
        
        return operations_map.get(category, ['load_full_dataframe', 'perform_analysis'])


def analyze_query(query: str, available_chunks: List[str] = None) -> QueryAnalysis:
    """
    Fun√ß√£o helper para an√°lise r√°pida de query.
    
    Args:
        query: Pergunta do usu√°rio
        available_chunks: Lista de chunk_types dispon√≠veis
    
    Returns:
        QueryAnalysis: Objeto tipado com an√°lise completa
    """
    analyzer = QueryAnalyzer()
    return analyzer.analyze(query, available_chunks)
