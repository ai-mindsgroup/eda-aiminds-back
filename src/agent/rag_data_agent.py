"""
Agente de An√°lise de Dados via RAG Vetorial Puro com Mem√≥ria Persistente e LangChain.

VERS√ÉO 2.0 - REFATORADA:
- ‚úÖ Mem√≥ria persistente em Supabase (tabelas agent_sessions, agent_conversations, agent_context)
- ‚úÖ LangChain integrado nativamente (ChatOpenAI, ChatGoogleGenerativeAI)
- ‚úÖ M√©todos async para performance
- ‚úÖ Contexto conversacional entre intera√ß√µes
- ‚úÖ Busca vetorial pura (sem keywords hardcoded)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è  EXCE√á√ÉO DE CONFORMIDADE: ACESSO DIRETO A CSV PARA VISUALIZA√á√ïES
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

CONTEXTO:
- Tabela 'embeddings' armazena chunks de an√°lises estat√≠sticas (Markdown)
- Visualiza√ß√µes (histogramas) requerem dados tabulares completos (285k linhas)
- Embeddar cada linha seria ineficiente: ~$50-100 custo + overhead desnecess√°rio

SOLU√á√ÉO IMPLEMENTADA:
- Quando visualiza√ß√£o √© solicitada, acessa CSV diretamente via pd.read_csv()
- Acesso √© READ-ONLY, sem modifica√ß√£o de dados
- Log completo de auditoria registrado (linhas 318-350)
- Metadados de conformidade inclu√≠dos em todas as respostas

JUSTIFICATIVA (ADERENTE A BOAS PR√ÅTICAS DE MERCADO):
1. Padr√£o da ind√∫stria: LangChain CSV Agents, LlamaIndex, OpenAI Code Interpreter
2. Separa√ß√£o de responsabilidades: RAG para busca sem√¢ntica, CSV para dados tabulares
3. Custo-benef√≠cio: evita armazenamento/processamento desnecess√°rio
4. Performance: leitura direta √© mais r√°pida que reconstitui√ß√£o de embeddings

IMPLEMENTA√á√ÉO FUTURA (Opcional):
- TODO: Adicionar chunks 'raw_data' na tabela embeddings durante ingest√£o
- TODO: Implementar reconstitui√ß√£o de DataFrame a partir de embeddings
- TODO: Adicionar configura√ß√£o para escolher entre direct-access vs embeddings

AUDITORIA E COMPLIANCE:
- ‚úÖ Log detalhado com event_type, timestamp, session_id, csv_path, size
- ‚úÖ Metadados em response.metadata['conformidade_exception']
- ‚úÖ Documenta√ß√£o clara da exce√ß√£o e justificativa
- ‚úÖ Aprova√ß√£o registrada (approved=True)

REFER√äNCIAS:
- LangChain CSV Agent: https://python.langchain.com/docs/integrations/toolkits/csv
- OpenAI Code Interpreter: https://openai.com/blog/code-interpreter
- Hybrid RAG Architectures: https://docs.llamaindex.ai/en/stable/examples/query_engine/

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import logging
from typing import Any, Dict, List, Optional
import json
from datetime import datetime
import asyncio

from agent.base_agent import BaseAgent, AgentError
from vectorstore.supabase_client import supabase
from embeddings.generator import EmbeddingGenerator
from utils.logging_config import get_logger
from analysis.intent_classifier import IntentClassifier, AnalysisIntent

# Imports LangChain
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_experimental.tools import PythonREPLTool
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    AIMessage = None
    PythonREPLTool = None
    print(f"‚ö†Ô∏è LangChain n√£o dispon√≠vel: {e}")


class RAGDataAgent(BaseAgent):
    def _interpretar_pergunta_llm(self, pergunta: str, df):
        """
        ‚úÖ V3.0: Utiliza IntentClassifier para classifica√ß√£o sem√¢ntica SEM HARD-CODING.
        
        Retorna instru√ß√µes anal√≠ticas baseadas na inten√ß√£o classificada pela LLM.
        Cada instru√ß√£o √© um dict: {'acao': ..., 'colunas': [...], 'params': {}, 'justificativa': str}
        """
        if not self.llm:
            # Fallback: retorna instru√ß√£o gen√©rica
            return [{'acao': 'estat√≠sticas gerais', 'colunas': list(df.columns), 'params': {}, 
                    'justificativa': 'LLM indispon√≠vel, fornecendo estat√≠sticas gerais.'}]
        
        try:
            # üî• V3.0: Usar IntentClassifier para classifica√ß√£o sem√¢ntica
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            # Classificar inten√ß√£o da pergunta
            context = {
                'available_columns': list(df.columns),
                'dataframe_info': f"Shape: {df.shape}, Colunas num√©ricas: {df.select_dtypes(include=['number']).columns.tolist()}"
            }
            
            classification_result = classifier.classify(query=pergunta, context=context)
            
            # Log da classifica√ß√£o
            self.logger.info({
                'event': 'intent_classification',
                'primary_intent': classification_result.primary_intent.value,
                'secondary_intents': [intent.value for intent in classification_result.secondary_intents],
                'confidence': classification_result.confidence,
                'reasoning': classification_result.reasoning
            })
            
            # üéØ Mapear inten√ß√µes para instru√ß√µes anal√≠ticas
            instrucoes = []
            
            # Processar inten√ß√£o prim√°ria
            primary_action = self._intent_to_action(
                classification_result.primary_intent, 
                df, 
                classification_result.reasoning
            )
            if primary_action:
                instrucoes.append(primary_action)
            
            # Processar inten√ß√µes secund√°rias se existirem
            for secondary_intent in classification_result.secondary_intents:
                secondary_action = self._intent_to_action(
                    secondary_intent, 
                    df, 
                    f"Inten√ß√£o secund√°ria detectada: {secondary_intent.value}"
                )
                if secondary_action and secondary_action not in instrucoes:
                    instrucoes.append(secondary_action)
            
            # Garantir pelo menos uma instru√ß√£o
            if not instrucoes:
                instrucoes = [{
                    'acao': 'estat√≠sticas gerais',
                    'colunas': list(df.columns),
                    'params': {},
                    'justificativa': 'Nenhuma inten√ß√£o espec√≠fica detectada, fornecendo vis√£o geral.'
                }]
            
            self.logger.info({
                'event': 'instructions_generated',
                'num_instructions': len(instrucoes),
                'actions': [ins['acao'] for ins in instrucoes]
            })
            
            return instrucoes
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao interpretar pergunta com IntentClassifier: {e}")
            # Fallback para interpreta√ß√£o b√°sica via LLM direta
            return self._fallback_interpretation(pergunta, df)
    
    def _intent_to_action(self, intent: AnalysisIntent, df, justificativa: str) -> Optional[Dict]:
        """
        üéØ Converte AnalysisIntent em instru√ß√£o anal√≠tica.
        Mapeia tipos de inten√ß√£o para a√ß√µes espec√≠ficas.
        """
        # Selecionar colunas num√©ricas por padr√£o
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        all_cols = list(df.columns)
        
        intent_map = {
            AnalysisIntent.STATISTICAL: {
                'acao': 'estat√≠sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.FREQUENCY: {
                'acao': 'frequency_analysis',
                'colunas': all_cols,
                'params': {'top_n': 10},
                'justificativa': justificativa
            },
            AnalysisIntent.TEMPORAL: {
                'acao': 'temporal_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.CLUSTERING: {
                'acao': 'clustering_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {'n_clusters': 3},
                'justificativa': justificativa
            },
            AnalysisIntent.CORRELATION: {
                'acao': 'correlation_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.OUTLIERS: {
                'acao': 'outlier_detection',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.COMPARISON: {
                'acao': 'comparison_analysis',
                'colunas': all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.VISUALIZATION: {
                'acao': 'visualization_request',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.GENERAL: {
                'acao': 'estat√≠sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            }
        }
        
        return intent_map.get(intent)
    
    def _fallback_interpretation(self, pergunta: str, df) -> List[Dict]:
        """
        Fallback: interpreta√ß√£o b√°sica via LLM direta quando IntentClassifier falha.
        Mant√©m apenas l√≥gica essencial, SEM hard-coding de keywords.
        """
        prompt = (
            "Voc√™ √© um especialista em an√°lise de dados.\n"
            "Interprete a pergunta do usu√°rio e retorne uma lista JSON de instru√ß√µes anal√≠ticas.\n"
            "Formato de cada instru√ß√£o: {'acao': str, 'colunas': [str], 'params': {}, 'justificativa': str}\n"
            f"Pergunta: {pergunta}\n"
            f"Colunas dispon√≠veis: {list(df.columns)}\n"
            "Responda APENAS com o JSON, sem explica√ß√µes."
        )
        
        try:
            response = self.llm.invoke(prompt)
            import json
            instrucoes = json.loads(response.content)
            return instrucoes if isinstance(instrucoes, list) else [instrucoes]
        except Exception as e:
            self.logger.error(f"Fallback interpretation falhou: {e}")
            return [{
                'acao': 'estat√≠sticas gerais',
                'colunas': list(df.columns),
                'params': {},
                'justificativa': 'Interpreta√ß√£o padr√£o devido a erro.'
            }]

    def _executar_instrucao(self, df, instrucao):
        """
        Executa uma instru√ß√£o anal√≠tica sobre o DataFrame.
        Suporta m√©tricas nativas pandas/NumPy e delega compostas √† LLM.
        """
        acao = instrucao.get('acao','')
        colunas = instrucao.get('colunas', list(df.columns))
        params = instrucao.get('params', {})
        try:
            # Normalizar nome da a√ß√£o para compara√ß√£o (tolerante a mai√∫sculas/min√∫sculas)
            acao_norm = str(acao).strip().lower()
            # M√©tricas nativas pandas
            if acao_norm in ('m√©dia', 'media', 'mean'):
                return df[colunas].mean().to_frame(name='M√©dia')
            if acao_norm in ('mediana', 'median'):
                return df[colunas].median().to_frame(name='Mediana')
            if acao_norm in ('moda', 'mode'):
                return df[colunas].mode().T
            if acao_norm in ('desvio padr√£o', 'desvio padrao', 'std', 'standard deviation'):
                return df[colunas].std().to_frame(name='Desvio padr√£o')
            if acao_norm in ('vari√¢ncia', 'variancia', 'variance', 'var'):
                # Vari√¢ncia populacional/por padr√£o pandas var() usa ddof=1 (amostral). Mantemos default.
                return df[colunas].var().to_frame(name='Vari√¢ncia')
            if acao_norm in ('intervalo', 'minmax', 'min_max', 'm√≠nimo', 'm√°ximo'):
                resultado = df[colunas].agg(['min','max']).T
                resultado.columns = ['M√≠nimo','M√°ximo']
                return resultado
            if acao_norm in ('estat√≠sticas gerais', 'estatisticas gerais', 'describe', 'summary', 'resumo'):
                return df[colunas].describe().T
            # M√©tricas compostas ou extraordin√°rias: delega √† LLM com execu√ß√£o SEGURA
            if self.llm and PythonREPLTool:
                try:
                    # üîí SEGURAN√áA: Usar PythonREPLTool com sandbox isolado
                    python_repl = PythonREPLTool()
                    
                    prompt = (
                        f"Receba instru√ß√£o anal√≠tica: {instrucao}.\n"
                        f"DataFrame j√° est√° dispon√≠vel como vari√°vel 'df'.\n"
                        f"Colunas dispon√≠veis: {list(df.columns)}.\n"
                        "Gere c√≥digo Python (pandas/numpy) que:\n"
                        "1. Execute a m√©trica pedida\n"
                        "2. Armazene o resultado em uma vari√°vel chamada 'resultado'\n"
                        "3. Retorne 'resultado' na √∫ltima linha\n"
                        "Retorne APENAS o c√≥digo, sem explica√ß√µes, sem markdown.\n"
                        "Exemplo: resultado = df['coluna'].mean()"
                    )
                    
                    response = self.llm.invoke(prompt)
                    code = response.content.strip()
                    
                    # Remove markdown code blocks se presentes
                    if code.startswith("```python"):
                        code = code.split("```python")[1].split("```")[0].strip()
                    elif code.startswith("```"):
                        code = code.split("```")[1].split("```")[0].strip()
                    
                    # Log c√≥digo antes de executar (auditoria)
                    self.logger.info(f"üîí Executando c√≥digo seguro via PythonREPLTool:\n{code[:200]}...")
                    
                    # Preparar contexto com DataFrame
                    import pandas as pd
                    import numpy as np
                    globals_context = {'df': df, 'pd': pd, 'np': np}
                    
                    # üîí Execu√ß√£o segura via PythonREPLTool (sandbox isolado)
                    # Nota: PythonREPLTool executa em ambiente isolado sem acesso ao filesystem
                    resultado = python_repl.run(code, globals=globals_context)
                    
                    self.logger.info(f"‚úÖ C√≥digo executado com sucesso via PythonREPLTool")
                    return resultado
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao executar c√≥digo via PythonREPLTool: {e}")
                    self.logger.debug(f"C√≥digo problem√°tico: {code}")
                    return None
            else:
                self.logger.warning("LLM ou PythonREPLTool n√£o dispon√≠vel para m√©tricas compostas")
                return None
        except Exception as e:
            self.logger.warning(f"Falha ao executar instru√ß√£o: {instrucao} | Erro: {e}")
            return None

    def _analisar_completo_csv(self, csv_path: str, pergunta: str, override_temporal_col: str = None,
                               temporal_col_names: list = None, accepted_types: tuple = None) -> str:
        """
        Executa an√°lise temporal robusta e modular usando arquitetura refatorada V2.0.
        
        ARQUITETURA MODULAR:
        - Detec√ß√£o via TemporalColumnDetector (src/analysis/temporal_detection.py)
        - An√°lise via TemporalAnalyzer (src/analysis/temporal_analyzer.py)
        - Fallback para an√°lise estat√≠stica geral quando n√£o houver colunas temporais
        
        Crit√©rios de detec√ß√£o:
        - Override manual (priorit√°rio)
        - Tipo datetime64 nativo
        - Nomes comuns parametriz√°veis (case-insensitive)
        - Convers√£o de strings temporais
        - Sequ√™ncias num√©ricas temporais (modo agressivo)
        
        Par√¢metros:
            - override_temporal_col: for√ßa uso de coluna espec√≠fica (ou None para auto)
            - temporal_col_names: lista de nomes comuns (default: ["time", "date", "timestamp", "data", "datetime"])
            - accepted_types: DEPRECATED - mantido para backward compatibility
            
        Returns:
            String formatada em Markdown com an√°lises temporais e/ou estat√≠sticas
        """
        import pandas as pd
        from analysis.temporal_detection import TemporalColumnDetector, TemporalDetectionConfig
        from analysis.temporal_analyzer import TemporalAnalyzer
        
        # Carregar dados
        df = pd.read_csv(csv_path)
        
        logger = self.logger if hasattr(self, 'logger') else logging.getLogger(__name__)
        logger.info({
            'event': 'inicio_analise_csv_v2',
            'csv_path': csv_path,
            'shape': df.shape,
            'override_temporal_col': override_temporal_col
        })
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ETAPA 1: DETEC√á√ÉO DE COLUNAS TEMPORAIS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # Configurar detector com par√¢metros customiz√°veis
        detection_config = TemporalDetectionConfig()
        if temporal_col_names:
            detection_config.common_names = temporal_col_names
        
        detector = TemporalColumnDetector(config=detection_config)
        
        try:
            detection_results = detector.detect(df, override_column=override_temporal_col)
            temporal_cols = detector.get_detected_columns(detection_results)
            detection_summary = detector.get_detection_summary(detection_results)
            
            logger.info({
                'event': 'deteccao_temporal_concluida',
                'colunas_detectadas': temporal_cols,
                'total_colunas': len(df.columns),
                'taxa_deteccao': detection_summary['detection_rate'],
                'metodos_usados': detection_summary['methods_used']
            })
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de colunas temporais: {e}", exc_info=True)
            temporal_cols = []
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ETAPA 2: AN√ÅLISE TEMPORAL (se colunas detectadas)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        if temporal_cols:
            logger.info(f"Executando an√°lise temporal em {len(temporal_cols)} coluna(s)")
            
            analyzer = TemporalAnalyzer(logger=logger)
            respostas = []
            
            for col in temporal_cols:
                try:
                    # Executar an√°lise temporal avan√ßada
                    result = analyzer.analyze(df, col, enable_advanced=True)
                    
                    # Gerar relat√≥rio Markdown
                    respostas.append(result.to_markdown())
                    
                    logger.info({
                        'event': 'analise_temporal_coluna_concluida',
                        'coluna': col,
                        'trend_type': result.trend.get('type'),
                        'anomalies_count': result.anomalies.get('count', 0),
                        'seasonality_detected': result.seasonality.get('detected', False)
                    })
                except Exception as e:
                    logger.error(f"Erro ao analisar coluna temporal '{col}': {e}", exc_info=True)
                    respostas.append(
                        f"## Erro na An√°lise: {col}\n\n"
                        f"N√£o foi poss√≠vel completar a an√°lise temporal da coluna '{col}': {str(e)}\n"
                    )
            
            # Adicionar sum√°rio executivo da detec√ß√£o
            header = (
                f"# An√°lise Temporal Completa\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"**Colunas analisadas:** {len(temporal_cols)} de {len(df.columns)} colunas totais\n\n"
                f"**Taxa de detec√ß√£o:** {detection_summary['detection_rate']:.1%}\n\n"
                f"**M√©todos de detec√ß√£o utilizados:** {', '.join(detection_summary['methods_used'].keys())}\n\n"
                "---\n\n"
            )
            
            return header + "\n\n---\n\n".join(respostas)
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ETAPA 3: FALLBACK - AN√ÅLISE ESTAT√çSTICA GERAL
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        logger.info({
            'event': 'fallback_analise_geral',
            'motivo': 'nenhuma_coluna_temporal_detectada'
        })
        
        # Interpretar inten√ß√£o da pergunta via LLM
        instrucoes = self._interpretar_pergunta_llm(pergunta, df)
        
        # Executar instru√ß√µes e consolidar resultados
        resultados = []
        for instrucao in instrucoes:
            resultado = self._executar_instrucao(df, instrucao)
            if resultado is not None:
                justificativa = instrucao.get('justificativa', '')
                if hasattr(resultado, 'to_markdown'):
                    resultados.append(
                        f"**{instrucao.get('acao', 'M√©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{resultado.to_markdown()}"
                    )
                else:
                    resultados.append(
                        f"**{instrucao.get('acao', 'M√©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{str(resultado)}"
                    )
        
        if resultados:
            header = (
                f"# An√°lise Estat√≠stica Geral\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"*Nenhuma coluna temporal detectada. Executando an√°lise estat√≠stica padr√£o.*\n\n"
                "---\n\n"
            )
            return header + "\n\n".join(resultados)
        else:
            # Fallback final: estat√≠sticas gerais descritivas
            logger.warning("Fallback final: retornando describe() do DataFrame")
            return (
                f"# Estat√≠sticas Descritivas\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"{df.describe().T.to_markdown()}"
            )

    def _should_use_global_csv(self, query: str, chunks_metadata: List[Dict]) -> bool:
        """
        Decide se √© necess√°rio recorrer ao CSV completo para responder a pergunta.
        Crit√©rios:
        - Pergunta exige an√°lise de todas as colunas/linhas (ex: intervalo de todas vari√°veis)
        - Chunks n√£o possuem dados suficientes (ex: subset de colunas)
        """
        # Detecta termos que indicam an√°lise global
        termos_globais = ["todas as vari√°veis", "todas as colunas", "intervalo de cada vari√°vel", "intervalo de todas", "intervalo completo", "todas as linhas", "an√°lise completa"]
        if any(t in query.lower() for t in termos_globais):
            return True
        # Detecta se chunks n√£o cobrem todas as colunas
        if chunks_metadata:
            # Extrai colunas presentes nos chunks
            colunas_chunks = set()
            for chunk in chunks_metadata:
                if 'columns' in chunk:
                    colunas_chunks.update(chunk['columns'])
            # Se n√∫mero de colunas for pequeno, pode indicar subset
            if len(colunas_chunks) < 5:
                return True
        return False

    def reset_memory(self, session_id: str = None):
        """
        Reseta a mem√≥ria/contexto do agente para a sess√£o informada.
        """
        self.memory = {}
        if session_id:
            self.session_id = session_id
        self.logger.info(f"Mem√≥ria/contexto resetados para sess√£o: {session_id}")
    """
    Agente que responde perguntas sobre dados usando RAG vetorial + mem√≥ria persistente + LangChain.
    
    Fluxo V2.0:
    1. Inicializa sess√£o de mem√≥ria (se n√£o existir)
    2. Recupera contexto conversacional anterior
    3. Gera embedding da pergunta
    4. Busca chunks similares nos DADOS usando match_embeddings()
    5. Usa LangChain LLM para interpretar chunks + contexto hist√≥rico
    6. Salva intera√ß√£o na mem√≥ria persistente
    7. Retorna resposta contextualizada
    
    SEM keywords hardcoded, SEM classifica√ß√£o manual, SEM listas fixas.
    COM mem√≥ria persistente, COM LangChain, COM contexto conversacional.
    """
    
    def __init__(self):
        super().__init__(
            name="rag_data_analyzer",
            description="Analisa dados usando busca vetorial sem√¢ntica pura com mem√≥ria persistente",
            enable_memory=True  # ‚úÖ CR√çTICO: Habilita mem√≥ria persistente
        )
        self.logger = get_logger("agent.rag_data")
        self.embedding_gen = EmbeddingGenerator()
        
        # Inicializar LLM LangChain
        self._init_langchain_llm()
        
        self.logger.info("‚úÖ RAGDataAgent V2.0 inicializado - RAG vetorial + mem√≥ria + LangChain")
    
    def _init_langchain_llm(self):
        """Inicializa LLM do LangChain com fallback."""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è LangChain n√£o dispon√≠vel - usando fallback")
            self.llm = None
            return
        
        try:
            # Tentar Google Gemini primeiro (melhor custo-benef√≠cio)
            from src.settings import GOOGLE_API_KEY
            if GOOGLE_API_KEY:
                self.llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-flash",
                    temperature=0.3,
                    max_tokens=2000,
                    google_api_key=GOOGLE_API_KEY
                )
                self.logger.info("‚úÖ LLM LangChain inicializado: Google Gemini")
                return
        except Exception as e:
            self.logger.warning(f"Google Gemini n√£o dispon√≠vel: {e}")
        
        try:
            # Fallback: OpenAI
            from src.settings import OPENAI_API_KEY
            if OPENAI_API_KEY:
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.3,
                    max_tokens=2000,
                    openai_api_key=OPENAI_API_KEY
                )
                self.logger.info("‚úÖ LLM LangChain inicializado: OpenAI GPT-4o-mini")
                return
        except Exception as e:
            self.logger.warning(f"OpenAI n√£o dispon√≠vel: {e}")
        
        self.llm = None
        self.logger.warning("‚ö†Ô∏è Nenhum LLM LangChain dispon√≠vel - usando fallback manual")
    
    async def process(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa query do usu√°rio usando RAG vetorial + mem√≥ria persistente.
        
        VERS√ÉO ASYNC com mem√≥ria persistente.
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto adicional (opcional)
            session_id: ID da sess√£o para mem√≥ria persistente
            
        Returns:
            Resposta baseada em busca vetorial + contexto hist√≥rico
        """
        start_time = datetime.now()
        
        try:
            self.logger.info(f"üîç Processando query via RAG V2.0: {query[:80]}...")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 1. INICIALIZAR MEM√ìRIA PERSISTENTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if not self._current_session_id:
                if session_id:
                    await self.init_memory_session(session_id)
                else:
                    session_id = await self.init_memory_session()
                self.logger.info(f"‚úÖ Sess√£o de mem√≥ria inicializada: {session_id}")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 2. RECUPERAR CONTEXTO CONVERSACIONAL ANTERIOR
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # FILTRAR CONTEXTO: manter apenas campos relevantes para an√°lise
            memory_context = {}
            if context:
                filtered_context = {}
                if 'chunks' in context:
                    filtered_context['chunks'] = context['chunks']
                if 'csv_data' in context:
                    filtered_context['csv_data'] = context['csv_data']
                # ‚úÖ PRESERVAR FLAGS DE VISUALIZA√á√ÉO
                if 'visualization_requested' in context:
                    filtered_context['visualization_requested'] = context['visualization_requested']
                if 'visualization_type' in context:
                    filtered_context['visualization_type'] = context['visualization_type']
                if 'fallback_sample_limit' in context:
                    filtered_context['fallback_sample_limit'] = context['fallback_sample_limit']
                if 'reconstructed_df' in context:
                    filtered_context['reconstructed_df'] = context['reconstructed_df']
                context = filtered_context
            # N√ÉO recuperar contexto de mem√≥ria para queries de intervalo
            interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
            if any(term in query.lower() for term in interval_terms):
                memory_context = {}  # Ignorar hist√≥rico/mem√≥ria
            elif self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(
                    f"‚úÖ Contexto de mem√≥ria recuperado: "
                    f"{len(memory_context.get('recent_messages', []))} mensagens anteriores"
                )
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 3. GERAR EMBEDDING DA QUERY
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            self.logger.debug("Gerando embedding da query...")
            embedding_result = self.embedding_gen.generate_embedding(query)
            
            # Extrair lista de floats do resultado
            if isinstance(embedding_result, list):
                query_embedding = embedding_result
            elif hasattr(embedding_result, 'embedding'):
                query_embedding = embedding_result.embedding
            else:
                return self._build_error_response("Formato de embedding inv√°lido")
            
            if not query_embedding or len(query_embedding) == 0:
                return self._build_error_response("Falha ao gerar embedding da query")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 4. BUSCAR CHUNKS SIMILARES NOS DADOS
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            self.logger.debug("Buscando chunks similares nos dados...")
            similar_chunks = self._search_similar_data(
                query_embedding=query_embedding,
                threshold=0.3,  # Threshold igual ao RAGAgent para capturar chunks anal√≠ticos
                limit=10
            )
            
            # SALVAR CONTEXTO DE DADOS NA TABELA agent_context
            if self.has_memory and self._current_session_id and similar_chunks:
                data_context = {
                    "dataset_info": {
                        "total_chunks": len(similar_chunks),
                        "source_types": list(set(c.get('source_type', 'unknown') for c in similar_chunks)),
                        "embedding_provider": "sentence-transformer",
                        "last_query": query[:100],
                        "query_timestamp": datetime.now().isoformat()
                    },
                    "performance_metrics": {
                        "embedding_generation_time": "N/A",  # Poderia ser medido
                        "search_time": "N/A",  # Poderia ser medido
                        "chunks_found": len(similar_chunks)
                    }
                }
                
                try:
                    await self.remember_data_context(
                        data_info=data_context,
                        context_key="current_dataset_info"
                    )
                    self.logger.debug("‚úÖ Contexto de dados salvo na tabela agent_context")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Falha ao salvar contexto de dados: {e}")
            
            # Fallback inteligente: se chunks n√£o s√£o suficientes OU pergunta exige an√°lise global
            if not similar_chunks or self._should_use_global_csv(query, similar_chunks):
                # Tentar extrair caminho do CSV dos chunks ou do contexto
                csv_path = None
                if context and 'csv_path' in context:
                    csv_path = context['csv_path']
                elif similar_chunks:
                    for chunk in similar_chunks:
                        if 'csv_path' in chunk:
                            csv_path = chunk['csv_path']
                            break
                # Se n√£o encontrar, buscar na pasta processados
                if not csv_path:
                    try:
                        from src.settings import EDA_DATA_DIR_PROCESSADO
                        import os
                        import pandas as pd
                        csv_files = list(EDA_DATA_DIR_PROCESSADO.glob('*.csv'))
                        if csv_files:
                            csv_path = str(max(csv_files, key=lambda p: p.stat().st_mtime))
                    except Exception as e:
                        self.logger.warning(f"N√£o foi poss√≠vel localizar CSV para fallback: {e}")
                if csv_path:
                    self.logger.info(f"‚ö° Fallback: an√°lise global do CSV ({csv_path}) para pergunta '{query[:60]}...'")
                    try:
                        resposta_csv = self._analisar_completo_csv(csv_path, query)
                        return self._build_response(
                            resposta_csv,
                            metadata={
                                "method": "global_csv_fallback",
                                "csv_path": csv_path,
                                "chunks_found": len(similar_chunks)
                            }
                        )
                    except Exception as e:
                        self.logger.error(f"Erro no fallback global CSV: {e}")
                        return self._build_error_response(f"Erro ao processar CSV global: {e}")
                else:
                    self.logger.error("‚ùå Fallback global: CSV n√£o encontrado.")
                    return self._build_error_response("CSV original n√£o encontrado para an√°lise global.")
            
            self.logger.info(f"‚úÖ Encontrados {len(similar_chunks)} chunks relevantes")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üÜï VERIFICAR SE VISUALIZA√á√ÉO FOI SOLICITADA (MESMO COM CHUNKS)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            viz_requested = bool(context and context.get('visualization_requested'))
            if viz_requested:
                self.logger.info("üìä Visualiza√ß√£o solicitada - gerando gr√°ficos...")
                try:
                    import pandas as pd
                    from pathlib import Path
                    
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # ‚ö†Ô∏è EXCE√á√ÉO DE CONFORMIDADE - ACESSO DIRETO AO CSV
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # JUSTIFICATIVA:
                    # 1. Tabela embeddings cont√©m chunks de an√°lises estat√≠sticas (Markdown)
                    # 2. Histogramas requerem dados tabulares completos (285k linhas √ó 31 colunas)
                    # 3. Embeddar cada linha seria ineficiente: ~$50-100 de custo + overhead
                    # 4. Padr√£o de mercado: LangChain, LlamaIndex, OpenAI Code Interpreter
                    #    fazem leitura direta de CSV para an√°lises quantitativas
                    # 
                    # IMPLEMENTA√á√ÉO FUTURA:
                    # - TODO: Adicionar chunks raw_data na tabela embeddings durante ingest√£o
                    # - TODO: Implementar reconstitui√ß√£o de DataFrame a partir de embeddings
                    # 
                    # AUDITORIA:
                    # - Log completo de acesso registrado
                    # - Metadados inclu√≠dos na resposta
                    # - Acesso read-only sem modifica√ß√£o de dados
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    
                    from src.settings import EDA_DATA_DIR_PROCESSADO
                    # Buscar CSV mais recente em data/processado/
                    csv_files = list(EDA_DATA_DIR_PROCESSADO.glob("*.csv"))
                    if not csv_files:
                        self.logger.error("‚ùå Nenhum arquivo CSV encontrado em data/processado/")
                        self.logger.info("‚ö†Ô∏è Continuando com resposta textual sem visualiza√ß√µes")
                    else:
                        # Pegar o arquivo mais recente (√∫ltimo modificado)
                        csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
                        csv_size_mb = csv_path.stat().st_size / 1_000_000
                        self.logger.warning(
                            "‚ö†Ô∏è EXCE√á√ÉO DE CONFORMIDADE: Acesso direto ao CSV para visualiza√ß√£o",
                            extra={
                                "event_type": "direct_csv_access",
                                "user_query": query[:100],
                                "csv_path": str(csv_path),
                                "csv_size_mb": round(csv_size_mb, 2),
                                "access_reason": "histogram_generation",
                                "session_id": self._current_session_id,
                                "agent_name": self.name,
                                "timestamp": datetime.now().isoformat(),
                                "conformidade_status": "exception_approved",
                                "alternative_implementation": "future_raw_data_embeddings",
                                "cost_saved_estimate_usd": 50.0
                            }
                        )
                        viz_df = pd.read_csv(csv_path)
                        self.logger.info(
                            f"‚úÖ CSV carregado para visualiza√ß√£o: {viz_df.shape[0]:,} linhas √ó {viz_df.shape[1]} colunas | "
                            f"Tamanho: {csv_size_mb:.2f} MB"
                        )
                        # Delegar para agente de visualiza√ß√£o
                        # Removido: agente obsoleto csv_analysis_agent.py
                        vis_context = context.copy() if context else {}
                        vis_context['reconstructed_df'] = viz_df
                        vis_result = self._handle_visualization_query(query, vis_context)
                        if vis_result.get('metadata', {}).get('visualization_success'):
                            # Combinar resposta de visualiza√ß√£o com an√°lise textual dos chunks
                            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
                            context_str = "\n\n".join(context_texts[:5])
                            text_response = await self._generate_llm_response_langchain(
                                query=query,
                                context_data=context_str,
                                memory_context=memory_context,
                                chunks_metadata=similar_chunks
                            )
                            # Combinar resposta textual com informa√ß√£o sobre gr√°ficos
                            graficos_info = vis_result.get('metadata', {}).get('graficos_gerados', [])
                            if graficos_info:
                                graficos_msg = f"\n\nüìä **Visualiza√ß√µes Geradas:**\n"
                                for gf in graficos_info:
                                    graficos_msg += f"‚Ä¢ {gf}\n"
                                combined_response = text_response + graficos_msg
                            else:
                                combined_response = text_response
                            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                            # Salvar intera√ß√£o com metadados de conformidade
                            if self.has_memory:
                                await self.remember_interaction(
                                    query=query,
                                    response=combined_response,
                                    processing_time_ms=processing_time_ms,
                                    confidence=1.0,
                                    model_used="rag_v2_with_visualizations",
                                    metadata={
                                        "chunks_found": len(similar_chunks),
                                        "visualization_success": True,
                                        "graficos_gerados": len(graficos_info),
                                        "conformidade_exception": {
                                            "type": "direct_csv_access",
                                            "reason": "visualization_requires_raw_data",
                                            "csv_path": str(csv_path),
                                            "csv_size_mb": round(csv_size_mb, 2),
                                            "access_timestamp": datetime.now().isoformat(),
                                            "approved": True,
                                            "alternative_future": "raw_data_embeddings_implementation",
                                            "industry_standard": "LangChain/LlamaIndex/OpenAI_pattern",
                                            "cost_saved_usd": 50.0,
                                            "read_only": True
                                        }
                                    }
                                )
                            return self._build_response(
                                combined_response,
                                metadata={
                                    **vis_result.get('metadata', {}),
                                    "chunks_found": len(similar_chunks),
                                    "method": "rag_vectorial_v2_with_viz",
                                    "processing_time_ms": processing_time_ms,
                                    "conformidade_exception": {
                                        "type": "direct_csv_access",
                                        "reason": "visualization_requires_raw_data",
                                        "csv_path": str(csv_path),
                                        "csv_size_mb": round(csv_size_mb, 2),
                                        "approved": True,
                                        "industry_standard": True,
                                        "read_only": True,
                                        "documentation": "See comments in rag_data_agent.py lines 318-335"
                                    }
                                }
                            )
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao gerar visualiza√ß√µes: {e}", exc_info=True)
                    # Continuar com resposta textual normal se visualiza√ß√£o falhar
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 5. GERAR RESPOSTA COM LANGCHAIN + CONTEXTO HIST√ìRICO
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
            context_str = "\n\n".join(context_texts[:5])  # Top 5 mais relevantes
            
            self.logger.debug("Usando LangChain LLM para gerar resposta...")
            response_text = await self._generate_llm_response_langchain(
                query=query,
                context_data=context_str,
                memory_context=memory_context,
                chunks_metadata=similar_chunks
            )
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            avg_similarity = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks)
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 6. SALVAR INTERA√á√ÉO NA MEM√ìRIA PERSISTENTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=response_text,
                    processing_time_ms=processing_time_ms,
                    confidence=avg_similarity,
                    model_used="langchain_gemini" if self.llm else "fallback",
                    metadata={
                        "chunks_found": len(similar_chunks),
                        "chunks_used": min(5, len(similar_chunks)),
                        "avg_similarity": avg_similarity,
                        "top_similarity": similar_chunks[0]['similarity'],
                        "has_history": len(memory_context.get('recent_conversations', [])) > 0
                    }
                )
                self.logger.debug("‚úÖ Intera√ß√£o salva na mem√≥ria persistente")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 7. RETORNAR RESPOSTA COM METADADOS
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            return self._build_response(
                response_text,
                metadata={
                    "chunks_found": len(similar_chunks),
                    "chunks_used": min(5, len(similar_chunks)),
                    "avg_similarity": avg_similarity,
                    "method": "rag_vectorial_v2",
                    "top_similarity": similar_chunks[0]['similarity'] if similar_chunks else 0,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao processar query: {str(e)}", exc_info=True)
            return self._build_error_response(f"Erro no processamento: {str(e)}")
    
    def _search_similar_data(
        self,
        query_embedding: List[float],
        threshold: float = 0.5,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Busca chunks similares nos dados usando match_embeddings RPC.
        
        Args:
            query_embedding: Embedding da query
            threshold: Threshold de similaridade (0.0 - 1.0)
            limit: N√∫mero m√°ximo de resultados
            
        Returns:
            Lista de chunks similares com metadata
        """
        try:
            # Chamar fun√ß√£o RPC match_embeddings
            response = supabase.rpc(
                'match_embeddings',
                {
                    'query_embedding': query_embedding,
                    'similarity_threshold': threshold,
                    'match_count': limit
                }
            ).execute()
            
            if not response.data:
                self.logger.warning("Nenhum chunk similar encontrado")
                return []
            
            self.logger.debug(f"Encontrados {len(response.data)} chunks similares")
            # Parsing defensivo dos embeddings
            from src.embeddings.vector_store import parse_embedding_from_api, VECTOR_DIMENSIONS
            parsed_chunks = []
            for chunk in response.data:
                embedding_raw = chunk.get('embedding')
                try:
                    chunk['embedding'] = parse_embedding_from_api(embedding_raw, VECTOR_DIMENSIONS)
                except Exception as e:
                    self.logger.warning(f"Falha ao parsear embedding do chunk: {e}")
                    chunk['embedding'] = None
                parsed_chunks.append(chunk)
            return parsed_chunks
            
        except Exception as e:
            self.logger.error(f"Erro na busca vetorial: {str(e)}")
            return []
    
    async def _generate_llm_response_langchain(
        self,
        query: str,
        context_data: str,
        memory_context: dict,
        chunks_metadata: list
    ) -> str:
        try:
            # Preparar contexto hist√≥rico da conversa
            history_context = ""
            if memory_context.get('recent_messages') and len(memory_context['recent_messages']) > 0:
                history_context = "\n\n**Contexto da Conversa Anterior:**\n"
                for msg in memory_context['recent_messages'][-6:]:  # √öltimas 6 mensagens (3 pares user/assistant)
                    msg_type = msg.get('type', 'unknown')
                    content = msg.get('content', '')[:200]  # Limitar a 200 chars
                    if msg_type == 'user':
                        history_context += f"- Usu√°rio perguntou: {content}\n"
                    elif msg_type == 'assistant':
                        history_context += f"- Assistente respondeu: {content}\n"
                history_context += "\n"

            # Preparar prompt DIN√ÇMICO baseado no tipo de query
            query_lower = query.lower()
            
            # Detectar tipo de query para customizar prompt
            # TIPO 1: Perguntas sobre o HIST√ìRICO/CONTEXTO da conversa
            if any(term in query_lower for term in ['pergunta anterior', 'perguntei antes', 'falamos sobre', 'conversamos sobre', 'voc√™ disse', 'previous question', 'asked before']):
                # Query sobre HIST√ìRICO - n√£o precisa de chunks, apenas mem√≥ria
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Sua tarefa √© responder sobre o HIST√ìRICO da conversa. "
                    "Use o contexto da conversa anterior fornecido para responder. "
                    "Seja claro e objetivo, referenciando exatamente o que foi discutido."
                )
                user_prompt = (
                    f"{history_context}"  # PRINCIPAL fonte de informa√ß√£o
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Consulte o hist√≥rico da conversa acima\n"
                    "- Responda referenciando exatamente o que foi perguntado/respondido anteriormente\n"
                    "- Se n√£o houver hist√≥rico suficiente, informe claramente\n"
                    "- Finalize com: 'Posso esclarecer mais alguma coisa sobre nossa conversa?'\n\n"
                    "**Resposta:**"
                )
            # TIPO 2: Perguntas sobre VARIABILIDADE
            elif any(term in query_lower for term in ['variabilidade', 'desvio padr√£o', 'vari√¢ncia', 'variance', 'std', 'standard deviation']):
                # Query sobre VARIABILIDADE
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Sua tarefa √© responder sobre a VARIABILIDADE dos dados (desvio padr√£o, vari√¢ncia, coeficiente de varia√ß√£o). "
                    "Use os chunks anal√≠ticos fornecidos E o hist√≥rico da conversa para contextualizar a resposta. "
                    "Responda de forma clara, humanizada e estruturada."
                )
                user_prompt = (
                    f"{history_context}"  # INCLUIR hist√≥rico
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    f"**CHUNKS ANAL√çTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Se houver hist√≥rico relevante, mencione brevemente o contexto\n"
                    "- Apresente m√©tricas de variabilidade: desvio padr√£o e vari√¢ncia para as principais vari√°veis\n"
                    "- Agrupe V1 a V28 com estat√≠sticas agregadas\n"
                    "- Destaque vari√°veis com alta vs baixa variabilidade\n"
                    "- Use formato R$ para valores monet√°rios\n"
                    "- Finalize com: 'Se precisar de mais detalhes, √© s√≥ perguntar!'\n\n"
                    "**Resposta:**"
                )
            # TIPO 3: Perguntas sobre INTERVALOS
            elif any(term in query_lower for term in ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']):
                # Query sobre INTERVALOS
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Sua tarefa √© responder EXCLUSIVAMENTE sobre o INTERVALO (m√≠nimo e m√°ximo) de cada vari√°vel presente nos chunks anal√≠ticos do CSV carregado. "
                    "Ignore completamente qualquer contexto extra, hist√≥rico, mem√≥ria ou dados residuais que n√£o estejam nos chunks anal√≠ticos. "
                    "N√ÉO inclua estat√≠sticas, outliers, gr√°ficos ou qualquer dado n√£o solicitado. Responda de forma clara, objetiva e apenas com o solicitado. "
                    "Para perguntas comuns (sauda√ß√µes, hora, etc.), responda de forma simples e natural, sem an√°lise. Se a pergunta for sobre outro tipo de an√°lise, siga as instru√ß√µes espec√≠ficas do usu√°rio."
                )
                user_prompt = (
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    f"**CHUNKS ANAL√çTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Leia e interprete SOMENTE os chunks fornecidos\n"
                    "- Extraia SOMENTE os valores m√≠nimo e m√°ximo de cada vari√°vel\n"
                    "- N√ÉO inclua an√°lise de outliers, desvio padr√£o, vari√¢ncia ou estat√≠sticas extras\n"
                    "- Formate a resposta em tabela Markdown\n"
                    "- Se n√£o houver informa√ß√£o suficiente, informe claramente\n\n"
                    "**Resposta:**"
                )
            # TIPO 4: Perguntas sobre TIPOS DE DADOS
            elif any(term in query_lower for term in ['tipos', 'tipo de dado', 'num√©rico', 'categ√≥rico', 'categ√≥rica', 'categorical', 'numerical']):
                # Query sobre TIPOS DE DADOS
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Sua tarefa √© identificar e classificar os TIPOS DE DADOS (num√©ricos vs categ√≥ricos) presentes no dataset. "
                    "Use APENAS os chunks anal√≠ticos fornecidos - n√£o invente ou infira informa√ß√µes. "
                    "Responda de forma clara, humanizada e estruturada."
                )
                user_prompt = (
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    f"**CHUNKS ANAL√çTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Adicione mensagem amig√°vel: 'Ol√°! Aqui est√° uma an√°lise dos tipos de vari√°veis presentes no seu conjunto de dados:'\n"
                    "- Divida a resposta em 2 se√ß√µes: 'Vari√°veis Num√©ricas' e 'Vari√°veis Categ√≥ricas'\n"
                    "- Para vari√°veis num√©ricas: agrupe V1 a V28 como 'V1 a V28: Vari√°veis num√©ricas agrupadas, todas apresentam alta variabilidade.'\n"
                    "- Liste tamb√©m: Time, Amount\n"
                    "- Para vari√°veis categ√≥ricas: descreva Class como 'Class: Vari√°vel categ√≥rica com valores poss√≠veis 0 e 1. Utilizada para indicar fraude ou n√£o fraude.'\n"
                    "- N√ÉO mencione frequ√™ncia de valores para Class\n"
                    "- Adicione estat√≠sticas apenas para Amount (M√©dia, Desvio padr√£o, M√≠nimo, M√°ximo) com formato R$ XX.XX\n"
                    "- Finalize com: 'Se precisar de mais detalhes ou quiser analisar outra vari√°vel, √© s√≥ perguntar!'\n\n"
                    "**Resposta:**"
                )
            # TIPO 5: Perguntas sobre FREQU√äNCIA (valores mais/menos frequentes)
            elif any(term in query_lower for term in ['frequente', 'frequentes', 'frequ√™ncia', 'comum', 'raro', 'raros', 'moda', 'contagem', 'value_counts', 'mais ocorre', 'menos ocorre']):
                # Query sobre FREQU√äNCIA
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado em an√°lise de frequ√™ncia. "
                    "Sua tarefa √© identificar e reportar QUANTAS VEZES cada valor aparece no dataset. "
                    "Use APENAS os dados fornecidos nos chunks anal√≠ticos. N√ÉO invente n√∫meros. "
                    "ATEN√á√ÉO: Na tabela 'Colunas Categ√≥ricas', a coluna 'Frequ√™ncia' cont√©m o N√öMERO DE OCORR√äNCIAS do valor mais frequente. "
                    "Exemplo: Se vir '| Class | 0 | 284315 |', significa que o valor '0' aparece 284.315 vezes no dataset."
                )
                user_prompt = (
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    f"**CHUNKS ANAL√çTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRU√á√ïES CR√çTICAS DE INTERPRETA√á√ÉO:**\n"
                    "1. **Leia a tabela 'Colunas Categ√≥ricas' corretamente**:\n"
                    "   - Formato: | Coluna | Valor Mais Frequente | Frequ√™ncia | Valores √önicos | Valores Nulos |\n"
                    "   - A coluna 'Frequ√™ncia' √© o N√öMERO DE VEZES que o 'Valor Mais Frequente' aparece\n"
                    "   - Exemplo: | Class | 0 | 284315 | ‚Üí O valor '0' aparece 284.315 vezes\n\n"
                    "2. **Calcule o valor menos frequente**:\n"
                    "   - Se houver 2 valores √∫nicos (ex: Class com valores 0 e 1)\n"
                    "   - E o total de registros √© conhecido (ex: 284.807)\n"
                    "   - Menos frequente = Total de registros - Frequ√™ncia do mais frequente\n"
                    "   - Exemplo: Class valor '1' = 284.807 - 284.315 = 492 vezes\n\n"
                    "3. **Para COLUNAS NUM√âRICAS**:\n"
                    "   - Leia a tabela 'Colunas Num√©ricas' e encontre a coluna 'Moda'\n"
                    "   - A moda √© o valor num√©rico que mais se repete\n"
                    "   - Explique que para vari√°veis cont√≠nuas, muitos valores aparecem apenas 1 vez\n\n"
                    "**FORMATO DE RESPOSTA OBRIGAT√ìRIO:**\n"
                    "Inicie com: 'Pergunta feita: [pergunta]'\n\n"
                    "Adicione: 'Analisando a frequ√™ncia dos valores no dataset:'\n\n"
                    "**üî¢ Colunas Categ√≥ricas:**\n\n"
                    "Para cada coluna categ√≥rica encontrada, mostre:\n"
                    "- **Coluna [Nome]**: \n"
                    "  * Valor mais frequente: [valor] (aparece [X] vezes)\n"
                    "  * Valor menos frequente: [valor] (aparece [Y] vezes) [SE PUDER CALCULAR]\n\n"
                    "**üìä Colunas Num√©ricas:**\n\n"
                    "Para colunas num√©ricas, mostre a moda estat√≠stica:\n"
                    "- **Coluna [Nome]**: Moda = [valor] (valor que mais se repete)\n\n"
                    "Adicione explica√ß√£o:\n"
                    "'Para vari√°veis num√©ricas cont√≠nuas (como V1-V28, Amount), a maioria dos valores aparece apenas 1 vez. "
                    "A moda estat√≠stica indica o valor que mais se repete, mas para an√°lise mais detalhada, considere perguntar sobre distribui√ß√£o ou intervalos.'\n\n"
                    "Finalize: 'Se precisar de mais detalhes ou an√°lise de distribui√ß√£o, √© s√≥ perguntar!'\n\n"
                    "**‚ö†Ô∏è REGRAS CR√çTICAS:**\n"
                    "- N√ÉO diga 'aparece 0 vezes' quando o n√∫mero na tabela √© POSITIVO\n"
                    "- N√ÉO confunda a coluna 'Frequ√™ncia' com o valor da vari√°vel\n"
                    "- N√ÉO mostre m√≠nimo/m√°ximo quando a pergunta √© sobre frequ√™ncia\n"
                    "- USE os n√∫meros EXATOS da tabela de chunks fornecidos\n\n"
                    "**Resposta:**"
                )
            
            # TIPO 6: Perguntas sobre CLUSTERING/AGRUPAMENTOS
            elif any(term in query_lower for term in ['cluster', 'clusters', 'agrupamento', 'agrupamentos', 'grupos', 'kmeans', 'k-means', 'dbscan', 'hier√°rquico', 'hierarquico', 'segmenta√ß√£o', 'segmentacao']):
                # üî¨ EXECU√á√ÉO REAL DE CLUSTERING usando PythonDataAnalyzer
                self.logger.info("üî¨ Detectada pergunta sobre clustering - executando an√°lise KMeans real...")
                
                try:
                    from src.tools.python_analyzer import python_analyzer
                    
                    # Executar clustering real nos dados
                    clustering_result = python_analyzer.calculate_clustering_analysis(n_clusters=3)
                    
                    if "error" in clustering_result:
                        # Se houve erro, informar ao usu√°rio
                        error_msg = clustering_result.get("error", "Erro desconhecido")
                        suggestion = clustering_result.get("suggestion", "")
                        
                        return (
                            f"Pergunta feita: {query}\n\n"
                            f"‚ùå **N√£o foi poss√≠vel realizar an√°lise de clustering:**\n"
                            f"{error_msg}\n\n"
                            f"{suggestion}\n\n"
                            "Se precisar de mais detalhes, √© s√≥ perguntar!"
                        )
                    
                    # Construir contexto enriquecido com resultados reais do clustering
                    clustering_context = clustering_result.get("interpretation", "")
                    cluster_distribution = clustering_result.get("cluster_distribution", {})
                    cluster_percentages = clustering_result.get("cluster_percentages", {})
                    numeric_vars = clustering_result.get("numeric_variables_used", [])
                    is_balanced = clustering_result.get("is_balanced", False)
                    
                    # Construir prompt com dados REAIS do clustering
                    system_prompt = (
                        "Voc√™ √© um agente EDA especializado em an√°lise de clustering. "
                        "Acabei de executar an√°lise de clustering KMeans REAL nos dados. "
                        "Sua tarefa √© apresentar os resultados de forma clara e estruturada. "
                        "Use APENAS os resultados reais fornecidos. N√ÉO invente n√∫meros."
                    )
                    
                    user_prompt = (
                        f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                        f"**RESULTADOS REAIS DO CLUSTERING EXECUTADO:**\n\n"
                        f"**Algoritmo:** KMeans com {clustering_result.get('n_clusters', 3)} clusters\n"
                        f"**Total de pontos analisados:** {clustering_result.get('total_points', 0):,}\n"
                        f"**Vari√°veis num√©ricas utilizadas:** {len(numeric_vars)} vari√°veis\n"
                        f"  - Exemplos: {', '.join(numeric_vars[:5])}{'...' if len(numeric_vars) > 5 else ''}\n\n"
                        f"**Distribui√ß√£o dos Clusters:**\n"
                    )
                    
                    # Adicionar distribui√ß√£o real dos clusters
                    for cluster_id in sorted(cluster_distribution.keys()):
                        count = cluster_distribution[cluster_id]
                        pct = cluster_percentages[cluster_id]
                        user_prompt += f"- Cluster {cluster_id}: {count:,} pontos ({pct:.1f}%)\n"
                    
                    user_prompt += f"\n**Balanceamento:** {'Clusters balanceados' if is_balanced else 'Clusters desbalanceados'}\n\n"
                    
                    user_prompt += (
                        f"**CHUNKS ANAL√çTICOS DO CSV (contexto adicional):**\n{context_data}\n\n"
                        "**FORMATO DE RESPOSTA OBRIGAT√ìRIO:**\n"
                        "Inicie com: 'Pergunta feita: [pergunta]'\n\n"
                        "Adicione: 'Para responder se h√° agrupamentos (clusters) nos dados, executei an√°lise de clustering KMeans:'\n\n"
                        "**üî¨ An√°lise de Clustering (KMeans):**\n\n"
                        "**Vari√°veis Utilizadas:**\n"
                        f"- {len(numeric_vars)} vari√°veis num√©ricas: {', '.join(numeric_vars[:8])}{'...' if len(numeric_vars) > 8 else ''}\n\n"
                        "**Resultado do Clustering (k=3):**\n"
                    )
                    
                    # Adicionar novamente para o LLM formatar
                    for cluster_id in sorted(cluster_distribution.keys()):
                        count = cluster_distribution[cluster_id]
                        pct = cluster_percentages[cluster_id]
                        user_prompt += f"- Cluster {cluster_id}: {count:,} pontos ({pct:.1f}%)\n"
                    
                    user_prompt += (
                        "\n**‚úÖ Conclus√£o:**\n"
                        f"- SIM, os dados apresentam {len(cluster_distribution)} agrupamentos distintos\n"
                        f"- Os clusters s√£o {'balanceados' if is_balanced else 'desbalanceados'}\n"
                        "- [Adicione insight interpretativo sobre o significado desses agrupamentos]\n\n"
                        "**üí° Recomenda√ß√µes:**\n"
                        "- Para visualizar os clusters, pergunte: 'mostre gr√°fico de dispers√£o dos clusters'\n"
                        "- Para an√°lise PCA 2D/3D, pergunte: 'aplique PCA nos dados'\n"
                        "- Para estat√≠sticas por cluster, pergunte: 'qual a m√©dia de cada cluster?'\n\n"
                        "Finalize: 'Se desejar aprofundar na an√°lise de clustering, √© s√≥ perguntar!'\n\n"
                        "**Resposta:**"
                    )
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao executar clustering: {str(e)}", exc_info=True)
                    # Fallback: usar prompt gen√©rico informando limita√ß√£o
                    system_prompt = (
                        "Voc√™ √© um agente EDA especializado. "
                        "Houve um erro t√©cnico ao tentar executar an√°lise de clustering real nos dados. "
                        "Explique ao usu√°rio que a an√°lise de clustering requer execu√ß√£o de algoritmos espec√≠ficos."
                    )
                    user_prompt = (
                        f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                        f"**STATUS:** Erro t√©cnico ao executar KMeans: {str(e)}\n\n"
                        "**INSTRU√á√ïES DE RESPOSTA:**\n"
                        "- Informe que houve uma limita√ß√£o t√©cnica tempor√°ria\n"
                        "- Explique que clustering requer execu√ß√£o de algoritmos (KMeans, DBSCAN, etc.)\n"
                        "- Sugira tentar novamente ou perguntar sobre outras an√°lises\n\n"
                        "**Resposta:**"
                    )

            
            # TIPO 7: Query gen√©rica - incluir hist√≥rico de conversa
            else:
                # Query gen√©rica - incluir hist√≥rico de conversa
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Responda √† pergunta do usu√°rio usando os chunks anal√≠ticos fornecidos E o hist√≥rico da conversa quando relevante. "
                    "Seja claro, objetivo, estruturado e humanizado. N√£o invente dados ou informa√ß√µes."
                )
                user_prompt = (
                    f"{history_context}"  # INCLUIR hist√≥rico aqui!
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    f"**CHUNKS ANAL√çTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Use as informa√ß√µes dos chunks fornecidos E o hist√≥rico da conversa quando relevante\n"
                    "- Se a pergunta se refere a algo mencionado anteriormente, considere o contexto\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Responda de forma clara, humanizada e estruturada\n"
                    "- Se n√£o houver informa√ß√£o suficiente, informe claramente\n"
                    "- Finalize com: 'Se precisar de mais detalhes, √© s√≥ perguntar!'\n\n"
                    "**Resposta:**"
                )
            
            # Usar LangChain LLM se dispon√≠vel
            if self.llm and LANGCHAIN_AVAILABLE:
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]
                
                response = await asyncio.to_thread(self.llm.invoke, messages)
                return response.content
            
            # Fallback: usar LLM Manager customizado
            else:
                from src.llm.manager import LLMManager, LLMConfig
                llm_manager = LLMManager()
                
                # Construir prompt √∫nico (LLMManager.chat espera string, n√£o messages)
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                llm_response = llm_manager.chat(
                    full_prompt,
                    config=LLMConfig(
                        temperature=0.3,
                        max_tokens=2000
                    )
                )
                
                # Verificar se houve erro (LLMResponse tem atributo .error)
                if not llm_response.success or llm_response.error:
                    return self._format_raw_data_response(query, chunks_metadata)
                
                # Extrair conte√∫do (LLMResponse tem atributo .content)
                response_text = llm_response.content
                
                if not response_text:
                    return self._format_raw_data_response(query, chunks_metadata)
                
                return response_text
            
        except Exception as e:
            self.logger.error(f"Erro ao gerar resposta LLM: {str(e)}", exc_info=True)
            return self._format_raw_data_response(query, chunks_metadata)
    
    def _format_raw_data_response(
        self,
        query: str,
        chunks_metadata: List[Dict]
        ) -> str:
        """
        Fallback: usa agente de s√≠ntese para consolidar dados se LLM falhar.
        """
        # Extrair apenas o texto dos chunks
        chunks = [chunk.get('chunk_text', '') for chunk in chunks_metadata]
        
        # Chamar agente de s√≠ntese para consolidar
        from src.agent.rag_synthesis_agent import synthesize_response
        try:
            return synthesize_response(chunks, query, use_llm=False)
        except Exception as e:
            self.logger.error(f"Erro no agente de s√≠ntese: {e}")
            # Fallback extremo: resposta estruturada m√≠nima
            return f"""## Resposta para: {query}

**Status:** ‚ö†Ô∏è Erro na s√≠ntese

N√£o foi poss√≠vel processar completamente a consulta devido a um erro t√©cnico.
Por favor, reformule sua pergunta ou entre em contato com o suporte.

_Erro: {str(e)}_"""
    
    def _build_error_response(self, error_msg: str) -> Dict[str, Any]:
        """Constr√≥i resposta de erro padronizada."""
        return self._build_response(
            f"‚ùå {error_msg}",
            metadata={"error": True, "method": "rag_vectorial_v2"}
        )
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTODO S√çNCRONO WRAPPER (para compatibilidade retroativa)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def process_sync(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Wrapper s√≠ncrono para compatibilidade com c√≥digo legado.
        
        ‚ö†Ô∏è DEPRECATED: Use process() async quando poss√≠vel.
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.process(query, context))
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTODO DE CARREGAMENTO CSV (mantido da vers√£o anterior)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    async def load_csv_to_embeddings(
        self,
        csv_path: str,
        chunk_size: int = 1000,
        overlap: int = 100
    ) -> Dict[str, Any]:
        """
        Carrega CSV para a tabela embeddings.
        
        Args:
            csv_path: Caminho do arquivo CSV
            chunk_size: Tamanho dos chunks
            overlap: Overlap entre chunks
            
        Returns:
            Status do carregamento
        """
        try:
            self.logger.info(f"üìÇ Carregando CSV: {csv_path}")
            
            import pandas as pd
            from src.embeddings.chunker import CSVChunker
            
            # Ler CSV
            df = pd.read_csv(csv_path)
            self.logger.info(f"‚úÖ CSV lido: {len(df)} linhas, {len(df.columns)} colunas")
            
            # Criar chunks
            chunker = CSVChunker(chunk_size=chunk_size, overlap=overlap)
            chunks = chunker.chunk_dataframe(df)
            self.logger.info(f"‚úÖ Criados {len(chunks)} chunks")
            
            # Gerar embeddings e salvar
            inserted_count = 0
            for i, chunk in enumerate(chunks):
                try:
                    # Gerar embedding
                    embedding = self.embedding_gen.generate_embedding(chunk['text'])
                    
                    # Salvar na tabela embeddings
                    insert_data = {
                        'chunk_text': chunk['text'],
                        'embedding': embedding,
                        'metadata': {
                            'source': csv_path,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'created_at': datetime.now().isoformat()
                        }
                    }
                    
                    result = supabase.table('embeddings').insert(insert_data).execute()
                    
                    if result.data:
                        inserted_count += 1
                        if (i + 1) % 10 == 0:
                            self.logger.info(f"Progresso: {i+1}/{len(chunks)} chunks inseridos")
                
                except Exception as chunk_error:
                    self.logger.warning(f"Erro no chunk {i}: {chunk_error}")
                    continue
            
            self.logger.info(f"‚úÖ Carregamento conclu√≠do: {inserted_count}/{len(chunks)} chunks inseridos")
            
            return self._build_response(
                f"‚úÖ CSV carregado com sucesso: {inserted_count} chunks inseridos na base vetorial",
                metadata={
                    'csv_path': csv_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'chunks_created': len(chunks),
                    'chunks_inserted': inserted_count
                }
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar CSV: {str(e)}")
            return self._build_error_response(f"Falha ao carregar CSV: {str(e)}")
