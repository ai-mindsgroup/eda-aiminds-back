"""
Agente de An√°lise de Dados via RAG Vetorial Puro com Mem√≥ria Persistente e LangChain.

VERS√ÉO 2.0 - REFATORADA:
- ‚úÖ Mem√≥ria persistente em Supabase (tabelas agent_sessions, agent_conversations, agent_context)
- ‚úÖ LangChain integrado nativamente (ChatOpenAI, ChatGoogleGenerativeAI)
- ‚úÖ M√©todos async para performance
- ‚úÖ Contexto conversacional entre intera√ß√µes
- ‚úÖ Busca vetorial pura (sem keywords hardcoded)

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚ö†Ô∏è  EXCE√á√ÉO DE CONFORMIDADE: ACESSO DIRETO A CSV PARA VISUALIZA√á√ïES
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

CONTEXTO:
- Tabela 'embeddings' armazena chunks de an√°lises estat√≠sticas (Markdown)
- Visualiza√ß√µes (histogramas) requerem dados tabulares completos (285k linhas)
- Embeddar cada linha seria ineficiente: ~$50-100 custo + overhead desnecess√°rio

SOLU√á√ÉO IMPLEMENTADA:
- Quando visualiza√ß√£o √© solicitada, acessa CSV diretamente via pd.read_csv()
- Acesso √© READ-ONLY, sem modifica√ß√£o de dados
- Log completo de auditoria registrado (linhas 318-350)
- Metadados de conformidade inclu√≠dos em todas as respostas

JUSTIFICATIVA (ADERENTE A BOAS PR√ÅTICAS DE MERCADO):
1. Padr√£o da ind√∫stria: LangChain CSV Agents, LlamaIndex, OpenAI Code Interpreter
2. Separa√ß√£o de responsabilidades: RAG para busca sem√¢ntica, CSV para dados tabulares
3. Custo-benef√≠cio: evita armazenamento/processamento desnecess√°rio
4. Performance: leitura direta √© mais r√°pida que reconstitui√ß√£o de embeddings

IMPLEMENTA√á√ÉO FUTURA (Opcional):
- TODO: Adicionar chunks 'raw_data' na tabela embeddings durante ingest√£o
- TODO: Implementar reconstitui√ß√£o de DataFrame a partir de embeddings
- TODO: Adicionar configura√ß√£o para escolher entre direct-access vs embeddings

AUDITORIA E COMPLIANCE:
- ‚úÖ Log detalhado com event_type, timestamp, session_id, csv_path, size
- ‚úÖ Metadados em response.metadata['conformidade_exception']
- ‚úÖ Documenta√ß√£o clara da exce√ß√£o e justificativa
- ‚úÖ Aprova√ß√£o registrada (approved=True)

REFER√äNCIAS:
- LangChain CSV Agent: https://python.langchain.com/docs/integrations/toolkits/csv
- OpenAI Code Interpreter: https://openai.com/blog/code-interpreter
- Hybrid RAG Architectures: https://docs.llamaindex.ai/en/stable/examples/query_engine/

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
"""

import logging
from typing import Any, Dict, List, Optional
import json
from datetime import datetime
import asyncio
import pandas as pd

from src.agent.base_agent import BaseAgent, AgentError
from src.vectorstore.supabase_client import supabase
from src.embeddings.generator import EmbeddingGenerator
from src.utils.logging_config import get_logger
from src.analysis.intent_classifier import IntentClassifier, AnalysisIntent
from src.analysis.orchestrator import AnalysisOrchestrator

# üîí SPRINT 3 P0-4: Import do Sandbox Seguro (RestrictedPython)
# Substitui PythonREPLTool inseguro detectado no Sprint 2
from src.security.sandbox import execute_in_sandbox

# ‚úÖ V4.0: Imports para prompts din√¢micos e configura√ß√µes otimizadas
from src.prompts.dynamic_prompts import DynamicPromptGenerator, DatasetContext
from src.llm.optimized_config import get_configs_for_intent, LLMOptimizedConfig, RAGOptimizedConfig

# Imports LangChain
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    # üîí REMOVIDO: PythonREPLTool (vulnerabilidade RCE cr√≠tica)
    # from langchain_experimental.tools import PythonREPLTool
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    AIMessage = None
    print(f"‚ö†Ô∏è LangChain n√£o dispon√≠vel: {e}")


class RAGDataAgent(BaseAgent):
    def _interpretar_pergunta_llm(self, pergunta: str, df):
        """
        ‚úÖ V3.0: Utiliza IntentClassifier para classifica√ß√£o sem√¢ntica SEM HARD-CODING.
        
        Retorna instru√ß√µes anal√≠ticas baseadas na inten√ß√£o classificada pela LLM.
        Cada instru√ß√£o √© um dict: {'acao': ..., 'colunas': [...], 'params': {}, 'justificativa': str}
        """
        if not self.llm:
            # Fallback: retorna instru√ß√£o gen√©rica
            return [{'acao': 'estat√≠sticas gerais', 'colunas': list(df.columns), 'params': {}, 
                    'justificativa': 'LLM indispon√≠vel, fornecendo estat√≠sticas gerais.'}]
        
        try:
            # üî• V3.0: Usar IntentClassifier para classifica√ß√£o sem√¢ntica
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            # Classificar inten√ß√£o da pergunta
            context = {
                'available_columns': list(df.columns),
                'dataframe_info': f"Shape: {df.shape}, Colunas num√©ricas: {df.select_dtypes(include=['number']).columns.tolist()}"
            }
            
            classification_result = classifier.classify(query=pergunta, context=context)
            
            # Log da classifica√ß√£o
            self.logger.info({
                'event': 'intent_classification',
                'primary_intent': classification_result.primary_intent.value,
                'secondary_intents': [intent.value for intent in classification_result.secondary_intents],
                'confidence': classification_result.confidence,
                'reasoning': classification_result.reasoning
            })
            
            # üéØ Mapear inten√ß√µes para instru√ß√µes anal√≠ticas
            instrucoes = []
            
            # Processar inten√ß√£o prim√°ria
            primary_action = self._intent_to_action(
                classification_result.primary_intent, 
                df, 
                classification_result.reasoning
            )
            if primary_action:
                instrucoes.append(primary_action)
            
            # Processar inten√ß√µes secund√°rias se existirem
            for secondary_intent in classification_result.secondary_intents:
                secondary_action = self._intent_to_action(
                    secondary_intent, 
                    df, 
                    f"Inten√ß√£o secund√°ria detectada: {secondary_intent.value}"
                )
                if secondary_action and secondary_action not in instrucoes:
                    instrucoes.append(secondary_action)
            
            # Garantir pelo menos uma instru√ß√£o
            if not instrucoes:
                instrucoes = [{
                    'acao': 'estat√≠sticas gerais',
                    'colunas': list(df.columns),
                    'params': {},
                    'justificativa': 'Nenhuma inten√ß√£o espec√≠fica detectada, fornecendo vis√£o geral.'
                }]
            
            self.logger.info({
                'event': 'instructions_generated',
                'num_instructions': len(instrucoes),
                'actions': [ins['acao'] for ins in instrucoes]
            })
            
            return instrucoes
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao interpretar pergunta com IntentClassifier: {e}")
            # Fallback para interpreta√ß√£o b√°sica via LLM direta
            return self._fallback_interpretation(pergunta, df)
    
    def _intent_to_action(self, intent: AnalysisIntent, df, justificativa: str) -> Optional[Dict]:
        """
        üéØ Converte AnalysisIntent em instru√ß√£o anal√≠tica.
        Mapeia tipos de inten√ß√£o para a√ß√µes espec√≠ficas.
        """
        # Selecionar colunas num√©ricas por padr√£o
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        all_cols = list(df.columns)
        
        intent_map = {
            AnalysisIntent.STATISTICAL: {
                'acao': 'estat√≠sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.FREQUENCY: {
                'acao': 'frequency_analysis',
                'colunas': all_cols,
                'params': {'top_n': 10},
                'justificativa': justificativa
            },
            AnalysisIntent.TEMPORAL: {
                'acao': 'temporal_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.CLUSTERING: {
                'acao': 'clustering_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {'n_clusters': 3},
                'justificativa': justificativa
            },
            AnalysisIntent.CORRELATION: {
                'acao': 'correlation_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.OUTLIERS: {
                'acao': 'outlier_detection',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.COMPARISON: {
                'acao': 'comparison_analysis',
                'colunas': all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.VISUALIZATION: {
                'acao': 'visualization_request',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.GENERAL: {
                'acao': 'estat√≠sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            }
        }
        
        return intent_map.get(intent)
    
    def _fallback_interpretation(self, pergunta: str, df) -> List[Dict]:
        """
        Fallback: interpreta√ß√£o b√°sica via LLM direta quando IntentClassifier falha.
        Mant√©m apenas l√≥gica essencial, SEM hard-coding de keywords.
        """
        prompt = (
            "Voc√™ √© um especialista em an√°lise de dados.\n"
            "Interprete a pergunta do usu√°rio e retorne uma lista JSON de instru√ß√µes anal√≠ticas.\n"
            "Formato de cada instru√ß√£o: {'acao': str, 'colunas': [str], 'params': {}, 'justificativa': str}\n"
            f"Pergunta: {pergunta}\n"
            f"Colunas dispon√≠veis: {list(df.columns)}\n"
            "Responda APENAS com o JSON, sem explica√ß√µes."
        )
        
        try:
            response = self.llm.invoke(prompt)
            import json
            instrucoes = json.loads(response.content)
            return instrucoes if isinstance(instrucoes, list) else [instrucoes]
        except Exception as e:
            self.logger.error(f"Fallback interpretation falhou: {e}")
            return [{
                'acao': 'estat√≠sticas gerais',
                'colunas': list(df.columns),
                'params': {},
                'justificativa': 'Interpreta√ß√£o padr√£o devido a erro.'
            }]
    
    def _build_analytical_response_v3(
        self,
        query: str,
        df: pd.DataFrame,
        context_data: str,
        history_context: str = ""
    ) -> str:
        """
        üî• V3.0: Constr√≥i resposta anal√≠tica usando AnalysisOrchestrator.
        
        Substitui ~240 linhas de cascata if/elif por orquestra√ß√£o inteligente.
        
        Args:
            query: Pergunta do usu√°rio
            df: DataFrame carregado
            context_data: Chunks anal√≠ticos do CSV
            history_context: Hist√≥rico conversacional
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            self.logger.info("üî• Usando V3.0: AnalysisOrchestrator")
            
            # Classificar inten√ß√£o via IntentClassifier
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            context_info = {
                'available_columns': list(df.columns),
                'dataframe_shape': df.shape,
                'has_history': bool(history_context)
            }
            
            intent_result = classifier.classify(query, context=context_info)
            
            # Converter IntentClassificationResult para dict de confian√ßa
            intent_dict = {intent_result.primary_intent.value.upper(): intent_result.confidence}
            
            # Adicionar inten√ß√µes secund√°rias
            for secondary in intent_result.secondary_intents:
                intent_dict[secondary.value.upper()] = 0.75  # Confian√ßa padr√£o para secund√°rias
            
            self.logger.info(f"Inten√ß√µes detectadas: {intent_dict}")
            
            # Criar orquestrador e executar an√°lises
            orchestrator = AnalysisOrchestrator(llm=self.llm, logger=self.logger)
            
            orchestration_result = orchestrator.orchestrate_v3_direct(
                intent_result=intent_dict,
                df=df,
                confidence_threshold=0.6
            )
            
            # Construir resposta formatada
            response = self._format_orchestrated_response(
                query=query,
                orchestration_result=orchestration_result,
                context_data=context_data,
                history_context=history_context,
                intent_result=intent_result
            )
            
            return response
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no V3 orchestrator: {e}", exc_info=True)
            # Fallback para resposta b√°sica
            return self._fallback_basic_response(query, context_data, history_context)
    
    def _format_orchestrated_response(
        self,
        query: str,
        orchestration_result: Dict[str, Any],
        context_data: str,
        history_context: str,
        intent_result
    ) -> str:
        """
        Formata resultado orquestrado em resposta humanizada.
        
        Args:
            query: Pergunta original
            orchestration_result: Resultado do orchestrator.orchestrate_v3_direct()
            context_data: Chunks anal√≠ticos
            history_context: Hist√≥rico
            intent_result: Classifica√ß√£o de inten√ß√£o
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            # Construir prompt para LLM formatar resposta final
            from langchain.schema import SystemMessage, HumanMessage
            
            system_prompt = """
Voc√™ √© um agente EDA especializado. Sua tarefa √© apresentar resultados anal√≠ticos de forma clara e estruturada.

Voc√™ receber√°:
1. Pergunta do usu√°rio
2. Resultados de an√°lises executadas (JSON estruturado)
3. Chunks anal√≠ticos do CSV (contexto adicional)
4. Hist√≥rico conversacional (se houver)

Sua resposta deve:
- Iniciar com: "Pergunta feita: [pergunta]"
- Apresentar resultados de forma humanizada e estruturada
- Usar tabelas Markdown quando apropriado
- Destacar insights relevantes
- Finalizar com: "Se precisar de mais detalhes, √© s√≥ perguntar!"
"""
            
            results_summary = []
            for analyzer_name, result in orchestration_result.get('results', {}).items():
                if isinstance(result, dict) and 'error' not in result:
                    results_summary.append(f"**{analyzer_name.title()}**: An√°lise executada com sucesso")
                elif isinstance(result, dict) and 'error' in result:
                    results_summary.append(f"**{analyzer_name.title()}**: Erro - {result['error']}")
            
            user_prompt = f"""
**Pergunta do Usu√°rio:**
{query}

{history_context}

**Resultados das An√°lises Executadas:**
{chr(10).join(results_summary)}

**Detalhes Completos (JSON):**
```json
{json.dumps(orchestration_result.get('results', {}), indent=2, ensure_ascii=False)}
```

**Chunks Anal√≠ticos do CSV (contexto adicional):**
{context_data[:1000]}...

**Inten√ß√£o Detectada:**
- Principal: {intent_result.primary_intent.value}
- Confian√ßa: {intent_result.confidence:.1%}
- Justificativa: {intent_result.reasoning}

Apresente os resultados de forma clara, humanizada e estruturada.
"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            return response.content
            
        except Exception as e:
            self.logger.error(f"Erro ao formatar resposta orquestrada: {e}")
            # Fallback: retornar JSON bruto formatado
            import json
            return f"""
Pergunta feita: {query}

**An√°lises Executadas:**
{chr(10).join([f"- {k}" for k in orchestration_result.get('results', {}).keys()])}

**Resultados:**
```json
{json.dumps(orchestration_result, indent=2, ensure_ascii=False)}
```

Se precisar de mais detalhes, √© s√≥ perguntar!
"""
    
    def _fallback_basic_response(
        self,
        query: str,
        context_data: str,
        history_context: str
    ) -> str:
        """
        Fallback b√°sico quando V3 orchestrator falha completamente.
        """
        from langchain.schema import SystemMessage, HumanMessage
        
        system_prompt = "Voc√™ √© um agente EDA. Responda √† pergunta usando os chunks fornecidos."
        
        user_prompt = f"""
Pergunta: {query}

{history_context}

Chunks anal√≠ticos:
{context_data}

Responda de forma clara e estruturada.
"""
        
        try:
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            return f"Erro ao processar an√°lise: {str(e)}"

    def _executar_instrucao(self, df, instrucao):
        """
        Executa uma instru√ß√£o anal√≠tica sobre o DataFrame.
        Suporta m√©tricas nativas pandas/NumPy e delega compostas √† LLM.
        """
        acao = instrucao.get('acao','')
        colunas = instrucao.get('colunas', list(df.columns))
        params = instrucao.get('params', {})
        try:
            # Normalizar nome da a√ß√£o para compara√ß√£o (tolerante a mai√∫sculas/min√∫sculas)
            acao_norm = str(acao).strip().lower()
            # M√©tricas nativas pandas
            if acao_norm in ('m√©dia', 'media', 'mean'):
                return df[colunas].mean().to_frame(name='M√©dia')
            if acao_norm in ('mediana', 'median'):
                return df[colunas].median().to_frame(name='Mediana')
            if acao_norm in ('moda', 'mode'):
                return df[colunas].mode().T
            if acao_norm in ('desvio padr√£o', 'desvio padrao', 'std', 'standard deviation'):
                return df[colunas].std().to_frame(name='Desvio padr√£o')
            if acao_norm in ('vari√¢ncia', 'variancia', 'variance', 'var'):
                # Vari√¢ncia populacional/por padr√£o pandas var() usa ddof=1 (amostral). Mantemos default.
                return df[colunas].var().to_frame(name='Vari√¢ncia')
            if acao_norm in ('intervalo', 'minmax', 'min_max', 'm√≠nimo', 'm√°ximo'):
                resultado = df[colunas].agg(['min','max']).T
                resultado.columns = ['M√≠nimo','M√°ximo']
                return resultado
            if acao_norm in ('estat√≠sticas gerais', 'estatisticas gerais', 'describe', 'summary', 'resumo'):
                return df[colunas].describe().T
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üîí SPRINT 3 P0-4: M√âTRICAS COMPOSTAS VIA SANDBOX SEGURO
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # M√©tricas complexas que requerem c√≥digo din√¢mico: delega √† LLM + SANDBOX
            if self.llm:
                try:
                    # Prompt estruturado para gera√ß√£o de c√≥digo anal√≠tico
                    prompt = (
                        f"Receba instru√ß√£o anal√≠tica: {instrucao}.\n"
                        f"DataFrame j√° est√° dispon√≠vel como vari√°vel 'df'.\n"
                        f"Colunas dispon√≠veis: {list(df.columns)}.\n"
                        f"Colunas num√©ricas: {df.select_dtypes(include=['number']).columns.tolist()}.\n"
                        "Gere c√≥digo Python (pandas/numpy) que:\n"
                        "1. Execute a m√©trica/an√°lise pedida\n"
                        "2. Armazene o resultado em uma vari√°vel chamada 'resultado'\n"
                        "3. O resultado deve ser um DataFrame, Series ou valor escalar\n"
                        "4. Use apenas bibliotecas permitidas: pandas, numpy, math, statistics, datetime\n"
                        "Retorne APENAS o c√≥digo, sem explica√ß√µes, sem markdown, sem coment√°rios.\n"
                        "Exemplo v√°lido: resultado = df['coluna'].mean()\n"
                        "Exemplo v√°lido: resultado = df.groupby('categoria')['valor'].sum()"
                    )
                    
                    # Gerar c√≥digo via LLM
                    response = self.llm.invoke(prompt)
                    code = response.content.strip()
                    
                    # Limpar markdown code blocks se presentes
                    if code.startswith("```python"):
                        code = code.split("```python")[1].split("```")[0].strip()
                    elif code.startswith("```"):
                        code = code.split("```")[1].split("```")[0].strip()
                    
                    # Remover coment√°rios inline (seguran√ßa: evitar inje√ß√£o via coment√°rios)
                    code_lines = [line for line in code.split('\n') if not line.strip().startswith('#')]
                    code = '\n'.join(code_lines)
                    
                    # Log c√≥digo antes de executar (auditoria de seguran√ßa)
                    self.logger.info({
                        'event': 'llm_code_generation',
                        'instrucao': str(instrucao)[:100],
                        'code_generated': code[:200] + ('...' if len(code) > 200 else ''),
                        'code_length': len(code),
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    # üîí EXECU√á√ÉO SEGURA VIA SANDBOX (RestrictedPython)
                    # Substitui PythonREPLTool vulner√°vel (Sprint 2)
                    sandbox_result = self._executar_codigo_sandbox(
                        code=code,
                        df=df,
                        timeout_seconds=5,
                        memory_limit_mb=100
                    )
                    
                    # Processar resultado do sandbox
                    if sandbox_result.get('success'):
                        resultado = sandbox_result.get('result')
                        exec_time = sandbox_result.get('execution_time_ms', 0)
                        
                        self.logger.info({
                            'event': 'sandbox_execution_success',
                            'execution_time_ms': exec_time,
                            'result_type': type(resultado).__name__,
                            'result_shape': resultado.shape if hasattr(resultado, 'shape') else None,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        return resultado
                    else:
                        # Erro na execu√ß√£o sandbox
                        error_msg = sandbox_result.get('error', 'Erro desconhecido')
                        error_type = sandbox_result.get('error_type', 'UnknownError')
                        
                        self.logger.error({
                            'event': 'sandbox_execution_failed',
                            'error_type': error_type,
                            'error_message': error_msg[:200],
                            'code_attempted': code[:200],
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        # Log detalhado para debugging
                        self.logger.debug(f"C√≥digo problem√°tico:\n{code}")
                        self.logger.debug(f"Logs do sandbox: {sandbox_result.get('logs', [])}")
                        
                        return None
                    
                except Exception as e:
                    self.logger.error({
                        'event': 'llm_code_execution_error',
                        'error': str(e),
                        'exception_type': type(e).__name__,
                        'instrucao': str(instrucao)[:100],
                        'timestamp': datetime.now().isoformat()
                    })
                    self.logger.debug(f"C√≥digo problem√°tico: {code if 'code' in locals() else 'N/A'}")
                    return None
            else:
                self.logger.warning({
                    'event': 'llm_unavailable_for_complex_metrics',
                    'instrucao': str(instrucao)[:100],
                    'timestamp': datetime.now().isoformat()
                })
                return None
        except Exception as e:
            self.logger.warning(f"Falha ao executar instru√ß√£o: {instrucao} | Erro: {e}")
            return None

    def _formatar_taxonomia_tipos(self, tipos_dict: dict) -> str:
        """
        Formata dicion√°rio de tipos de dados em texto estruturado para o prompt da LLM.
        
        Args:
            tipos_dict: Dicion√°rio com tipos comuns e suas caracter√≠sticas
            
        Returns:
            String formatada em markdown para inclus√£o no prompt
        """
        texto = ""
        for tipo_key, tipo_info in tipos_dict.items():
            tipo_nome = tipo_key.replace('_', ' ').title()
            texto += f"""
**{tipo_nome}:**
- Descri√ß√£o: {tipo_info['descricao']}
- Exemplos de nomes: {', '.join(tipo_info['exemplos'])}
- Indicadores: {', '.join(tipo_info['indicadores'])}
"""
        return texto

    def _analisar_completo_csv(self, csv_path: str, pergunta: str, override_temporal_col: str = None,
                               temporal_col_names: list = None, accepted_types: tuple = None) -> str:
        """
        ‚úÖ V3.0: An√°lise inteligente via LLM (SEM LIMITA√á√ÉO, SEM HARDCODING).
        
        A LLM decide dinamicamente:
        - Se resposta deve ser concisa ou detalhada
        - Quais colunas analisar
        - Como interpretar tipos (ex: Class como booleano disfar√ßado de int)
        
        ARQUITETURA MODULAR:
        - Detec√ß√£o via TemporalColumnDetector (src/analysis/temporal_detection.py) - OPCIONAL
        - An√°lise via LLM para TODAS as colunas (gen√©rico, n√£o limitado)
        - Resposta adaptada √† complexidade da pergunta
        
        Par√¢metros:
            - override_temporal_col: for√ßa uso de coluna espec√≠fica (ou None para auto)
            - temporal_col_names: lista de nomes comuns (default: ["time", "date", "timestamp", "data", "datetime"])
            - accepted_types: DEPRECATED - mantido para backward compatibility
            
        Returns:
            String formatada em Markdown com an√°lise adaptada √† pergunta
        """
        import pandas as pd
        from src.analysis.temporal_detection import TemporalColumnDetector, TemporalDetectionConfig
        from src.analysis.temporal_analyzer import TemporalAnalyzer
        
        # Carregar dados
        df = pd.read_csv(csv_path)
        
        logger = self.logger if hasattr(self, 'logger') else logging.getLogger(__name__)
        logger.info({
            'event': 'inicio_analise_csv_v3',
            'csv_path': csv_path,
            'shape': df.shape,
            'pergunta': pergunta,
            'override_temporal_col': override_temporal_col
        })
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ‚úÖ V3.0: LLM INTERPRETA A PERGUNTA E GERA RESPOSTA ADAPTADA
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # Detectar se pergunta √© SIMPLES (tipos de dados) ou COMPLEXA (an√°lise detalhada)
        pergunta_lower = pergunta.lower()
        keywords_simples = [
            'quais tipos', 'tipos de dados', 'tipo de dado', 'tipos das colunas',
            'colunas num√©ricas', 'colunas categ√≥ricas', 'colunas temporais',
            'data types', 'column types', 'tipos das vari√°veis'
        ]
        
        is_simple_query = any(keyword in pergunta_lower for keyword in keywords_simples)
        
        if is_simple_query:
            logger.info("üìã Pergunta SIMPLES detectada: LLM interpretar√° dados e responder√° de forma humanizada")
            
            # Coletar informa√ß√µes COMPLETAS de TODAS as colunas para an√°lise pela LLM
            colunas_info = []
            
            for col in df.columns:
                col_data = df[col]
                dtype = str(col_data.dtype)
                unique_count = col_data.nunique()
                null_count = col_data.isnull().sum()
                sample_values = col_data.dropna().head(10).tolist()
                
                # Estat√≠sticas b√°sicas para LLM analisar
                info = {
                    'nome': col,
                    'dtype_python': dtype,
                    'valores_unicos': unique_count,
                    'valores_nulos': null_count,
                    'amostra_valores': sample_values,
                    'total_linhas': len(col_data)
                }
                
                # Adicionar estat√≠sticas num√©ricas se aplic√°vel
                if pd.api.types.is_numeric_dtype(col_data):
                    info['min'] = float(col_data.min())
                    info['max'] = float(col_data.max())
                    info['media'] = float(col_data.mean())
                    info['std'] = float(col_data.std())
                
                colunas_info.append(info)
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üß† PROMPT ENGINEERING: Sistema Adaptativo de Classifica√ß√£o de Tipos
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            # üìã CONFIGURA√á√ÉO: Tipos de dados comuns (extens√≠vel via config)
            TIPOS_COMUNS = {
                'temporal': {
                    'descricao': 'Dados relacionados a tempo, data, ou marcadores temporais',
                    'exemplos': ['Time', 'Date', 'Timestamp', 'Duration', 'Year', 'Month'],
                    'indicadores': ['representam momentos, per√≠odos ou dura√ß√µes']
                },
                'categorica': {
                    'descricao': 'Dados discretos que representam categorias, classes ou grupos',
                    'exemplos': ['Class', 'Category', 'Type', 'Label', 'Status', 'Gender'],
                    'indicadores': ['valores distintos limitados', 'categorias predefinidas', 'classes binomiais ou multinomiais']
                },
                'numerica_continua': {
                    'descricao': 'Valores num√©ricos cont√≠nuos usados para medi√ß√µes ou c√°lculos',
                    'exemplos': ['Amount', 'Price', 'Temperature', 'Distance', 'Weight'],
                    'indicadores': ['medi√ß√µes', 'quantidades', 'valores monet√°rios', 'm√©tricas']
                },
                'numerica_discreta': {
                    'descricao': 'Valores num√©ricos inteiros representando contagens',
                    'exemplos': ['Count', 'Quantity', 'Age', 'ID', 'Rank'],
                    'indicadores': ['contagens', 'n√∫meros inteiros', 'identificadores sequenciais']
                },
                'booleana': {
                    'descricao': 'Valores l√≥gicos verdadeiro/falso (literal)',
                    'exemplos': ['is_active', 'has_discount', 'True/False'],
                    'indicadores': ['apenas valores True/False', 'n√£o confundir com categ√≥ricas bin√°rias 0/1']
                },
                'textual': {
                    'descricao': 'Texto livre ou strings descritivas',
                    'exemplos': ['Description', 'Comment', 'Name', 'Address'],
                    'indicadores': ['texto longo', 'descri√ß√µes', 'nomes pr√≥prios']
                },
                'mista': {
                    'descricao': 'Colunas com tipos mistos ou dados heterog√™neos',
                    'exemplos': ['mixed_data', 'various_formats'],
                    'indicadores': ['m√∫ltiplos tipos no mesmo campo', 'dados inconsistentes']
                }
            }
            
            # ‚úÖ USAR LLM COM INTELIG√äNCIA TOTAL E PROMPT SOFISTICADO
            if self.llm and LANGCHAIN_AVAILABLE:
                try:
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # PARTE 1: SISTEMA - Define o dom√≠nio e capacidades do agente
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    system_prompt = f"""Voc√™ √© um cientista de dados s√™nior especializado em an√°lise explorat√≥ria de dados (EDA).

**SUAS CAPACIDADES:**
- An√°lise sem√¢ntica profunda de estruturas de dados
- Classifica√ß√£o adaptativa de tipos de dados
- Interpreta√ß√£o contextual de nomes de colunas
- Detec√ß√£o de padr√µes estat√≠sticos em amostras
- Identifica√ß√£o de tipos n√£o convencionais ou mistos

**DOM√çNIO DO PROBLEMA:**
Voc√™ analisar√° colunas de um dataset CSV e classificar√° seus tipos de dados de forma inteligente, considerando:
1. **Contexto sem√¢ntico**: nome da coluna e significado no dom√≠nio
2. **Estrutura dos dados**: dtype Python, valores √∫nicos, distribui√ß√£o
3. **Amostras reais**: padr√µes observados nos valores
4. **Cardinalidade**: quantidade de valores distintos vs total de registros

**TAXONOMIA DE TIPOS (n√£o limitada a estes):**
Os tipos comuns incluem:

{self._formatar_taxonomia_tipos(TIPOS_COMUNS)}

**IMPORTANTE - ADAPTABILIDADE:**
- Esta lista N√ÉO √© exaustiva. Voc√™ pode identificar tipos h√≠bridos, especializados ou at√≠picos
- Se encontrar um tipo que n√£o se encaixa perfeitamente, descreva-o com suas pr√≥prias palavras
- Priorize o SIGNIFICADO CONTEXTUAL sobre o dtype t√©cnico
- Para dados bin√°rios (0/1), avalie se s√£o categ√≥ricos (classes) ou num√©ricos (contadores)

**REGRAS DE OURO:**
1. **Time/Timestamp num√©rico** ‚Üí Classifique como TEMPORAL (n√£o num√©rico)
2. **Bin√°rios 0/1 como categorias** ‚Üí Classifique como CATEG√ìRICA BIN√ÅRIA (n√£o booleana)
3. **Booleana** ‚Üí Reserve APENAS para True/False literal
4. **IDs num√©ricos sequenciais** ‚Üí Podem ser IDENTIFICADORES (n√£o num√©ricos cont√≠nuos)
5. **Tipos amb√≠guos** ‚Üí Explique a ambiguidade e sugira interpreta√ß√£o baseada no contexto

**TOM DE COMUNICA√á√ÉO:**
- Humanizado, did√°tico e conversacional
- Explique o RACIOC√çNIO por tr√°s de cada classifica√ß√£o
- Use analogias quando √∫til
- Seja acess√≠vel para n√£o-especialistas
- Demonstre entusiasmo pelo trabalho anal√≠tico"""
                    
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # PARTE 2: DADOS - Prepara informa√ß√µes detalhadas das colunas
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    colunas_detalhadas = []
                    for c in colunas_info:
                        detalhes = f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä Coluna: **{c['nome']}**
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  üîπ Tipo Python: {c['dtype_python']}
  üîπ Cardinalidade: {c['valores_unicos']} valores √∫nicos em {c['total_linhas']:,} registros ({c['valores_unicos']/c['total_linhas']*100:.1f}% de diversidade)
  üîπ Valores ausentes: {c['valores_nulos']} ({c['valores_nulos']/c['total_linhas']*100:.1f}%)
  üîπ Amostra (10 primeiros valores): {c['amostra_valores']}"""
                        
                        if 'min' in c:
                            detalhes += f"""
  üîπ Faixa num√©rica: [{c['min']:.2f}, {c['max']:.2f}]
  üîπ M√©dia: {c['media']:.2f} | Desvio padr√£o: {c['std']:.2f}"""
                        
                        colunas_detalhadas.append(detalhes)
                    
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # PARTE 3: TAREFA - Define o que fazer com formata√ß√£o esperada
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    user_prompt = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üìÅ DATASET PARA AN√ÅLISE                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

**Arquivo:** {csv_path}
**Dimens√µes:** {len(df.columns)} colunas √ó {len(df):,} linhas

{''.join(colunas_detalhadas)}

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    ‚ùì PERGUNTA DO USU√ÅRIO                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

{pergunta}

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üìù INSTRU√á√ïES PARA RESPOSTA                         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

**Seu trabalho:**
1. Analise CADA coluna usando sua intelig√™ncia sem√¢ntica e estat√≠stica
2. Classifique o tipo de cada coluna baseando-se em:
   - Nome e significado contextual
   - Padr√µes nos valores amostrados
   - Distribui√ß√£o estat√≠stica
   - Finalidade prov√°vel no dataset
3. Agrupe colunas por tipo identificado
4. Explique brevemente o MOTIVO de cada classifica√ß√£o
5. Se identificar tipos at√≠picos ou mistos, descreva-os

**Formato da resposta:**
- Use tom conversacional e did√°tico
- Estruture em se√ß√µes por tipo (use emojis para clareza)
- Para cada tipo, liste as colunas e explique resumidamente
- Finalize com uma observa√ß√£o geral sobre o dataset

**Exemplo de resposta humanizada e adaptativa:**

"Ol√°! üëã Analisando esse dataset de transa√ß√µes financeiras, identifiquei os seguintes tipos de dados:

**‚è±Ô∏è Colunas Temporais (1 coluna)**
- **Time**: Representa o momento de cada transa√ß√£o em segundos desde o in√≠cio da coleta. Embora seja num√©rico (int64), seu significado √© claramente temporal, marcando quando cada evento ocorreu.

**üè∑Ô∏è Colunas Categ√≥ricas (1 coluna)**
- **Class**: Vari√°vel bin√°ria (0 ou 1) que indica a classe da transa√ß√£o. Apesar de ser num√©rica, funciona como uma categoria binomial, onde 0 = transa√ß√£o normal e 1 = transa√ß√£o fraudulenta. √â o r√≥tulo alvo para classifica√ß√£o.

**üí∞ Colunas Num√©ricas Cont√≠nuas (29 colunas)**
- **Amount**: Valor monet√°rio da transa√ß√£o em unidade monet√°ria n√£o especificada
- **V1 a V28**: Caracter√≠sticas num√©ricas resultantes de transforma√ß√£o PCA (Principal Component Analysis), representando padr√µes ocultos nos dados originais. Mantidas an√¥nimas por quest√µes de privacidade.

**Observa√ß√£o geral:** Este √© um dataset t√≠pico de detec√ß√£o de fraude, com features anonimizadas (V1-V28) para proteger dados sens√≠veis, uma marca√ß√£o temporal e um r√≥tulo bin√°rio de classifica√ß√£o. üîç"

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üöÄ AGORA √â SUA VEZ - ANALISE E RESPONDA!            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
                    
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    
                    response = self.llm.invoke(messages)
                    resposta_llm = response.content
                    
                    logger.info("‚úÖ Resposta humanizada gerada via LLM com an√°lise sem√¢ntica profunda")
                    return resposta_llm
                
                except Exception as e:
                    logger.error(f"Erro ao usar LLM para resposta inteligente: {e}", exc_info=True)
                    # Fallback: resposta manual
            
            # Fallback manual (caso LLM indispon√≠vel)
            logger.warning("‚ö†Ô∏è LLM indispon√≠vel - usando fallback de an√°lise b√°sica")
            
            resposta = f"""# An√°lise dos Tipos de Dados

Analisando o dataset `{csv_path}` com {len(df.columns)} colunas e {len(df):,} linhas.

"""
            
            # An√°lise manual b√°sica
            for info in colunas_info:
                col_name = info['nome']
                unique = info['valores_unicos']
                
                # An√°lise b√°sica por nome e padr√£o
                if 'time' in col_name.lower() or 'date' in col_name.lower() or 'timestamp' in col_name.lower():
                    resposta += f"- **{col_name}**: Temporal (marcador de tempo)\n"
                elif unique == 2 and 'class' in col_name.lower():
                    resposta += f"- **{col_name}**: Categ√≥rica bin√°ria (2 categorias: {info['amostra_valores'][:2]})\n"
                elif pd.api.types.is_numeric_dtype(df[col_name]):
                    resposta += f"- **{col_name}**: Num√©rica\n"
                else:
                    resposta += f"- **{col_name}**: Categ√≥rica\n"
            
            return resposta
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # PERGUNTA COMPLEXA: An√°lise detalhada (manter c√≥digo original)
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        logger.info("üìä Pergunta COMPLEXA detectada: an√°lise detalhada")
        
        respostas = []  # ‚úÖ Inicializar lista de respostas
        temporal_cols = []
        
        # Configurar detector com par√¢metros customiz√°veis
        detection_config = TemporalDetectionConfig()
        if temporal_col_names:
            detection_config.common_names = temporal_col_names
        
        detector = TemporalColumnDetector(config=detection_config)
        
        try:
            detection_results = detector.detect(df, override_column=override_temporal_col)
            temporal_cols = detector.get_detected_columns(detection_results)
            
            logger.info({
                'event': 'deteccao_temporal_concluida',
                'colunas_temporais_detectadas': temporal_cols,
                'total_colunas': len(df.columns)
            })
            
            # Analisar colunas temporais com TemporalAnalyzer
            if temporal_cols:
                analyzer = TemporalAnalyzer(logger=logger)
                
                for col in temporal_cols:
                    try:
                        result = analyzer.analyze(df, col, enable_advanced=True)
                        respostas.append(result.to_markdown())
                        
                        logger.info({
                            'event': 'analise_temporal_coluna_concluida',
                            'coluna': col,
                            'tipo': 'temporal'
                        })
                    except Exception as e:
                        logger.error(f"Erro ao analisar coluna temporal '{col}': {e}", exc_info=True)
                        respostas.append(
                            f"## Erro na An√°lise Temporal: {col}\n\n"
                            f"N√£o foi poss√≠vel completar a an√°lise temporal: {str(e)}\n"
                        )
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o temporal: {e}", exc_info=True)
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ETAPA 2: AN√ÅLISE ESTAT√çSTICA DE TODAS AS OUTRAS COLUNAS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        # Identificar colunas N√ÉO temporais para an√°lise estat√≠stica completa
        non_temporal_cols = [col for col in df.columns if col not in temporal_cols]
        
        logger.info({
            'event': 'inicio_analise_estatistica_colunas',
            'total_colunas_nao_temporais': len(non_temporal_cols),
            'colunas': non_temporal_cols
        })
        
        # An√°lise estat√≠stica completa para cada coluna n√£o-temporal
        for col in non_temporal_cols:
            try:
                col_data = df[col]
                
                # An√°lise NUM√âRICA
                if pd.api.types.is_numeric_dtype(col_data):
                    stats_dict = {
                        'count': col_data.count(),
                        'mean': col_data.mean(),
                        'std': col_data.std(),
                        'min': col_data.min(),
                        '25%': col_data.quantile(0.25),
                        '50%': col_data.quantile(0.50),
                        '75%': col_data.quantile(0.75),
                        'max': col_data.max(),
                        'nulls': col_data.isnull().sum(),
                        'unique': col_data.nunique()
                    }
                    
                    analise_md = f"""## An√°lise Estat√≠stica: {col}

**Tipo:** Num√©rica ({col_data.dtype})

### Estat√≠sticas Descritivas

| M√©trica | Valor |
|---------|-------|
| Contagem | {stats_dict['count']:,} |
| M√©dia | {stats_dict['mean']:.6f} |
| Desvio Padr√£o | {stats_dict['std']:.6f} |
| M√≠nimo | {stats_dict['min']} |
| Q1 (25%) | {stats_dict['25%']} |
| Mediana (50%) | {stats_dict['50%']} |
| Q3 (75%) | {stats_dict['75%']} |
| M√°ximo | {stats_dict['max']} |
| Valores Nulos | {stats_dict['nulls']} |
| Valores √önicos | {stats_dict['unique']} |

### Interpreta√ß√£o

- **Amplitude:** {stats_dict['max'] - stats_dict['min']:.6f}
- **IQR:** {stats_dict['75%'] - stats_dict['25%']:.6f}
- **Coef. Varia√ß√£o:** {(stats_dict['std']/stats_dict['mean']*100) if stats_dict['mean'] != 0 else 0:.2f}%
"""
                    respostas.append(analise_md)
                
                # An√°lise CATEG√ìRICA
                else:
                    freq = col_data.value_counts().head(10)
                    
                    analise_md = f"""## An√°lise Estat√≠stica: {col}

**Tipo:** Categ√≥rica ({col_data.dtype})

### Estat√≠sticas Descritivas

| M√©trica | Valor |
|---------|-------|
| Contagem | {col_data.count():,} |
| Valores Nulos | {col_data.isnull().sum()} |
| Valores √önicos | {col_data.nunique()} |
| Moda | {col_data.mode().iloc[0] if not col_data.mode().empty else 'N/A'} |

### Distribui√ß√£o de Frequ√™ncia (Top 10)

{freq.to_markdown()}

### Interpreta√ß√£o

- **Valor mais frequente:** {freq.idxmax() if not freq.empty else 'N/A'}
- **Frequ√™ncia:** {freq.max() if not freq.empty else 0} ({freq.max()/len(col_data)*100 if not freq.empty else 0:.2f}%)
"""
                    respostas.append(analise_md)
                
                logger.info({
                    'event': 'analise_estatistica_coluna_concluida',
                    'coluna': col,
                    'tipo': 'numerica' if pd.api.types.is_numeric_dtype(col_data) else 'categorica'
                })
                
            except Exception as e:
                logger.error(f"Erro ao analisar coluna '{col}': {e}", exc_info=True)
                respostas.append(
                    f"## Erro na An√°lise: {col}\n\n"
                    f"N√£o foi poss√≠vel completar a an√°lise: {str(e)}\n"
                )
        
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # ETAPA 3: CONSOLIDAR RESULTADOS
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
        if respostas:
            header = (
                f"# An√°lise Completa do Dataset\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"**Total de colunas:** {len(df.columns)}\n\n"
                f"**Colunas temporais:** {len(temporal_cols)}\n\n"
                f"**Colunas num√©ricas/categ√≥ricas:** {len(non_temporal_cols)}\n\n"
                f"**Linhas:** {len(df):,}\n\n"
                "---\n\n"
            )
            
            return header + "\n\n---\n\n".join(respostas)
        else:
            # Fallback final: estat√≠sticas gerais descritivas
            logger.warning("Fallback final: retornando describe() do DataFrame")
            return (
                f"# Estat√≠sticas Descritivas\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"{df.describe().T.to_markdown()}"
            )

    def _should_use_global_csv(self, query: str, chunks_metadata: List[Dict]) -> bool:
        """
        Decide se √© necess√°rio recorrer ao CSV completo para responder a pergunta.
        Crit√©rios:
        - Pergunta exige an√°lise de todas as colunas/linhas (ex: intervalo de todas vari√°veis)
        - Chunks n√£o possuem dados suficientes (ex: subset de colunas)
        """
        # Detecta termos que indicam an√°lise global
        termos_globais = ["todas as vari√°veis", "todas as colunas", "intervalo de cada vari√°vel", "intervalo de todas", "intervalo completo", "todas as linhas", "an√°lise completa"]
        if any(t in query.lower() for t in termos_globais):
            return True
        # Detecta se chunks n√£o cobrem todas as colunas
        if chunks_metadata:
            # Extrai colunas presentes nos chunks
            colunas_chunks = set()
            for chunk in chunks_metadata:
                if 'columns' in chunk:
                    colunas_chunks.update(chunk['columns'])
            # Se n√∫mero de colunas for pequeno, pode indicar subset
            if len(colunas_chunks) < 5:
                return True
        return False

    def reset_memory(self, session_id: str = None):
        """
        Reseta a mem√≥ria/contexto do agente para a sess√£o informada.
        """
        self.memory = {}
        if session_id:
            self.session_id = session_id
        self.logger.info(f"Mem√≥ria/contexto resetados para sess√£o: {session_id}")
    """
    Agente que responde perguntas sobre dados usando RAG vetorial + mem√≥ria persistente + LangChain.
    
    Fluxo V2.0:
    1. Inicializa sess√£o de mem√≥ria (se n√£o existir)
    2. Recupera contexto conversacional anterior
    3. Gera embedding da pergunta
    4. Busca chunks similares nos DADOS usando match_embeddings()
    5. Usa LangChain LLM para interpretar chunks + contexto hist√≥rico
    6. Salva intera√ß√£o na mem√≥ria persistente
    7. Retorna resposta contextualizada
    
    SEM keywords hardcoded, SEM classifica√ß√£o manual, SEM listas fixas.
    COM mem√≥ria persistente, COM LangChain, COM contexto conversacional.
    """
    
    def __init__(self):
        super().__init__(
            name="rag_data_analyzer",
            description="Analisa dados usando busca vetorial sem√¢ntica pura com mem√≥ria persistente",
            enable_memory=True  # ‚úÖ CR√çTICO: Habilita mem√≥ria persistente
        )
        self.logger = get_logger("agent.rag_data")
        self.embedding_gen = EmbeddingGenerator()
        
        # Inicializar LLM LangChain
        self._init_langchain_llm()
        
        # ‚úÖ V4.0: Inicializar gerador de prompts din√¢micos e cache de contexto
        self.prompt_generator = DynamicPromptGenerator()
        self.current_dataset_context: Optional[DatasetContext] = None
        
        self.logger.info("‚úÖ RAGDataAgent V4.0 inicializado - prompts din√¢micos + par√¢metros otimizados + mem√≥ria")
    
    def _init_langchain_llm(self):
        """Inicializa LLM via camada de abstra√ß√£o LangChainLLMManager.
        
        ‚úÖ V4.1: Refatorado para usar abstra√ß√£o existente (elimina duplica√ß√£o).
        Ordem de prioridade: GROQ ‚Üí Google ‚Üí OpenAI (via LangChainLLMManager)
        """
        try:
            from src.llm.langchain_manager import get_langchain_llm_manager, LLMConfig
            
            # Obter inst√¢ncia singleton do manager
            manager = get_langchain_llm_manager()
            
            # Criar configura√ß√£o para o LLM
            config = LLMConfig(temperature=0.3, max_tokens=2000, top_p=0.25)
            
            # Obter cliente LangChain do provedor ativo
            self.llm = manager._get_client(manager.active_provider, config)
            
            self.logger.info(
                f"‚úÖ LLM inicializado via abstra√ß√£o: {manager.active_provider.value.upper()} "
                f"(fallback autom√°tico: GROQ ‚Üí Google ‚Üí OpenAI)"
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Falha ao inicializar LLM via abstra√ß√£o: {e}")
            self.llm = None
            self.logger.warning("‚ö†Ô∏è Sistema operando sem LLM - funcionalidade limitada")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ‚úÖ V4.0: M√âTODO PARA ATUALIZAR CONTEXTO DO DATASET
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _update_dataset_context(self, df: pd.DataFrame, file_path: str) -> DatasetContext:
        """
        Atualiza contexto do dataset baseado no DataFrame REAL.
        
        ELIMINA HARDCODING:
        - Detecta automaticamente n√∫mero de colunas com df.shape
        - Extrai dtypes reais com df.dtypes
        - Identifica categ√≥ricas bin√°rias (ex: Class com {0,1})
        - Calcula estat√≠sticas reais com df.describe()
        
        Args:
            df: DataFrame carregado do CSV
            file_path: Caminho do arquivo CSV
            
        Returns:
            DatasetContext com tipos, colunas e estat√≠sticas REAIS
        """
        try:
            context = DatasetContext.from_dataframe(df, file_path)
            self.current_dataset_context = context
            
            self.logger.info({
                'event': 'dataset_context_updated',
                'file': file_path,
                'shape': df.shape,
                'numeric_cols': len(context.numeric_columns),
                'categorical_cols': len(context.categorical_columns),
                'temporal_cols': len(context.temporal_columns),
                'memory_usage_mb': context.memory_usage_mb
            })
            
            return context
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao atualizar contexto do dataset: {e}", exc_info=True)
            return None
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # üîí SPRINT 3 P0-4: M√âTODO DE EXECU√á√ÉO SEGURA VIA SANDBOX
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def _executar_codigo_sandbox(
        self, 
        code: str, 
        df: pd.DataFrame,
        timeout_seconds: int = 5,
        memory_limit_mb: int = 100
    ) -> Dict[str, Any]:
        """
        üîí Executa c√≥digo Python din√¢mico de forma SEGURA usando RestrictedPython sandbox.
        
        SEGURAN√áA (5 camadas):
        1. Compila√ß√£o restritiva (bloqueia eval, exec, compile)
        2. Whitelist de imports (pandas, numpy, math, statistics, datetime, json, collections, re)
        3. Blacklist de imports perigosos (os, subprocess, sys, socket, urllib, requests, etc.)
        4. Ambiente isolado (sem acesso a open, __import__, globals, locals)
        5. Limites de recursos (timeout 5s, mem√≥ria 100MB)
        
        Args:
            code: C√≥digo Python a executar (deve definir vari√°vel 'resultado')
            df: DataFrame pandas dispon√≠vel como vari√°vel 'df' no contexto
            timeout_seconds: Tempo m√°ximo de execu√ß√£o (default: 5s)
            memory_limit_mb: Limite de mem√≥ria (default: 100MB, apenas Unix/Linux)
            
        Returns:
            Dict com chaves:
            - success (bool): True se execu√ß√£o bem-sucedida
            - result (Any): Resultado da execu√ß√£o (valor da vari√°vel 'resultado')
            - error (str): Mensagem de erro se falha
            - error_type (str): Tipo do erro (SandboxImportError, SandboxTimeoutError, etc.)
            - execution_time_ms (float): Tempo de execu√ß√£o em milissegundos
            - logs (List[str]): Logs de auditoria da execu√ß√£o
            
        Raises:
            Nunca levanta exce√ß√µes - sempre retorna dict com 'success': False em caso de erro
            
        Example:
            >>> code = "resultado = df['Amount'].mean()"
            >>> result = agent._executar_codigo_sandbox(code, df)
            >>> if result['success']:
            ...     print(f"M√©dia: {result['result']}")
            >>> else:
            ...     print(f"Erro: {result['error']}")
        """
        # Preparar contexto global seguro (DataFrame dispon√≠vel)
        import pandas as pd
        import numpy as np
        
        # Log de auditoria ANTES da execu√ß√£o
        self.logger.info({
            'event': 'sandbox_execution_request',
            'code_length': len(code),
            'code_preview': code[:200] + ('...' if len(code) > 200 else ''),
            'timeout_seconds': timeout_seconds,
            'memory_limit_mb': memory_limit_mb,
            'dataframe_shape': df.shape if df is not None else None,
            'timestamp': datetime.now().isoformat()
        })
        
        try:
            # üîí PREPARAR VARI√ÅVEIS GLOBAIS CUSTOMIZADAS (DataFrame + bibliotecas)
            custom_globals = {
                'df': df,  # DataFrame dispon√≠vel como 'df' no c√≥digo
                'pd': pd,  # pandas dispon√≠vel como 'pd'
                'np': np   # numpy dispon√≠vel como 'np'
            }
            
            # üîí EXECU√á√ÉO SEGURA via RestrictedPython
            sandbox_result = execute_in_sandbox(
                code=code,
                timeout_seconds=timeout_seconds,
                memory_limit_mb=memory_limit_mb,
                allowed_imports=['pandas', 'numpy', 'math', 'statistics', 'datetime', 'json', 'collections', 're'],
                return_variable='resultado',
                custom_globals=custom_globals  # üîë INJETAR VARI√ÅVEIS NO SANDBOX
            )
            
            # Validar que sandbox retornou dict v√°lido
            if not isinstance(sandbox_result, dict):
                raise TypeError(f"Sandbox retornou tipo inv√°lido: {type(sandbox_result)}")
            
            # Log de auditoria AP√ìS execu√ß√£o
            self.logger.info({
                'event': 'sandbox_execution_completed',
                'success': sandbox_result.get('success', False),
                'execution_time_ms': sandbox_result.get('execution_time_ms', 0),
                'error_type': sandbox_result.get('error_type'),
                'error_message': str(sandbox_result.get('error', ''))[:200],  # Truncar erros longos
                'logs_count': len(sandbox_result.get('logs', [])),
                'timestamp': datetime.now().isoformat()
            })
            
            # Registrar logs do sandbox no logger principal
            for log_entry in sandbox_result.get('logs', []):
                self.logger.debug(f"[SANDBOX LOG] {log_entry}")
            
            return sandbox_result
            
        except Exception as e:
            # Fallback extremo: erro na pr√≥pria chamada do sandbox
            import traceback
            error_msg = f"Erro cr√≠tico ao chamar sandbox: {str(e)}"
            traceback_str = traceback.format_exc()
            
            self.logger.error({
                'event': 'sandbox_execution_critical_error',
                'error': error_msg,
                'exception_type': type(e).__name__,
                'traceback': traceback_str[:500],  # Truncar traceback
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'success': False,
                'result': None,
                'error': error_msg,
                'error_type': 'CriticalSandboxError',
                'execution_time_ms': 0.0,
                'logs': [f"ERRO CR√çTICO: {error_msg}", f"Traceback: {traceback_str}"]
            }
    
    async def process(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,  # Pylance: context est√° definido aqui
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa query do usu√°rio usando RAG vetorial + mem√≥ria persistente.
        
        VERS√ÉO ASYNC com mem√≥ria persistente.
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto adicional (opcional) - DEFINIDO NO ESCOPO
            session_id: ID da sess√£o para mem√≥ria persistente
            
        Returns:
            Resposta baseada em busca vetorial + contexto hist√≥rico
        """
        start_time = datetime.now()  # Pylance: start_time est√° definido aqui
        
        try:
            self.logger.info(f"üîç Processando query via RAG V2.0: {query[:80]}...")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 1. INICIALIZAR MEM√ìRIA PERSISTENTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if not self._current_session_id:
                if session_id:
                    await self.init_memory_session(session_id)
                else:
                    session_id = await self.init_memory_session()
                self.logger.info(f"‚úÖ Sess√£o de mem√≥ria inicializada: {session_id}")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 2. RECUPERAR CONTEXTO CONVERSACIONAL ANTERIOR
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # FILTRAR CONTEXTO: manter apenas campos relevantes para an√°lise
            memory_context = {}
            if context:
                filtered_context = {}
                if 'chunks' in context:
                    filtered_context['chunks'] = context['chunks']
                if 'csv_data' in context:
                    filtered_context['csv_data'] = context['csv_data']
                # ‚úÖ PRESERVAR FLAGS DE VISUALIZA√á√ÉO
                if 'visualization_requested' in context:
                    filtered_context['visualization_requested'] = context['visualization_requested']
                if 'visualization_type' in context:
                    filtered_context['visualization_type'] = context['visualization_type']
                if 'fallback_sample_limit' in context:
                    filtered_context['fallback_sample_limit'] = context['fallback_sample_limit']
                if 'reconstructed_df' in context:
                    filtered_context['reconstructed_df'] = context['reconstructed_df']
                context = filtered_context
            # N√ÉO recuperar contexto de mem√≥ria para queries de intervalo
            interval_terms = ['intervalo', 'm√≠nimo', 'm√°ximo', 'range', 'amplitude']
            if any(term in query.lower() for term in interval_terms):
                memory_context = {}  # Ignorar hist√≥rico/mem√≥ria
            elif self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(
                    f"‚úÖ Contexto de mem√≥ria recuperado: "
                    f"{len(memory_context.get('recent_messages', []))} mensagens anteriores"
                )
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 3. GERAR EMBEDDING DA QUERY
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            self.logger.debug("Gerando embedding da query...")
            embedding_result = self.embedding_gen.generate_embedding(query)
            
            # Extrair lista de floats do resultado
            if isinstance(embedding_result, list):
                query_embedding = embedding_result
            elif hasattr(embedding_result, 'embedding'):
                query_embedding = embedding_result.embedding
            else:
                return self._build_error_response("Formato de embedding inv√°lido")
            
            if not query_embedding or len(query_embedding) == 0:
                return self._build_error_response("Falha ao gerar embedding da query")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # ‚úÖ V4.0: CLASSIFICAR INTENT E OBTER CONFIGURA√á√ïES OTIMIZADAS
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            llm_config, rag_config = None, None
            if self.llm:
                try:
                    classifier = IntentClassifier(llm=self.llm, logger=self.logger)
                    classification_result = classifier.classify(query=query, context={})
                    
                    # Obter configura√ß√µes otimizadas baseadas na inten√ß√£o
                    llm_config, rag_config = get_configs_for_intent(classification_result.primary_intent.value)
                    
                    self.logger.info({
                        'event': 'v4_configs_applied',
                        'intent': classification_result.primary_intent.value,
                        'temperature': llm_config.temperature,
                        'max_tokens': llm_config.max_tokens,
                        'rag_threshold': rag_config.similarity_threshold,  # ‚úÖ V4.1: Corrigido nome do atributo
                        'rag_max_chunks': rag_config.max_chunks
                    })
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Falha ao classificar intent, usando configs default: {e}")
            
            # Usar configura√ß√µes otimizadas ou defaults (‚úÖ V4.1: Corrigido atributo)
            rag_threshold = rag_config.similarity_threshold if rag_config else 0.3
            rag_limit = rag_config.max_chunks if rag_config else 10
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 4. BUSCAR CHUNKS SIMILARES NOS DADOS (com configs otimizados)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            self.logger.debug(f"Buscando chunks similares (threshold={rag_threshold}, limit={rag_limit})...")
            similar_chunks = self._search_similar_data(
                query_embedding=query_embedding,
                threshold=rag_threshold,  # ‚úÖ V4.0: Threshold otimizado (0.6-0.65 vs 0.3)
                limit=rag_limit  # ‚úÖ V4.0: Max chunks otimizado (10)
            )
            
            # SALVAR CONTEXTO DE DADOS NA TABELA agent_context
            if self.has_memory and self._current_session_id and similar_chunks:
                data_context = {
                    "dataset_info": {
                        "total_chunks": len(similar_chunks),
                        "source_types": list(set(c.get('source_type', 'unknown') for c in similar_chunks)),
                        "embedding_provider": "sentence-transformer",
                        "last_query": query[:100],
                        "query_timestamp": datetime.now().isoformat()
                    },
                    "performance_metrics": {
                        "embedding_generation_time": "N/A",  # Poderia ser medido
                        "search_time": "N/A",  # Poderia ser medido
                        "chunks_found": len(similar_chunks)
                    }
                }
                
                try:
                    await self.remember_data_context(
                        data_info=data_context,
                        context_key="current_dataset_info"
                    )
                    self.logger.debug("‚úÖ Contexto de dados salvo na tabela agent_context")
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Falha ao salvar contexto de dados: {e}")
            
            # Fallback inteligente: se chunks n√£o s√£o suficientes OU pergunta exige an√°lise global
            if not similar_chunks or self._should_use_global_csv(query, similar_chunks):
                # Tentar extrair caminho do CSV dos chunks ou do contexto
                csv_path = None
                if context and 'csv_path' in context:
                    csv_path = context['csv_path']
                elif similar_chunks:
                    for chunk in similar_chunks:
                        if 'csv_path' in chunk:
                            csv_path = chunk['csv_path']
                            break
                # Se n√£o encontrar, buscar na pasta processados
                if not csv_path:
                    try:
                        from src.settings import EDA_DATA_DIR_PROCESSADO
                        import os
                        import pandas as pd
                        csv_files = list(EDA_DATA_DIR_PROCESSADO.glob('*.csv'))
                        if csv_files:
                            csv_path = str(max(csv_files, key=lambda p: p.stat().st_mtime))
                    except Exception as e:
                        self.logger.warning(f"N√£o foi poss√≠vel localizar CSV para fallback: {e}")
                if csv_path:
                    self.logger.info(f"‚ö° Fallback: an√°lise global do CSV ({csv_path}) para pergunta '{query[:60]}...'")
                    try:
                        resposta_csv = self._analisar_completo_csv(csv_path, query)
                        return self._build_response(
                            resposta_csv,
                            metadata={
                                "method": "global_csv_fallback",
                                "csv_path": csv_path,
                                "chunks_found": len(similar_chunks)
                            }
                        )
                    except Exception as e:
                        self.logger.error(f"Erro no fallback global CSV: {e}")
                        return self._build_error_response(f"Erro ao processar CSV global: {e}")
                else:
                    self.logger.error("‚ùå Fallback global: CSV n√£o encontrado.")
                    return self._build_error_response("CSV original n√£o encontrado para an√°lise global.")
            
            self.logger.info(f"‚úÖ Encontrados {len(similar_chunks)} chunks relevantes")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üÜï VERIFICAR SE VISUALIZA√á√ÉO FOI SOLICITADA (MESMO COM CHUNKS)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            viz_requested = bool(context and context.get('visualization_requested'))
            if viz_requested:
                self.logger.info("üìä Visualiza√ß√£o solicitada - gerando gr√°ficos...")
                try:
                    import pandas as pd
                    from pathlib import Path
                    
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # ‚ö†Ô∏è EXCE√á√ÉO DE CONFORMIDADE - ACESSO DIRETO AO CSV
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    # JUSTIFICATIVA:
                    # 1. Tabela embeddings cont√©m chunks de an√°lises estat√≠sticas (Markdown)
                    # 2. Histogramas requerem dados tabulares completos (285k linhas √ó 31 colunas)
                    # 3. Embeddar cada linha seria ineficiente: ~$50-100 de custo + overhead
                    # 4. Padr√£o de mercado: LangChain, LlamaIndex, OpenAI Code Interpreter
                    #    fazem leitura direta de CSV para an√°lises quantitativas
                    # 
                    # IMPLEMENTA√á√ÉO FUTURA:
                    # - TODO: Adicionar chunks raw_data na tabela embeddings durante ingest√£o
                    # - TODO: Implementar reconstitui√ß√£o de DataFrame a partir de embeddings
                    # 
                    # AUDITORIA:
                    # - Log completo de acesso registrado
                    # - Metadados inclu√≠dos na resposta
                    # - Acesso read-only sem modifica√ß√£o de dados
                    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                    
                    from src.settings import EDA_DATA_DIR_PROCESSADO
                    # Buscar CSV mais recente em data/processado/
                    csv_files = list(EDA_DATA_DIR_PROCESSADO.glob("*.csv"))
                    if not csv_files:
                        self.logger.error("‚ùå Nenhum arquivo CSV encontrado em data/processado/")
                        self.logger.info("‚ö†Ô∏è Continuando com resposta textual sem visualiza√ß√µes")
                    else:
                        # Pegar o arquivo mais recente (√∫ltimo modificado)
                        csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
                        csv_size_mb = csv_path.stat().st_size / 1_000_000
                        self.logger.warning(
                            "‚ö†Ô∏è EXCE√á√ÉO DE CONFORMIDADE: Acesso direto ao CSV para visualiza√ß√£o",
                            extra={
                                "event_type": "direct_csv_access",
                                "user_query": query[:100],
                                "csv_path": str(csv_path),
                                "csv_size_mb": round(csv_size_mb, 2),
                                "access_reason": "histogram_generation",
                                "session_id": self._current_session_id,
                                "agent_name": self.name,
                                "timestamp": datetime.now().isoformat(),
                                "conformidade_status": "exception_approved",
                                "alternative_implementation": "future_raw_data_embeddings",
                                "cost_saved_estimate_usd": 50.0
                            }
                        )
                        viz_df = pd.read_csv(csv_path)
                        self.logger.info(
                            f"‚úÖ CSV carregado para visualiza√ß√£o: {viz_df.shape[0]:,} linhas √ó {viz_df.shape[1]} colunas | "
                            f"Tamanho: {csv_size_mb:.2f} MB"
                        )
                        # Delegar para agente de visualiza√ß√£o
                        # Removido: agente obsoleto csv_analysis_agent.py
                        vis_context = context.copy() if context else {}
                        vis_context['reconstructed_df'] = viz_df
                        vis_result = self._handle_visualization_query(query, vis_context)
                        if vis_result.get('metadata', {}).get('visualization_success'):
                            # Combinar resposta de visualiza√ß√£o com an√°lise textual dos chunks
                            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
                            context_str = "\n\n".join(context_texts[:5])
                            text_response = await self._generate_llm_response_langchain(
                                query=query,
                                context_data=context_str,
                                memory_context=memory_context,
                                chunks_metadata=similar_chunks
                            )
                            # Combinar resposta textual com informa√ß√£o sobre gr√°ficos
                            graficos_info = vis_result.get('metadata', {}).get('graficos_gerados', [])
                            if graficos_info:
                                graficos_msg = f"\n\nüìä **Visualiza√ß√µes Geradas:**\n"
                                for gf in graficos_info:
                                    graficos_msg += f"‚Ä¢ {gf}\n"
                                combined_response = text_response + graficos_msg
                            else:
                                combined_response = text_response
                            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                            # Salvar intera√ß√£o com metadados de conformidade
                            if self.has_memory:
                                await self.remember_interaction(
                                    query=query,
                                    response=combined_response,
                                    processing_time_ms=processing_time_ms,
                                    confidence=1.0,
                                    model_used="rag_v2_with_visualizations",
                                    metadata={
                                        "chunks_found": len(similar_chunks),
                                        "visualization_success": True,
                                        "graficos_gerados": len(graficos_info),
                                        "conformidade_exception": {
                                            "type": "direct_csv_access",
                                            "reason": "visualization_requires_raw_data",
                                            "csv_path": str(csv_path),
                                            "csv_size_mb": round(csv_size_mb, 2),
                                            "access_timestamp": datetime.now().isoformat(),
                                            "approved": True,
                                            "alternative_future": "raw_data_embeddings_implementation",
                                            "industry_standard": "LangChain/LlamaIndex/OpenAI_pattern",
                                            "cost_saved_usd": 50.0,
                                            "read_only": True
                                        }
                                    }
                                )
                            return self._build_response(
                                combined_response,
                                metadata={
                                    **vis_result.get('metadata', {}),
                                    "chunks_found": len(similar_chunks),
                                    "method": "rag_vectorial_v2_with_viz",
                                    "processing_time_ms": processing_time_ms,
                                    "conformidade_exception": {
                                        "type": "direct_csv_access",
                                        "reason": "visualization_requires_raw_data",
                                        "csv_path": str(csv_path),
                                        "csv_size_mb": round(csv_size_mb, 2),
                                        "approved": True,
                                        "industry_standard": True,
                                        "read_only": True,
                                        "documentation": "See comments in rag_data_agent.py lines 318-335"
                                    }
                                }
                            )
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Erro ao gerar visualiza√ß√µes: {e}", exc_info=True)
                    # Continuar com resposta textual normal se visualiza√ß√£o falhar
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 5. GERAR RESPOSTA COM LANGCHAIN + CONTEXTO HIST√ìRICO
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
            context_str = "\n\n".join(context_texts[:5])  # Top 5 mais relevantes
            
            self.logger.debug("Usando LangChain LLM para gerar resposta...")
            response_text = await self._generate_llm_response_langchain(
                query=query,
                context_data=context_str,
                memory_context=memory_context,
                chunks_metadata=similar_chunks
            )
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            avg_similarity = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks)
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 6. SALVAR INTERA√á√ÉO NA MEM√ìRIA PERSISTENTE
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=response_text,
                    processing_time_ms=processing_time_ms,
                    confidence=avg_similarity,
                    model_used="langchain_gemini" if self.llm else "fallback",
                    metadata={
                        "chunks_found": len(similar_chunks),
                        "chunks_used": min(5, len(similar_chunks)),
                        "avg_similarity": avg_similarity,
                        "top_similarity": similar_chunks[0]['similarity'],
                        "has_history": len(memory_context.get('recent_conversations', [])) > 0
                    }
                )
                self.logger.debug("‚úÖ Intera√ß√£o salva na mem√≥ria persistente")
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 7. RETORNAR RESPOSTA COM METADADOS
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            return self._build_response(
                response_text,
                metadata={
                    "chunks_found": len(similar_chunks),
                    "chunks_used": min(5, len(similar_chunks)),
                    "avg_similarity": avg_similarity,
                    "method": "rag_vectorial_v2",
                    "top_similarity": similar_chunks[0]['similarity'] if similar_chunks else 0,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao processar query: {str(e)}", exc_info=True)
            return self._build_error_response(f"Erro no processamento: {str(e)}")
    
    def _search_similar_data(
        self,
        query_embedding: List[float],
        threshold: float = 0.5,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Busca chunks similares nos dados usando match_embeddings RPC.
        
        Args:
            query_embedding: Embedding da query
            threshold: Threshold de similaridade (0.0 - 1.0)
            limit: N√∫mero m√°ximo de resultados
            
        Returns:
            Lista de chunks similares com metadata
        """
        try:
            # Chamar fun√ß√£o RPC match_embeddings
            response = supabase.rpc(
                'match_embeddings',
                {
                    'query_embedding': query_embedding,
                    'similarity_threshold': threshold,
                    'match_count': limit
                }
            ).execute()
            
            if not response.data:
                self.logger.warning("Nenhum chunk similar encontrado")
                return []
            
            self.logger.debug(f"Encontrados {len(response.data)} chunks similares")
            # Parsing defensivo dos embeddings
            from src.embeddings.vector_store import parse_embedding_from_api, VECTOR_DIMENSIONS
            parsed_chunks = []
            for chunk in response.data:
                embedding_raw = chunk.get('embedding')
                try:
                    chunk['embedding'] = parse_embedding_from_api(embedding_raw, VECTOR_DIMENSIONS)
                except Exception as e:
                    self.logger.warning(f"Falha ao parsear embedding do chunk: {e}")
                    chunk['embedding'] = None
                parsed_chunks.append(chunk)
            return parsed_chunks
            
        except Exception as e:
            self.logger.error(f"Erro na busca vetorial: {str(e)}")
            return []
    
    async def _generate_llm_response_langchain(
        self,
        query: str,
        context_data: str,
        memory_context: dict,
        chunks_metadata: list
    ) -> str:
        try:
            # Preparar contexto hist√≥rico da conversa
            history_context = ""
            if memory_context.get('recent_messages') and len(memory_context['recent_messages']) > 0:
                history_context = "\n\n**Contexto da Conversa Anterior:**\n"
                for msg in memory_context['recent_messages'][-6:]:  # √öltimas 6 mensagens (3 pares user/assistant)
                    msg_type = msg.get('type', 'unknown')
                    content = msg.get('content', '')[:200]  # Limitar a 200 chars
                    if msg_type == 'user':
                        history_context += f"- Usu√°rio perguntou: {content}\n"
                    elif msg_type == 'assistant':
                        history_context += f"- Assistente respondeu: {content}\n"
                history_context += "\n"

            # Preparar prompt DIN√ÇMICO baseado no tipo de query
            query_lower = query.lower()
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # üî• V3.0: ORQUESTRA√á√ÉO INTELIGENTE VIA LLM (ZERO HARD-CODING)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 
            # ANTES (V2.0): ~240 linhas de cascata if/elif com keywords hardcoded
            # DEPOIS (V3.0): Classifica√ß√£o sem√¢ntica + orquestra√ß√£o modular
            # 
            # Benef√≠cios:
            # - ‚úÖ Reconhece QUALQUER sin√¥nimo (n√£o limitado a lista fixa)
            # - ‚úÖ Processa queries mistas simultaneamente
            # - ‚úÖ Extens√≠vel (novos tipos sem modificar c√≥digo)
            # - ‚úÖ Manuten√≠vel (c√≥digo limpo e modular)
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            # Detectar se √© query sobre HIST√ìRICO (caso especial)
            is_history_query = any(term in query_lower for term in [
                'pergunta anterior', 'perguntei antes', 'falamos sobre',
                'conversamos sobre', 'voc√™ disse', 'previous question', 'asked before'
            ])
            
            if is_history_query:
                # Query sobre HIST√ìRICO - usar mem√≥ria conversacional
                self.logger.info("üìú Query sobre hist√≥rico conversacional detectada")
                
                system_prompt = (
                    "Voc√™ √© um agente EDA especializado. Sua tarefa √© responder sobre o HIST√ìRICO da conversa. "
                    "Use o contexto da conversa anterior fornecido para responder. "
                    "Seja claro e objetivo, referenciando exatamente o que foi discutido."
                )
                user_prompt = (
                    f"{history_context}"
                    f"**Pergunta do Usu√°rio:**\n{query}\n\n"
                    "**INSTRU√á√ïES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Consulte o hist√≥rico da conversa acima\n"
                    "- Responda referenciando exatamente o que foi perguntado/respondido anteriormente\n"
                    "- Se n√£o houver hist√≥rico suficiente, informe claramente\n"
                    "- Finalize com: 'Posso esclarecer mais alguma coisa sobre nossa conversa?'\n\n"
                    "**Resposta:**"
                )
                
                # Usar LLM diretamente para query de hist√≥rico
                if self.llm and LANGCHAIN_AVAILABLE:
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    response = await asyncio.to_thread(self.llm.invoke, messages)
                    final_response = response.content
                else:
                    final_response = "Hist√≥rico n√£o dispon√≠vel (LLM indispon√≠vel)"
            
            else:
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                # üî• V3.0: ORQUESTRA√á√ÉO ANAL√çTICA MODULAR
                # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                try:
                    self.logger.info("üî• Executando V3.0: AnalysisOrchestrator")
                    
                    # Carregar DataFrame do CSV se dispon√≠vel
                    # Note: 'context' est√° definido no par√¢metro do m√©todo process()
                    df = None
                    if context and 'csv_data' in context:  # type: ignore[has-type]
                        import pandas as pd
                        csv_path = context['csv_data'].get('path')  # type: ignore[has-type]
                        if csv_path:
                            try:
                                df = pd.read_csv(csv_path)
                                self.logger.info(f"üìä DataFrame carregado: {df.shape}")
                            except Exception as e:
                                self.logger.error(f"Erro ao carregar CSV: {e}")
                    
                    # Se DataFrame dispon√≠vel, usar orchestrator V3
                    if df is not None and not df.empty:
                        final_response = self._build_analytical_response_v3(
                            query=query,
                            df=df,
                            context_data=context_data,
                            history_context=history_context
                        )
                    else:
                        # Fallback: resposta baseada apenas em chunks (sem an√°lise executada)
                        self.logger.warning("‚ö†Ô∏è DataFrame n√£o dispon√≠vel - usando fallback chunks-only")
                        final_response = self._fallback_basic_response(
                            query=query,
                            context_data=context_data,
                            history_context=history_context
                        )
                
                except Exception as e:
                    self.logger.error(f"‚ùå Erro no fluxo V3.0: {e}", exc_info=True)
                    # Fallback final
                    final_response = self._fallback_basic_response(
                        query=query,
                        context_data=context_data,
                        history_context=history_context
                    )
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # RESPOSTA FINAL
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # 6. SALVAR NA MEM√ìRIA E RETORNAR
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
            # Note: start_time est√° definido no in√≠cio do m√©todo (linha ~821)
            # Note: similar_chunks est√° definido ap√≥s busca vetorial (linha ~889)
            # Pylance false positive: essas vari√°veis EST√ÉO no escopo do m√©todo
            processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000  # type: ignore[has-type]
            
            # Calcular m√©tricas se similar_chunks dispon√≠vel
            chunks_count = len(similar_chunks) if similar_chunks else 0  # type: ignore[has-type]
            avg_sim = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks) if similar_chunks else 0.0  # type: ignore[has-type]
            top_sim = similar_chunks[0]['similarity'] if similar_chunks else 0.0  # type: ignore[has-type]
            
            # Salvar intera√ß√£o na mem√≥ria persistente
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=final_response,
                    processing_time_ms=processing_time_ms,
                    confidence=0.85,
                    model_used="rag_v3_orchestrated",
                    metadata={
                        "chunks_found": chunks_count,
                        "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                        "method": "rag_vectorial_v3",
                        "architecture": "modular_orchestrated",
                        "zero_hardcoding": True
                    }
                )
            
            # Retornar resposta formatada
            return self._build_response(
                final_response,
                metadata={
                    "chunks_found": chunks_count,
                    "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                    "avg_similarity": avg_sim,
                    "method": "rag_vectorial_v3",
                    "architecture": "modular_orchestrated",
                    "top_similarity": top_sim,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao gerar resposta LLM: {str(e)}", exc_info=True)
            return self._format_raw_data_response(query, chunks_metadata)
    
    def _format_raw_data_response(
        self,
        query: str,
        chunks_metadata: List[Dict]
        ) -> str:
        """
        Fallback: usa agente de s√≠ntese para consolidar dados se LLM falhar.
        """
        # Extrair apenas o texto dos chunks
        chunks = [chunk.get('chunk_text', '') for chunk in chunks_metadata]
        
        # Chamar agente de s√≠ntese para consolidar
        from src.agent.rag_synthesis_agent import synthesize_response
        try:
            return synthesize_response(chunks, query, use_llm=False)
        except Exception as e:
            self.logger.error(f"Erro no agente de s√≠ntese: {e}")
            # Fallback extremo: resposta estruturada m√≠nima
            return f"""## Resposta para: {query}

**Status:** ‚ö†Ô∏è Erro na s√≠ntese

N√£o foi poss√≠vel processar completamente a consulta devido a um erro t√©cnico.
Por favor, reformule sua pergunta ou entre em contato com o suporte.

_Erro: {str(e)}_"""
    
    def _build_error_response(self, error_msg: str) -> Dict[str, Any]:
        """Constr√≥i resposta de erro padronizada."""
        return self._build_response(
            f"‚ùå {error_msg}",
            metadata={"error": True, "method": "rag_vectorial_v2"}
        )
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTODO S√çNCRONO WRAPPER (para compatibilidade retroativa)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    def process_sync(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Wrapper s√≠ncrono para compatibilidade com c√≥digo legado.
        
        ‚ö†Ô∏è DEPRECATED: Use process() async quando poss√≠vel.
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.process(query, context))
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # M√âTODO DE CARREGAMENTO CSV (mantido da vers√£o anterior)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    async def load_csv_to_embeddings(
        self,
        csv_path: str,
        chunk_size: int = 1000,
        overlap: int = 100
    ) -> Dict[str, Any]:
        """
        Carrega CSV para a tabela embeddings.
        
        Args:
            csv_path: Caminho do arquivo CSV
            chunk_size: Tamanho dos chunks
            overlap: Overlap entre chunks
            
        Returns:
            Status do carregamento
        """
        try:
            self.logger.info(f"üìÇ Carregando CSV: {csv_path}")
            
            import pandas as pd
            from src.embeddings.chunker import CSVChunker
            
            # Ler CSV
            df = pd.read_csv(csv_path)
            self.logger.info(f"‚úÖ CSV lido: {len(df)} linhas, {len(df.columns)} colunas")
            
            # Criar chunks
            chunker = CSVChunker(chunk_size=chunk_size, overlap=overlap)
            chunks = chunker.chunk_dataframe(df)
            self.logger.info(f"‚úÖ Criados {len(chunks)} chunks")
            
            # Gerar embeddings e salvar
            inserted_count = 0
            for i, chunk in enumerate(chunks):
                try:
                    # Gerar embedding
                    embedding = self.embedding_gen.generate_embedding(chunk['text'])
                    
                    # Salvar na tabela embeddings
                    insert_data = {
                        'chunk_text': chunk['text'],
                        'embedding': embedding,
                        'metadata': {
                            'source': csv_path,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'created_at': datetime.now().isoformat()
                        }
                    }
                    
                    result = supabase.table('embeddings').insert(insert_data).execute()
                    
                    if result.data:
                        inserted_count += 1
                        if (i + 1) % 10 == 0:
                            self.logger.info(f"Progresso: {i+1}/{len(chunks)} chunks inseridos")
                
                except Exception as chunk_error:
                    self.logger.warning(f"Erro no chunk {i}: {chunk_error}")
                    continue
            
            self.logger.info(f"‚úÖ Carregamento conclu√≠do: {inserted_count}/{len(chunks)} chunks inseridos")
            
            return self._build_response(
                f"‚úÖ CSV carregado com sucesso: {inserted_count} chunks inseridos na base vetorial",
                metadata={
                    'csv_path': csv_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'chunks_created': len(chunks),
                    'chunks_inserted': inserted_count
                }
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar CSV: {str(e)}")
            return self._build_error_response(f"Falha ao carregar CSV: {str(e)}")
