"""
Agente de AnÃ¡lise de Dados via RAG Vetorial Puro com MemÃ³ria Persistente e LangChain.

VERSÃƒO 2.0 - REFATORADA:
- âœ… MemÃ³ria persistente em Supabase (tabelas agent_sessions, agent_conversations, agent_context)
- âœ… LangChain integrado nativamente (ChatOpenAI, ChatGoogleGenerativeAI)
- âœ… MÃ©todos async para performance
- âœ… Contexto conversacional entre interaÃ§Ãµes
- âœ… Busca vetorial pura (sem keywords hardcoded)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  EXCEÃ‡ÃƒO DE CONFORMIDADE: ACESSO DIRETO A CSV PARA VISUALIZAÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTEXTO:
- Tabela 'embeddings' armazena chunks de anÃ¡lises estatÃ­sticas (Markdown)
- VisualizaÃ§Ãµes (histogramas) requerem dados tabulares completos (285k linhas)
- Embeddar cada linha seria ineficiente: ~$50-100 custo + overhead desnecessÃ¡rio

SOLUÃ‡ÃƒO IMPLEMENTADA:
- Quando visualizaÃ§Ã£o Ã© solicitada, acessa CSV diretamente via pd.read_csv()
- Acesso Ã© READ-ONLY, sem modificaÃ§Ã£o de dados
- Log completo de auditoria registrado (linhas 318-350)
- Metadados de conformidade incluÃ­dos em todas as respostas

JUSTIFICATIVA (ADERENTE A BOAS PRÃTICAS DE MERCADO):
1. PadrÃ£o da indÃºstria: LangChain CSV Agents, LlamaIndex, OpenAI Code Interpreter
2. SeparaÃ§Ã£o de responsabilidades: RAG para busca semÃ¢ntica, CSV para dados tabulares
3. Custo-benefÃ­cio: evita armazenamento/processamento desnecessÃ¡rio
4. Performance: leitura direta Ã© mais rÃ¡pida que reconstituiÃ§Ã£o de embeddings

IMPLEMENTAÃ‡ÃƒO FUTURA (Opcional):
- TODO: Adicionar chunks 'raw_data' na tabela embeddings durante ingestÃ£o
- TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
- TODO: Adicionar configuraÃ§Ã£o para escolher entre direct-access vs embeddings

AUDITORIA E COMPLIANCE:
- âœ… Log detalhado com event_type, timestamp, session_id, csv_path, size
- âœ… Metadados em response.metadata['conformidade_exception']
- âœ… DocumentaÃ§Ã£o clara da exceÃ§Ã£o e justificativa
- âœ… AprovaÃ§Ã£o registrada (approved=True)

REFERÃŠNCIAS:
- LangChain CSV Agent: https://python.langchain.com/docs/integrations/toolkits/csv
- OpenAI Code Interpreter: https://openai.com/blog/code-interpreter
- Hybrid RAG Architectures: https://docs.llamaindex.ai/en/stable/examples/query_engine/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import logging
from typing import Any, Dict, List, Optional
import json
from datetime import datetime
import asyncio
import pandas as pd

from src.agent.base_agent import BaseAgent, AgentError
from src.vectorstore.supabase_client import supabase
from src.embeddings.generator import EmbeddingGenerator
from src.utils.logging_config import get_logger
from src.analysis.intent_classifier import IntentClassifier, AnalysisIntent
from src.analysis.orchestrator import AnalysisOrchestrator

# ğŸ”’ SPRINT 3 P0-4: Import do Sandbox Seguro (RestrictedPython)
# Substitui PythonREPLTool inseguro detectado no Sprint 2
from src.security.sandbox import execute_in_sandbox

# âœ… V4.0: Imports para prompts dinÃ¢micos e configuraÃ§Ãµes otimizadas
from src.prompts.dynamic_prompts import DynamicPromptGenerator, DatasetContext
from src.llm.optimized_config import get_configs_for_intent, LLMOptimizedConfig, RAGOptimizedConfig

# Imports LangChain
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    # ğŸ”’ REMOVIDO: PythonREPLTool (vulnerabilidade RCE crÃ­tica)
    # from langchain_experimental.tools import PythonREPLTool
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    AIMessage = None
    print(f"âš ï¸ LangChain nÃ£o disponÃ­vel: {e}")


class RAGDataAgent(BaseAgent):
    def _interpretar_pergunta_llm(self, pergunta: str, df):
        """
        âœ… V3.0: Utiliza IntentClassifier para classificaÃ§Ã£o semÃ¢ntica SEM HARD-CODING.
        
        Retorna instruÃ§Ãµes analÃ­ticas baseadas na intenÃ§Ã£o classificada pela LLM.
        Cada instruÃ§Ã£o Ã© um dict: {'acao': ..., 'colunas': [...], 'params': {}, 'justificativa': str}
        """
        if not self.llm:
            # Fallback: retorna instruÃ§Ã£o genÃ©rica
            return [{'acao': 'estatÃ­sticas gerais', 'colunas': list(df.columns), 'params': {}, 
                    'justificativa': 'LLM indisponÃ­vel, fornecendo estatÃ­sticas gerais.'}]
        
        try:
            # ğŸ”¥ V3.0: Usar IntentClassifier para classificaÃ§Ã£o semÃ¢ntica
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            # Classificar intenÃ§Ã£o da pergunta
            context = {
                'available_columns': list(df.columns),
                'dataframe_info': f"Shape: {df.shape}, Colunas numÃ©ricas: {df.select_dtypes(include=['number']).columns.tolist()}"
            }
            
            classification_result = classifier.classify(query=pergunta, context=context)
            
            # Log da classificaÃ§Ã£o
            self.logger.info({
                'event': 'intent_classification',
                'primary_intent': classification_result.primary_intent.value,
                'secondary_intents': [intent.value for intent in classification_result.secondary_intents],
                'confidence': classification_result.confidence,
                'reasoning': classification_result.reasoning
            })
            
            # ğŸ¯ Mapear intenÃ§Ãµes para instruÃ§Ãµes analÃ­ticas
            instrucoes = []
            
            # Processar intenÃ§Ã£o primÃ¡ria
            primary_action = self._intent_to_action(
                classification_result.primary_intent, 
                df, 
                classification_result.reasoning
            )
            if primary_action:
                instrucoes.append(primary_action)
            
            # Processar intenÃ§Ãµes secundÃ¡rias se existirem
            for secondary_intent in classification_result.secondary_intents:
                secondary_action = self._intent_to_action(
                    secondary_intent, 
                    df, 
                    f"IntenÃ§Ã£o secundÃ¡ria detectada: {secondary_intent.value}"
                )
                if secondary_action and secondary_action not in instrucoes:
                    instrucoes.append(secondary_action)
            
            # Garantir pelo menos uma instruÃ§Ã£o
            if not instrucoes:
                instrucoes = [{
                    'acao': 'estatÃ­sticas gerais',
                    'colunas': list(df.columns),
                    'params': {},
                    'justificativa': 'Nenhuma intenÃ§Ã£o especÃ­fica detectada, fornecendo visÃ£o geral.'
                }]
            
            self.logger.info({
                'event': 'instructions_generated',
                'num_instructions': len(instrucoes),
                'actions': [ins['acao'] for ins in instrucoes]
            })
            
            return instrucoes
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao interpretar pergunta com IntentClassifier: {e}")
            # Fallback para interpretaÃ§Ã£o bÃ¡sica via LLM direta
            return self._fallback_interpretation(pergunta, df)
    
    def _intent_to_action(self, intent: AnalysisIntent, df, justificativa: str) -> Optional[Dict]:
        """
        ğŸ¯ Converte AnalysisIntent em instruÃ§Ã£o analÃ­tica.
        Mapeia tipos de intenÃ§Ã£o para aÃ§Ãµes especÃ­ficas.
        """
        # Selecionar colunas numÃ©ricas por padrÃ£o
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        all_cols = list(df.columns)
        
        intent_map = {
            AnalysisIntent.STATISTICAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.FREQUENCY: {
                'acao': 'frequency_analysis',
                'colunas': all_cols,
                'params': {'top_n': 10},
                'justificativa': justificativa
            },
            AnalysisIntent.TEMPORAL: {
                'acao': 'temporal_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.CLUSTERING: {
                'acao': 'clustering_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {'n_clusters': 3},
                'justificativa': justificativa
            },
            AnalysisIntent.CORRELATION: {
                'acao': 'correlation_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.OUTLIERS: {
                'acao': 'outlier_detection',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.COMPARISON: {
                'acao': 'comparison_analysis',
                'colunas': all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.VISUALIZATION: {
                'acao': 'visualization_request',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.GENERAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            }
        }
        
        return intent_map.get(intent)
    
    def _fallback_interpretation(self, pergunta: str, df) -> List[Dict]:
        """
        Fallback: interpretaÃ§Ã£o bÃ¡sica via LLM direta quando IntentClassifier falha.
        MantÃ©m apenas lÃ³gica essencial, SEM hard-coding de keywords.
        """
        prompt = (
            "VocÃª Ã© um especialista em anÃ¡lise de dados.\n"
            "Interprete a pergunta do usuÃ¡rio e retorne uma lista JSON de instruÃ§Ãµes analÃ­ticas.\n"
            "Formato de cada instruÃ§Ã£o: {'acao': str, 'colunas': [str], 'params': {}, 'justificativa': str}\n"
            f"Pergunta: {pergunta}\n"
            f"Colunas disponÃ­veis: {list(df.columns)}\n"
            "Responda APENAS com o JSON, sem explicaÃ§Ãµes."
        )
        
        try:
            response = self.llm.invoke(prompt)
            import json
            instrucoes = json.loads(response.content)
            return instrucoes if isinstance(instrucoes, list) else [instrucoes]
        except Exception as e:
            self.logger.error(f"Fallback interpretation falhou: {e}")
            return [{
                'acao': 'estatÃ­sticas gerais',
                'colunas': list(df.columns),
                'params': {},
                'justificativa': 'InterpretaÃ§Ã£o padrÃ£o devido a erro.'
            }]
    
    def _build_analytical_response_v3(
        self,
        query: str,
        df: pd.DataFrame,
        context_data: str,
        history_context: str = ""
    ) -> str:
        """
        ğŸ”¥ V3.0: ConstrÃ³i resposta analÃ­tica usando AnalysisOrchestrator.
        
        Substitui ~240 linhas de cascata if/elif por orquestraÃ§Ã£o inteligente.
        
        Args:
            query: Pergunta do usuÃ¡rio
            df: DataFrame carregado
            context_data: Chunks analÃ­ticos do CSV
            history_context: HistÃ³rico conversacional
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            self.logger.info("ğŸ”¥ Usando V3.0: AnalysisOrchestrator")
            
            # Classificar intenÃ§Ã£o via IntentClassifier
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            context_info = {
                'available_columns': list(df.columns),
                'dataframe_shape': df.shape,
                'has_history': bool(history_context)
            }
            
            intent_result = classifier.classify(query, context=context_info)
            
            # Converter IntentClassificationResult para dict de confianÃ§a
            intent_dict = {intent_result.primary_intent.value.upper(): intent_result.confidence}
            
            # Adicionar intenÃ§Ãµes secundÃ¡rias
            for secondary in intent_result.secondary_intents:
                intent_dict[secondary.value.upper()] = 0.75  # ConfianÃ§a padrÃ£o para secundÃ¡rias
            
            self.logger.info(f"IntenÃ§Ãµes detectadas: {intent_dict}")
            
            # Criar orquestrador e executar anÃ¡lises
            orchestrator = AnalysisOrchestrator(llm=self.llm, logger=self.logger)
            
            orchestration_result = orchestrator.orchestrate_v3_direct(
                intent_result=intent_dict,
                df=df,
                confidence_threshold=0.6
            )
            
            # Construir resposta formatada
            response = self._format_orchestrated_response(
                query=query,
                orchestration_result=orchestration_result,
                context_data=context_data,
                history_context=history_context,
                intent_result=intent_result
            )
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ Erro no V3 orchestrator: {e}", exc_info=True)
            # Fallback para resposta bÃ¡sica
            return self._fallback_basic_response(query, context_data, history_context)
    
    def _format_orchestrated_response(
        self,
        query: str,
        orchestration_result: Dict[str, Any],
        context_data: str,
        history_context: str,
        intent_result
    ) -> str:
        """
        Formata resultado orquestrado em resposta humanizada.
        
        Args:
            query: Pergunta original
            orchestration_result: Resultado do orchestrator.orchestrate_v3_direct()
            context_data: Chunks analÃ­ticos
            history_context: HistÃ³rico
            intent_result: ClassificaÃ§Ã£o de intenÃ§Ã£o
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            # Construir prompt para LLM formatar resposta final
            from langchain.schema import SystemMessage, HumanMessage
            
            system_prompt = """
VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© apresentar resultados analÃ­ticos de forma clara e estruturada.

VocÃª receberÃ¡:
1. Pergunta do usuÃ¡rio
2. Resultados de anÃ¡lises executadas (JSON estruturado)
3. Chunks analÃ­ticos do CSV (contexto adicional)
4. HistÃ³rico conversacional (se houver)

Sua resposta deve:
- Iniciar com: "Pergunta feita: [pergunta]"
- Apresentar resultados de forma humanizada e estruturada
- Usar tabelas Markdown quando apropriado
- Destacar insights relevantes
- Finalizar com: "Se precisar de mais detalhes, Ã© sÃ³ perguntar!"
"""
            
            results_summary = []
            for analyzer_name, result in orchestration_result.get('results', {}).items():
                if isinstance(result, dict) and 'error' not in result:
                    results_summary.append(f"**{analyzer_name.title()}**: AnÃ¡lise executada com sucesso")
                elif isinstance(result, dict) and 'error' in result:
                    results_summary.append(f"**{analyzer_name.title()}**: Erro - {result['error']}")
            
            user_prompt = f"""
**Pergunta do UsuÃ¡rio:**
{query}

{history_context}

**Resultados das AnÃ¡lises Executadas:**
{chr(10).join(results_summary)}

**Detalhes Completos (JSON):**
```json
{json.dumps(orchestration_result.get('results', {}), indent=2, ensure_ascii=False)}
```

**Chunks AnalÃ­ticos do CSV (contexto adicional):**
{context_data[:1000]}...

**IntenÃ§Ã£o Detectada:**
- Principal: {intent_result.primary_intent.value}
- ConfianÃ§a: {intent_result.confidence:.1%}
- Justificativa: {intent_result.reasoning}

Apresente os resultados de forma clara, humanizada e estruturada.
"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            return response.content
            
        except Exception as e:
            self.logger.error(f"Erro ao formatar resposta orquestrada: {e}")
            # Fallback: retornar JSON bruto formatado
            import json
            return f"""
Pergunta feita: {query}

**AnÃ¡lises Executadas:**
{chr(10).join([f"- {k}" for k in orchestration_result.get('results', {}).keys()])}

**Resultados:**
```json
{json.dumps(orchestration_result, indent=2, ensure_ascii=False)}
```

Se precisar de mais detalhes, Ã© sÃ³ perguntar!
"""
    
    def _fallback_basic_response(
        self,
        query: str,
        context_data: str,
        history_context: str
    ) -> str:
        """
        Fallback bÃ¡sico quando V3 orchestrator falha completamente.
        """
        from langchain.schema import SystemMessage, HumanMessage
        
        system_prompt = "VocÃª Ã© um agente EDA. Responda Ã  pergunta usando os chunks fornecidos."
        
        user_prompt = f"""
Pergunta: {query}

{history_context}

Chunks analÃ­ticos:
{context_data}

Responda de forma clara e estruturada.
"""
        
        try:
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            return f"Erro ao processar anÃ¡lise: {str(e)}"

    def _executar_instrucao(self, df, instrucao):
        """
        Executa uma instruÃ§Ã£o analÃ­tica sobre o DataFrame.
        Suporta mÃ©tricas nativas pandas/NumPy e delega compostas Ã  LLM.
        """
        acao = instrucao.get('acao','')
        colunas = instrucao.get('colunas', list(df.columns))
        params = instrucao.get('params', {})
        try:
            # Normalizar nome da aÃ§Ã£o para comparaÃ§Ã£o (tolerante a maiÃºsculas/minÃºsculas)
            acao_norm = str(acao).strip().lower()
            # MÃ©tricas nativas pandas
            if acao_norm in ('mÃ©dia', 'media', 'mean'):
                return df[colunas].mean().to_frame(name='MÃ©dia')
            if acao_norm in ('mediana', 'median'):
                return df[colunas].median().to_frame(name='Mediana')
            if acao_norm in ('moda', 'mode'):
                return df[colunas].mode().T
            if acao_norm in ('desvio padrÃ£o', 'desvio padrao', 'std', 'standard deviation'):
                return df[colunas].std().to_frame(name='Desvio padrÃ£o')
            if acao_norm in ('variÃ¢ncia', 'variancia', 'variance', 'var'):
                # VariÃ¢ncia populacional/por padrÃ£o pandas var() usa ddof=1 (amostral). Mantemos default.
                return df[colunas].var().to_frame(name='VariÃ¢ncia')
            if acao_norm in ('intervalo', 'minmax', 'min_max', 'mÃ­nimo', 'mÃ¡ximo'):
                resultado = df[colunas].agg(['min','max']).T
                resultado.columns = ['MÃ­nimo','MÃ¡ximo']
                return resultado
            if acao_norm in ('estatÃ­sticas gerais', 'estatisticas gerais', 'describe', 'summary', 'resumo'):
                return df[colunas].describe().T
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ”’ SPRINT 3 P0-4: MÃ‰TRICAS COMPOSTAS VIA SANDBOX SEGURO
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # MÃ©tricas complexas que requerem cÃ³digo dinÃ¢mico: delega Ã  LLM + SANDBOX
            if self.llm:
                try:
                    # Prompt estruturado para geraÃ§Ã£o de cÃ³digo analÃ­tico
                    prompt = (
                        f"Receba instruÃ§Ã£o analÃ­tica: {instrucao}.\n"
                        f"DataFrame jÃ¡ estÃ¡ disponÃ­vel como variÃ¡vel 'df'.\n"
                        f"Colunas disponÃ­veis: {list(df.columns)}.\n"
                        f"Colunas numÃ©ricas: {df.select_dtypes(include=['number']).columns.tolist()}.\n"
                        "Gere cÃ³digo Python (pandas/numpy) que:\n"
                        "1. Execute a mÃ©trica/anÃ¡lise pedida\n"
                        "2. Armazene o resultado em uma variÃ¡vel chamada 'resultado'\n"
                        "3. O resultado deve ser um DataFrame, Series ou valor escalar\n"
                        "4. Use apenas bibliotecas permitidas: pandas, numpy, math, statistics, datetime\n"
                        "Retorne APENAS o cÃ³digo, sem explicaÃ§Ãµes, sem markdown, sem comentÃ¡rios.\n"
                        "Exemplo vÃ¡lido: resultado = df['coluna'].mean()\n"
                        "Exemplo vÃ¡lido: resultado = df.groupby('categoria')['valor'].sum()"
                    )
                    
                    # Gerar cÃ³digo via LLM
                    response = self.llm.invoke(prompt)
                    code = response.content.strip()
                    
                    # Limpar markdown code blocks se presentes
                    if code.startswith("```python"):
                        code = code.split("```python")[1].split("```")[0].strip()
                    elif code.startswith("```"):
                        code = code.split("```")[1].split("```")[0].strip()
                    
                    # Remover comentÃ¡rios inline (seguranÃ§a: evitar injeÃ§Ã£o via comentÃ¡rios)
                    code_lines = [line for line in code.split('\n') if not line.strip().startswith('#')]
                    code = '\n'.join(code_lines)
                    
                    # Log cÃ³digo antes de executar (auditoria de seguranÃ§a)
                    self.logger.info({
                        'event': 'llm_code_generation',
                        'instrucao': str(instrucao)[:100],
                        'code_generated': code[:200] + ('...' if len(code) > 200 else ''),
                        'code_length': len(code),
                        'timestamp': datetime.now().isoformat()
                    })
                    
                    # ğŸ”’ EXECUÃ‡ÃƒO SEGURA VIA SANDBOX (RestrictedPython)
                    # Substitui PythonREPLTool vulnerÃ¡vel (Sprint 2)
                    sandbox_result = self._executar_codigo_sandbox(
                        code=code,
                        df=df,
                        timeout_seconds=5,
                        memory_limit_mb=100
                    )
                    
                    # Processar resultado do sandbox
                    if sandbox_result.get('success'):
                        resultado = sandbox_result.get('result')
                        exec_time = sandbox_result.get('execution_time_ms', 0)
                        
                        self.logger.info({
                            'event': 'sandbox_execution_success',
                            'execution_time_ms': exec_time,
                            'result_type': type(resultado).__name__,
                            'result_shape': resultado.shape if hasattr(resultado, 'shape') else None,
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        return resultado
                    else:
                        # Erro na execuÃ§Ã£o sandbox
                        error_msg = sandbox_result.get('error', 'Erro desconhecido')
                        error_type = sandbox_result.get('error_type', 'UnknownError')
                        
                        self.logger.error({
                            'event': 'sandbox_execution_failed',
                            'error_type': error_type,
                            'error_message': error_msg[:200],
                            'code_attempted': code[:200],
                            'timestamp': datetime.now().isoformat()
                        })
                        
                        # Log detalhado para debugging
                        self.logger.debug(f"CÃ³digo problemÃ¡tico:\n{code}")
                        self.logger.debug(f"Logs do sandbox: {sandbox_result.get('logs', [])}")
                        
                        return None
                    
                except Exception as e:
                    self.logger.error({
                        'event': 'llm_code_execution_error',
                        'error': str(e),
                        'exception_type': type(e).__name__,
                        'instrucao': str(instrucao)[:100],
                        'timestamp': datetime.now().isoformat()
                    })
                    self.logger.debug(f"CÃ³digo problemÃ¡tico: {code if 'code' in locals() else 'N/A'}")
                    return None
            else:
                self.logger.warning({
                    'event': 'llm_unavailable_for_complex_metrics',
                    'instrucao': str(instrucao)[:100],
                    'timestamp': datetime.now().isoformat()
                })
                return None
        except Exception as e:
            self.logger.warning(f"Falha ao executar instruÃ§Ã£o: {instrucao} | Erro: {e}")
            return None

    def _formatar_taxonomia_tipos(self, tipos_dict: dict) -> str:
        """
        Formata dicionÃ¡rio de tipos de dados em texto estruturado para o prompt da LLM.
        
        Args:
            tipos_dict: DicionÃ¡rio com tipos comuns e suas caracterÃ­sticas
            
        Returns:
            String formatada em markdown para inclusÃ£o no prompt
        """
        texto = ""
        for tipo_key, tipo_info in tipos_dict.items():
            tipo_nome = tipo_key.replace('_', ' ').title()
            texto += f"""
**{tipo_nome}:**
- DescriÃ§Ã£o: {tipo_info['descricao']}
- Exemplos de nomes: {', '.join(tipo_info['exemplos'])}
- Indicadores: {', '.join(tipo_info['indicadores'])}
"""
        return texto

    def _analisar_completo_csv(self, csv_path: str, pergunta: str, override_temporal_col: str = None,
                               temporal_col_names: list = None, accepted_types: tuple = None) -> str:
        """
        âœ… V3.0: AnÃ¡lise inteligente via LLM (SEM LIMITAÃ‡ÃƒO, SEM HARDCODING).
        
        A LLM decide dinamicamente:
        - Se resposta deve ser concisa ou detalhada
        - Quais colunas analisar
        - Como interpretar tipos (ex: Class como booleano disfarÃ§ado de int)
        
        ARQUITETURA MODULAR:
        - DetecÃ§Ã£o via TemporalColumnDetector (src/analysis/temporal_detection.py) - OPCIONAL
        - AnÃ¡lise via LLM para TODAS as colunas (genÃ©rico, nÃ£o limitado)
        - Resposta adaptada Ã  complexidade da pergunta
        
        ParÃ¢metros:
            - override_temporal_col: forÃ§a uso de coluna especÃ­fica (ou None para auto)
            - temporal_col_names: lista de nomes comuns (default: ["time", "date", "timestamp", "data", "datetime"])
            - accepted_types: DEPRECATED - mantido para backward compatibility
            
        Returns:
            String formatada em Markdown com anÃ¡lise adaptada Ã  pergunta
        """
        import pandas as pd
        from src.analysis.temporal_detection import TemporalColumnDetector, TemporalDetectionConfig
        from src.analysis.temporal_analyzer import TemporalAnalyzer
        
        # Carregar dados
        df = pd.read_csv(csv_path)
        
        logger = self.logger if hasattr(self, 'logger') else logging.getLogger(__name__)
        logger.info({
            'event': 'inicio_analise_csv_v3',
            'csv_path': csv_path,
            'shape': df.shape,
            'pergunta': pergunta,
            'override_temporal_col': override_temporal_col
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # âœ… V3.0: LLM INTERPRETA A PERGUNTA E GERA RESPOSTA ADAPTADA
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Detectar se pergunta Ã© SIMPLES (tipos de dados) ou COMPLEXA (anÃ¡lise detalhada)
        pergunta_lower = pergunta.lower()
        keywords_simples = [
            'quais tipos', 'tipos de dados', 'tipo de dado', 'tipos das colunas',
            'colunas numÃ©ricas', 'colunas categÃ³ricas', 'colunas temporais',
            'data types', 'column types', 'tipos das variÃ¡veis'
        ]
        
        is_simple_query = any(keyword in pergunta_lower for keyword in keywords_simples)
        
        if is_simple_query:
            logger.info("ğŸ“‹ Pergunta SIMPLES detectada: LLM interpretarÃ¡ dados e responderÃ¡ de forma humanizada")
            
            # Coletar informaÃ§Ãµes COMPLETAS de TODAS as colunas para anÃ¡lise pela LLM
            colunas_info = []
            
            for col in df.columns:
                col_data = df[col]
                dtype = str(col_data.dtype)
                unique_count = col_data.nunique()
                null_count = col_data.isnull().sum()
                sample_values = col_data.dropna().head(10).tolist()
                
                # EstatÃ­sticas bÃ¡sicas para LLM analisar
                info = {
                    'nome': col,
                    'dtype_python': dtype,
                    'valores_unicos': unique_count,
                    'valores_nulos': null_count,
                    'amostra_valores': sample_values,
                    'total_linhas': len(col_data)
                }
                
                # Adicionar estatÃ­sticas numÃ©ricas se aplicÃ¡vel
                if pd.api.types.is_numeric_dtype(col_data):
                    info['min'] = float(col_data.min())
                    info['max'] = float(col_data.max())
                    info['media'] = float(col_data.mean())
                    info['std'] = float(col_data.std())
                
                colunas_info.append(info)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ§  PROMPT ENGINEERING: Sistema Adaptativo de ClassificaÃ§Ã£o de Tipos
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # ğŸ“‹ CONFIGURAÃ‡ÃƒO: Tipos de dados comuns (extensÃ­vel via config)
            TIPOS_COMUNS = {
                'temporal': {
                    'descricao': 'Dados relacionados a tempo, data, ou marcadores temporais',
                    'exemplos': ['Time', 'Date', 'Timestamp', 'Duration', 'Year', 'Month'],
                    'indicadores': ['representam momentos, perÃ­odos ou duraÃ§Ãµes']
                },
                'categorica': {
                    'descricao': 'Dados discretos que representam categorias, classes ou grupos',
                    'exemplos': ['Class', 'Category', 'Type', 'Label', 'Status', 'Gender'],
                    'indicadores': ['valores distintos limitados', 'categorias predefinidas', 'classes binomiais ou multinomiais']
                },
                'numerica_continua': {
                    'descricao': 'Valores numÃ©ricos contÃ­nuos usados para mediÃ§Ãµes ou cÃ¡lculos',
                    'exemplos': ['Amount', 'Price', 'Temperature', 'Distance', 'Weight'],
                    'indicadores': ['mediÃ§Ãµes', 'quantidades', 'valores monetÃ¡rios', 'mÃ©tricas']
                },
                'numerica_discreta': {
                    'descricao': 'Valores numÃ©ricos inteiros representando contagens',
                    'exemplos': ['Count', 'Quantity', 'Age', 'ID', 'Rank'],
                    'indicadores': ['contagens', 'nÃºmeros inteiros', 'identificadores sequenciais']
                },
                'booleana': {
                    'descricao': 'Valores lÃ³gicos verdadeiro/falso (literal)',
                    'exemplos': ['is_active', 'has_discount', 'True/False'],
                    'indicadores': ['apenas valores True/False', 'nÃ£o confundir com categÃ³ricas binÃ¡rias 0/1']
                },
                'textual': {
                    'descricao': 'Texto livre ou strings descritivas',
                    'exemplos': ['Description', 'Comment', 'Name', 'Address'],
                    'indicadores': ['texto longo', 'descriÃ§Ãµes', 'nomes prÃ³prios']
                },
                'mista': {
                    'descricao': 'Colunas com tipos mistos ou dados heterogÃªneos',
                    'exemplos': ['mixed_data', 'various_formats'],
                    'indicadores': ['mÃºltiplos tipos no mesmo campo', 'dados inconsistentes']
                }
            }
            
            # âœ… USAR LLM COM INTELIGÃŠNCIA TOTAL E PROMPT SOFISTICADO
            if self.llm and LANGCHAIN_AVAILABLE:
                try:
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # PARTE 1: SISTEMA - Define o domÃ­nio e capacidades do agente
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    system_prompt = f"""VocÃª Ã© um cientista de dados sÃªnior especializado em anÃ¡lise exploratÃ³ria de dados (EDA).

**SUAS CAPACIDADES:**
- AnÃ¡lise semÃ¢ntica profunda de estruturas de dados
- ClassificaÃ§Ã£o adaptativa de tipos de dados
- InterpretaÃ§Ã£o contextual de nomes de colunas
- DetecÃ§Ã£o de padrÃµes estatÃ­sticos em amostras
- IdentificaÃ§Ã£o de tipos nÃ£o convencionais ou mistos

**DOMÃNIO DO PROBLEMA:**
VocÃª analisarÃ¡ colunas de um dataset CSV e classificarÃ¡ seus tipos de dados de forma inteligente, considerando:
1. **Contexto semÃ¢ntico**: nome da coluna e significado no domÃ­nio
2. **Estrutura dos dados**: dtype Python, valores Ãºnicos, distribuiÃ§Ã£o
3. **Amostras reais**: padrÃµes observados nos valores
4. **Cardinalidade**: quantidade de valores distintos vs total de registros

**TAXONOMIA DE TIPOS (nÃ£o limitada a estes):**
Os tipos comuns incluem:

{self._formatar_taxonomia_tipos(TIPOS_COMUNS)}

**IMPORTANTE - ADAPTABILIDADE:**
- Esta lista NÃƒO Ã© exaustiva. VocÃª pode identificar tipos hÃ­bridos, especializados ou atÃ­picos
- Se encontrar um tipo que nÃ£o se encaixa perfeitamente, descreva-o com suas prÃ³prias palavras
- Priorize o SIGNIFICADO CONTEXTUAL sobre o dtype tÃ©cnico
- Para dados binÃ¡rios (0/1), avalie se sÃ£o categÃ³ricos (classes) ou numÃ©ricos (contadores)

**REGRAS DE OURO:**
1. **Time/Timestamp numÃ©rico** â†’ Classifique como TEMPORAL (nÃ£o numÃ©rico)
2. **BinÃ¡rios 0/1 como categorias** â†’ Classifique como CATEGÃ“RICA BINÃRIA (nÃ£o booleana)
3. **Booleana** â†’ Reserve APENAS para True/False literal
4. **IDs numÃ©ricos sequenciais** â†’ Podem ser IDENTIFICADORES (nÃ£o numÃ©ricos contÃ­nuos)
5. **Tipos ambÃ­guos** â†’ Explique a ambiguidade e sugira interpretaÃ§Ã£o baseada no contexto

**TOM DE COMUNICAÃ‡ÃƒO:**
- Humanizado, didÃ¡tico e conversacional
- Explique o RACIOCÃNIO por trÃ¡s de cada classificaÃ§Ã£o
- Use analogias quando Ãºtil
- Seja acessÃ­vel para nÃ£o-especialistas
- Demonstre entusiasmo pelo trabalho analÃ­tico"""
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # PARTE 2: DADOS - Prepara informaÃ§Ãµes detalhadas das colunas
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    colunas_detalhadas = []
                    for c in colunas_info:
                        detalhes = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š Coluna: **{c['nome']}**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  ğŸ”¹ Tipo Python: {c['dtype_python']}
  ğŸ”¹ Cardinalidade: {c['valores_unicos']} valores Ãºnicos em {c['total_linhas']:,} registros ({c['valores_unicos']/c['total_linhas']*100:.1f}% de diversidade)
  ğŸ”¹ Valores ausentes: {c['valores_nulos']} ({c['valores_nulos']/c['total_linhas']*100:.1f}%)
  ğŸ”¹ Amostra (10 primeiros valores): {c['amostra_valores']}"""
                        
                        if 'min' in c:
                            detalhes += f"""
  ğŸ”¹ Faixa numÃ©rica: [{c['min']:.2f}, {c['max']:.2f}]
  ğŸ”¹ MÃ©dia: {c['media']:.2f} | Desvio padrÃ£o: {c['std']:.2f}"""
                        
                        colunas_detalhadas.append(detalhes)
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # PARTE 3: TAREFA - Define o que fazer com formataÃ§Ã£o esperada
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    user_prompt = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“ DATASET PARA ANÃLISE                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Arquivo:** {csv_path}
**DimensÃµes:** {len(df.columns)} colunas Ã— {len(df):,} linhas

{''.join(colunas_detalhadas)}

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    â“ PERGUNTA DO USUÃRIO                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{pergunta}

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ“ INSTRUÃ‡Ã•ES PARA RESPOSTA                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Seu trabalho:**
1. Analise CADA coluna usando sua inteligÃªncia semÃ¢ntica e estatÃ­stica
2. Classifique o tipo de cada coluna baseando-se em:
   - Nome e significado contextual
   - PadrÃµes nos valores amostrados
   - DistribuiÃ§Ã£o estatÃ­stica
   - Finalidade provÃ¡vel no dataset
3. Agrupe colunas por tipo identificado
4. Explique brevemente o MOTIVO de cada classificaÃ§Ã£o
5. Se identificar tipos atÃ­picos ou mistos, descreva-os

**Formato da resposta:**
- Use tom conversacional e didÃ¡tico
- Estruture em seÃ§Ãµes por tipo (use emojis para clareza)
- Para cada tipo, liste as colunas e explique resumidamente
- Finalize com uma observaÃ§Ã£o geral sobre o dataset

**Exemplo de resposta humanizada e adaptativa:**

"OlÃ¡! ğŸ‘‹ Analisando esse dataset de transaÃ§Ãµes financeiras, identifiquei os seguintes tipos de dados:

**â±ï¸ Colunas Temporais (1 coluna)**
- **Time**: Representa o momento de cada transaÃ§Ã£o em segundos desde o inÃ­cio da coleta. Embora seja numÃ©rico (int64), seu significado Ã© claramente temporal, marcando quando cada evento ocorreu.

**ğŸ·ï¸ Colunas CategÃ³ricas (1 coluna)**
- **Class**: VariÃ¡vel binÃ¡ria (0 ou 1) que indica a classe da transaÃ§Ã£o. Apesar de ser numÃ©rica, funciona como uma categoria binomial, onde 0 = transaÃ§Ã£o normal e 1 = transaÃ§Ã£o fraudulenta. Ã‰ o rÃ³tulo alvo para classificaÃ§Ã£o.

**ğŸ’° Colunas NumÃ©ricas ContÃ­nuas (29 colunas)**
- **Amount**: Valor monetÃ¡rio da transaÃ§Ã£o em unidade monetÃ¡ria nÃ£o especificada
- **V1 a V28**: CaracterÃ­sticas numÃ©ricas resultantes de transformaÃ§Ã£o PCA (Principal Component Analysis), representando padrÃµes ocultos nos dados originais. Mantidas anÃ´nimas por questÃµes de privacidade.

**ObservaÃ§Ã£o geral:** Este Ã© um dataset tÃ­pico de detecÃ§Ã£o de fraude, com features anonimizadas (V1-V28) para proteger dados sensÃ­veis, uma marcaÃ§Ã£o temporal e um rÃ³tulo binÃ¡rio de classificaÃ§Ã£o. ğŸ”"

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸš€ AGORA Ã‰ SUA VEZ - ANALISE E RESPONDA!            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
                    
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    
                    response = self.llm.invoke(messages)
                    resposta_llm = response.content
                    
                    logger.info("âœ… Resposta humanizada gerada via LLM com anÃ¡lise semÃ¢ntica profunda")
                    return resposta_llm
                
                except Exception as e:
                    logger.error(f"Erro ao usar LLM para resposta inteligente: {e}", exc_info=True)
                    # Fallback: resposta manual
            
            # Fallback manual (caso LLM indisponÃ­vel)
            logger.warning("âš ï¸ LLM indisponÃ­vel - usando fallback de anÃ¡lise bÃ¡sica")
            
            resposta = f"""# AnÃ¡lise dos Tipos de Dados

Analisando o dataset `{csv_path}` com {len(df.columns)} colunas e {len(df):,} linhas.

"""
            
            # AnÃ¡lise manual bÃ¡sica
            for info in colunas_info:
                col_name = info['nome']
                unique = info['valores_unicos']
                
                # AnÃ¡lise bÃ¡sica por nome e padrÃ£o
                if 'time' in col_name.lower() or 'date' in col_name.lower() or 'timestamp' in col_name.lower():
                    resposta += f"- **{col_name}**: Temporal (marcador de tempo)\n"
                elif unique == 2 and 'class' in col_name.lower():
                    resposta += f"- **{col_name}**: CategÃ³rica binÃ¡ria (2 categorias: {info['amostra_valores'][:2]})\n"
                elif pd.api.types.is_numeric_dtype(df[col_name]):
                    resposta += f"- **{col_name}**: NumÃ©rica\n"
                else:
                    resposta += f"- **{col_name}**: CategÃ³rica\n"
            
            return resposta
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PERGUNTA COMPLEXA: AnÃ¡lise detalhada (manter cÃ³digo original)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        logger.info("ğŸ“Š Pergunta COMPLEXA detectada: anÃ¡lise detalhada")
        
        respostas = []  # âœ… Inicializar lista de respostas
        temporal_cols = []
        
        # Configurar detector com parÃ¢metros customizÃ¡veis
        detection_config = TemporalDetectionConfig()
        if temporal_col_names:
            detection_config.common_names = temporal_col_names
        
        detector = TemporalColumnDetector(config=detection_config)
        
        try:
            detection_results = detector.detect(df, override_column=override_temporal_col)
            temporal_cols = detector.get_detected_columns(detection_results)
            
            logger.info({
                'event': 'deteccao_temporal_concluida',
                'colunas_temporais_detectadas': temporal_cols,
                'total_colunas': len(df.columns)
            })
            
            # Analisar colunas temporais com TemporalAnalyzer
            if temporal_cols:
                analyzer = TemporalAnalyzer(logger=logger)
                
                for col in temporal_cols:
                    try:
                        result = analyzer.analyze(df, col, enable_advanced=True)
                        respostas.append(result.to_markdown())
                        
                        logger.info({
                            'event': 'analise_temporal_coluna_concluida',
                            'coluna': col,
                            'tipo': 'temporal'
                        })
                    except Exception as e:
                        logger.error(f"Erro ao analisar coluna temporal '{col}': {e}", exc_info=True)
                        respostas.append(
                            f"## Erro na AnÃ¡lise Temporal: {col}\n\n"
                            f"NÃ£o foi possÃ­vel completar a anÃ¡lise temporal: {str(e)}\n"
                        )
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o temporal: {e}", exc_info=True)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 2: ANÃLISE ESTATÃSTICA DE TODAS AS OUTRAS COLUNAS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Identificar colunas NÃƒO temporais para anÃ¡lise estatÃ­stica completa
        non_temporal_cols = [col for col in df.columns if col not in temporal_cols]
        
        logger.info({
            'event': 'inicio_analise_estatistica_colunas',
            'total_colunas_nao_temporais': len(non_temporal_cols),
            'colunas': non_temporal_cols
        })
        
        # AnÃ¡lise estatÃ­stica completa para cada coluna nÃ£o-temporal
        for col in non_temporal_cols:
            try:
                col_data = df[col]
                
                # AnÃ¡lise NUMÃ‰RICA
                if pd.api.types.is_numeric_dtype(col_data):
                    stats_dict = {
                        'count': col_data.count(),
                        'mean': col_data.mean(),
                        'std': col_data.std(),
                        'min': col_data.min(),
                        '25%': col_data.quantile(0.25),
                        '50%': col_data.quantile(0.50),
                        '75%': col_data.quantile(0.75),
                        'max': col_data.max(),
                        'nulls': col_data.isnull().sum(),
                        'unique': col_data.nunique()
                    }
                    
                    analise_md = f"""## AnÃ¡lise EstatÃ­stica: {col}

**Tipo:** NumÃ©rica ({col_data.dtype})

### EstatÃ­sticas Descritivas

| MÃ©trica | Valor |
|---------|-------|
| Contagem | {stats_dict['count']:,} |
| MÃ©dia | {stats_dict['mean']:.6f} |
| Desvio PadrÃ£o | {stats_dict['std']:.6f} |
| MÃ­nimo | {stats_dict['min']} |
| Q1 (25%) | {stats_dict['25%']} |
| Mediana (50%) | {stats_dict['50%']} |
| Q3 (75%) | {stats_dict['75%']} |
| MÃ¡ximo | {stats_dict['max']} |
| Valores Nulos | {stats_dict['nulls']} |
| Valores Ãšnicos | {stats_dict['unique']} |

### InterpretaÃ§Ã£o

- **Amplitude:** {stats_dict['max'] - stats_dict['min']:.6f}
- **IQR:** {stats_dict['75%'] - stats_dict['25%']:.6f}
- **Coef. VariaÃ§Ã£o:** {(stats_dict['std']/stats_dict['mean']*100) if stats_dict['mean'] != 0 else 0:.2f}%
"""
                    respostas.append(analise_md)
                
                # AnÃ¡lise CATEGÃ“RICA
                else:
                    freq = col_data.value_counts().head(10)
                    
                    analise_md = f"""## AnÃ¡lise EstatÃ­stica: {col}

**Tipo:** CategÃ³rica ({col_data.dtype})

### EstatÃ­sticas Descritivas

| MÃ©trica | Valor |
|---------|-------|
| Contagem | {col_data.count():,} |
| Valores Nulos | {col_data.isnull().sum()} |
| Valores Ãšnicos | {col_data.nunique()} |
| Moda | {col_data.mode().iloc[0] if not col_data.mode().empty else 'N/A'} |

### DistribuiÃ§Ã£o de FrequÃªncia (Top 10)

{freq.to_markdown()}

### InterpretaÃ§Ã£o

- **Valor mais frequente:** {freq.idxmax() if not freq.empty else 'N/A'}
- **FrequÃªncia:** {freq.max() if not freq.empty else 0} ({freq.max()/len(col_data)*100 if not freq.empty else 0:.2f}%)
"""
                    respostas.append(analise_md)
                
                logger.info({
                    'event': 'analise_estatistica_coluna_concluida',
                    'coluna': col,
                    'tipo': 'numerica' if pd.api.types.is_numeric_dtype(col_data) else 'categorica'
                })
                
            except Exception as e:
                logger.error(f"Erro ao analisar coluna '{col}': {e}", exc_info=True)
                respostas.append(
                    f"## Erro na AnÃ¡lise: {col}\n\n"
                    f"NÃ£o foi possÃ­vel completar a anÃ¡lise: {str(e)}\n"
                )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 3: CONSOLIDAR RESULTADOS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if respostas:
            header = (
                f"# AnÃ¡lise Completa do Dataset\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"**Total de colunas:** {len(df.columns)}\n\n"
                f"**Colunas temporais:** {len(temporal_cols)}\n\n"
                f"**Colunas numÃ©ricas/categÃ³ricas:** {len(non_temporal_cols)}\n\n"
                f"**Linhas:** {len(df):,}\n\n"
                "---\n\n"
            )
            
            return header + "\n\n---\n\n".join(respostas)
        else:
            # Fallback final: estatÃ­sticas gerais descritivas
            logger.warning("Fallback final: retornando describe() do DataFrame")
            return (
                f"# EstatÃ­sticas Descritivas\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"{df.describe().T.to_markdown()}"
            )

    def _should_use_global_csv(self, query: str, chunks_metadata: List[Dict]) -> bool:
        """
        Decide se Ã© necessÃ¡rio recorrer ao CSV completo para responder a pergunta.
        CritÃ©rios:
        - Pergunta exige anÃ¡lise de todas as colunas/linhas (ex: intervalo de todas variÃ¡veis)
        - Chunks nÃ£o possuem dados suficientes (ex: subset de colunas)
        """
        # Detecta termos que indicam anÃ¡lise global
        termos_globais = ["todas as variÃ¡veis", "todas as colunas", "intervalo de cada variÃ¡vel", "intervalo de todas", "intervalo completo", "todas as linhas", "anÃ¡lise completa"]
        if any(t in query.lower() for t in termos_globais):
            return True
        # Detecta se chunks nÃ£o cobrem todas as colunas
        if chunks_metadata:
            # Extrai colunas presentes nos chunks
            colunas_chunks = set()
            for chunk in chunks_metadata:
                if 'columns' in chunk:
                    colunas_chunks.update(chunk['columns'])
            # Se nÃºmero de colunas for pequeno, pode indicar subset
            if len(colunas_chunks) < 5:
                return True
        return False

    def reset_memory(self, session_id: str = None):
        """
        Reseta a memÃ³ria/contexto do agente para a sessÃ£o informada.
        """
        self.memory = {}
        if session_id:
            self.session_id = session_id
        self.logger.info(f"MemÃ³ria/contexto resetados para sessÃ£o: {session_id}")
    """
    Agente que responde perguntas sobre dados usando RAG vetorial + memÃ³ria persistente + LangChain.
    
    Fluxo V2.0:
    1. Inicializa sessÃ£o de memÃ³ria (se nÃ£o existir)
    2. Recupera contexto conversacional anterior
    3. Gera embedding da pergunta
    4. Busca chunks similares nos DADOS usando match_embeddings()
    5. Usa LangChain LLM para interpretar chunks + contexto histÃ³rico
    6. Salva interaÃ§Ã£o na memÃ³ria persistente
    7. Retorna resposta contextualizada
    
    SEM keywords hardcoded, SEM classificaÃ§Ã£o manual, SEM listas fixas.
    COM memÃ³ria persistente, COM LangChain, COM contexto conversacional.
    """
    
    def __init__(self):
        super().__init__(
            name="rag_data_analyzer",
            description="Analisa dados usando busca vetorial semÃ¢ntica pura com memÃ³ria persistente",
            enable_memory=True  # âœ… CRÃTICO: Habilita memÃ³ria persistente
        )
        self.logger = get_logger("agent.rag_data")
        self.embedding_gen = EmbeddingGenerator()
        
        # Inicializar LLM LangChain
        self._init_langchain_llm()
        
        # âœ… V4.0: Inicializar gerador de prompts dinÃ¢micos e cache de contexto
        self.prompt_generator = DynamicPromptGenerator()
        self.current_dataset_context: Optional[DatasetContext] = None
        
        self.logger.info("âœ… RAGDataAgent V4.0 inicializado - prompts dinÃ¢micos + parÃ¢metros otimizados + memÃ³ria")
    
    def _init_langchain_llm(self):
        """Inicializa LLM via camada de abstraÃ§Ã£o LangChainLLMManager.
        
        âœ… V4.1: Refatorado para usar abstraÃ§Ã£o existente (elimina duplicaÃ§Ã£o).
        Ordem de prioridade: GROQ â†’ Google â†’ OpenAI (via LangChainLLMManager)
        """
        try:
            from src.llm.langchain_manager import get_langchain_llm_manager, LLMConfig
            
            # Obter instÃ¢ncia singleton do manager
            manager = get_langchain_llm_manager()
            
            # Criar configuraÃ§Ã£o para o LLM
            config = LLMConfig(temperature=0.3, max_tokens=2000, top_p=0.25)
            
            # Obter cliente LangChain do provedor ativo
            self.llm = manager._get_client(manager.active_provider, config)
            
            self.logger.info(
                f"âœ… LLM inicializado via abstraÃ§Ã£o: {manager.active_provider.value.upper()} "
                f"(fallback automÃ¡tico: GROQ â†’ Google â†’ OpenAI)"
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Falha ao inicializar LLM via abstraÃ§Ã£o: {e}")
            self.llm = None
            self.logger.warning("âš ï¸ Sistema operando sem LLM - funcionalidade limitada")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # âœ… V4.0: MÃ‰TODO PARA ATUALIZAR CONTEXTO DO DATASET
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _update_dataset_context(self, df: pd.DataFrame, file_path: str) -> DatasetContext:
        """
        Atualiza contexto do dataset baseado no DataFrame REAL.
        
        ELIMINA HARDCODING:
        - Detecta automaticamente nÃºmero de colunas com df.shape
        - Extrai dtypes reais com df.dtypes
        - Identifica categÃ³ricas binÃ¡rias (ex: Class com {0,1})
        - Calcula estatÃ­sticas reais com df.describe()
        
        Args:
            df: DataFrame carregado do CSV
            file_path: Caminho do arquivo CSV
            
        Returns:
            DatasetContext com tipos, colunas e estatÃ­sticas REAIS
        """
        try:
            context = DatasetContext.from_dataframe(df, file_path)
            self.current_dataset_context = context
            
            self.logger.info({
                'event': 'dataset_context_updated',
                'file': file_path,
                'shape': df.shape,
                'numeric_cols': len(context.numeric_columns),
                'categorical_cols': len(context.categorical_columns),
                'temporal_cols': len(context.temporal_columns),
                'memory_usage_mb': context.memory_usage_mb
            })
            
            return context
        except Exception as e:
            self.logger.error(f"âŒ Erro ao atualizar contexto do dataset: {e}", exc_info=True)
            return None
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ”’ SPRINT 3 P0-4: MÃ‰TODO DE EXECUÃ‡ÃƒO SEGURA VIA SANDBOX
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def _executar_codigo_sandbox(
        self, 
        code: str, 
        df: pd.DataFrame,
        timeout_seconds: int = 5,
        memory_limit_mb: int = 100
    ) -> Dict[str, Any]:
        """
        ğŸ”’ Executa cÃ³digo Python dinÃ¢mico de forma SEGURA usando RestrictedPython sandbox.
        
        SEGURANÃ‡A (5 camadas):
        1. CompilaÃ§Ã£o restritiva (bloqueia eval, exec, compile)
        2. Whitelist de imports (pandas, numpy, math, statistics, datetime, json, collections, re)
        3. Blacklist de imports perigosos (os, subprocess, sys, socket, urllib, requests, etc.)
        4. Ambiente isolado (sem acesso a open, __import__, globals, locals)
        5. Limites de recursos (timeout 5s, memÃ³ria 100MB)
        
        Args:
            code: CÃ³digo Python a executar (deve definir variÃ¡vel 'resultado')
            df: DataFrame pandas disponÃ­vel como variÃ¡vel 'df' no contexto
            timeout_seconds: Tempo mÃ¡ximo de execuÃ§Ã£o (default: 5s)
            memory_limit_mb: Limite de memÃ³ria (default: 100MB, apenas Unix/Linux)
            
        Returns:
            Dict com chaves:
            - success (bool): True se execuÃ§Ã£o bem-sucedida
            - result (Any): Resultado da execuÃ§Ã£o (valor da variÃ¡vel 'resultado')
            - error (str): Mensagem de erro se falha
            - error_type (str): Tipo do erro (SandboxImportError, SandboxTimeoutError, etc.)
            - execution_time_ms (float): Tempo de execuÃ§Ã£o em milissegundos
            - logs (List[str]): Logs de auditoria da execuÃ§Ã£o
            
        Raises:
            Nunca levanta exceÃ§Ãµes - sempre retorna dict com 'success': False em caso de erro
            
        Example:
            >>> code = "resultado = df['Amount'].mean()"
            >>> result = agent._executar_codigo_sandbox(code, df)
            >>> if result['success']:
            ...     print(f"MÃ©dia: {result['result']}")
            >>> else:
            ...     print(f"Erro: {result['error']}")
        """
        # Preparar contexto global seguro (DataFrame disponÃ­vel)
        import pandas as pd
        import numpy as np
        
        # Log de auditoria ANTES da execuÃ§Ã£o
        self.logger.info({
            'event': 'sandbox_execution_request',
            'code_length': len(code),
            'code_preview': code[:200] + ('...' if len(code) > 200 else ''),
            'timeout_seconds': timeout_seconds,
            'memory_limit_mb': memory_limit_mb,
            'dataframe_shape': df.shape if df is not None else None,
            'timestamp': datetime.now().isoformat()
        })
        
        try:
            # ğŸ”’ PREPARAR VARIÃVEIS GLOBAIS CUSTOMIZADAS (DataFrame + bibliotecas)
            custom_globals = {
                'df': df,  # DataFrame disponÃ­vel como 'df' no cÃ³digo
                'pd': pd,  # pandas disponÃ­vel como 'pd'
                'np': np   # numpy disponÃ­vel como 'np'
            }
            
            # ğŸ”’ EXECUÃ‡ÃƒO SEGURA via RestrictedPython
            sandbox_result = execute_in_sandbox(
                code=code,
                timeout_seconds=timeout_seconds,
                memory_limit_mb=memory_limit_mb,
                allowed_imports=['pandas', 'numpy', 'math', 'statistics', 'datetime', 'json', 'collections', 're'],
                return_variable='resultado',
                custom_globals=custom_globals  # ğŸ”‘ INJETAR VARIÃVEIS NO SANDBOX
            )
            
            # Validar que sandbox retornou dict vÃ¡lido
            if not isinstance(sandbox_result, dict):
                raise TypeError(f"Sandbox retornou tipo invÃ¡lido: {type(sandbox_result)}")
            
            # Log de auditoria APÃ“S execuÃ§Ã£o
            self.logger.info({
                'event': 'sandbox_execution_completed',
                'success': sandbox_result.get('success', False),
                'execution_time_ms': sandbox_result.get('execution_time_ms', 0),
                'error_type': sandbox_result.get('error_type'),
                'error_message': str(sandbox_result.get('error', ''))[:200],  # Truncar erros longos
                'logs_count': len(sandbox_result.get('logs', [])),
                'timestamp': datetime.now().isoformat()
            })
            
            # Registrar logs do sandbox no logger principal
            for log_entry in sandbox_result.get('logs', []):
                self.logger.debug(f"[SANDBOX LOG] {log_entry}")
            
            return sandbox_result
            
        except Exception as e:
            # Fallback extremo: erro na prÃ³pria chamada do sandbox
            import traceback
            error_msg = f"Erro crÃ­tico ao chamar sandbox: {str(e)}"
            traceback_str = traceback.format_exc()
            
            self.logger.error({
                'event': 'sandbox_execution_critical_error',
                'error': error_msg,
                'exception_type': type(e).__name__,
                'traceback': traceback_str[:500],  # Truncar traceback
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'success': False,
                'result': None,
                'error': error_msg,
                'error_type': 'CriticalSandboxError',
                'execution_time_ms': 0.0,
                'logs': [f"ERRO CRÃTICO: {error_msg}", f"Traceback: {traceback_str}"]
            }
    
    async def process(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,  # Pylance: context estÃ¡ definido aqui
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa query do usuÃ¡rio usando RAG vetorial + memÃ³ria persistente.
        
        VERSÃƒO ASYNC com memÃ³ria persistente.
        
        Args:
            query: Pergunta do usuÃ¡rio
            context: Contexto adicional (opcional) - DEFINIDO NO ESCOPO
            session_id: ID da sessÃ£o para memÃ³ria persistente
            
        Returns:
            Resposta baseada em busca vetorial + contexto histÃ³rico
        """
        start_time = datetime.now()  # Pylance: start_time estÃ¡ definido aqui
        
        try:
            self.logger.info(f"ğŸ” Processando query via RAG V2.0: {query[:80]}...")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 1. INICIALIZAR MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if not self._current_session_id:
                if session_id:
                    await self.init_memory_session(session_id)
                else:
                    session_id = await self.init_memory_session()
                self.logger.info(f"âœ… SessÃ£o de memÃ³ria inicializada: {session_id}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 2. RECUPERAR CONTEXTO CONVERSACIONAL ANTERIOR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # FILTRAR CONTEXTO: manter apenas campos relevantes para anÃ¡lise
            memory_context = {}
            if context:
                filtered_context = {}
                if 'chunks' in context:
                    filtered_context['chunks'] = context['chunks']
                if 'csv_data' in context:
                    filtered_context['csv_data'] = context['csv_data']
                # âœ… PRESERVAR FLAGS DE VISUALIZAÃ‡ÃƒO
                if 'visualization_requested' in context:
                    filtered_context['visualization_requested'] = context['visualization_requested']
                if 'visualization_type' in context:
                    filtered_context['visualization_type'] = context['visualization_type']
                if 'fallback_sample_limit' in context:
                    filtered_context['fallback_sample_limit'] = context['fallback_sample_limit']
                if 'reconstructed_df' in context:
                    filtered_context['reconstructed_df'] = context['reconstructed_df']
                context = filtered_context
            # NÃƒO recuperar contexto de memÃ³ria para queries de intervalo
            interval_terms = ['intervalo', 'mÃ­nimo', 'mÃ¡ximo', 'range', 'amplitude']
            if any(term in query.lower() for term in interval_terms):
                memory_context = {}  # Ignorar histÃ³rico/memÃ³ria
            elif self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(
                    f"âœ… Contexto de memÃ³ria recuperado: "
                    f"{len(memory_context.get('recent_messages', []))} mensagens anteriores"
                )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 3. GERAR EMBEDDING DA QUERY
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug("Gerando embedding da query...")
            embedding_result = self.embedding_gen.generate_embedding(query)
            
            # Extrair lista de floats do resultado
            if isinstance(embedding_result, list):
                query_embedding = embedding_result
            elif hasattr(embedding_result, 'embedding'):
                query_embedding = embedding_result.embedding
            else:
                return self._build_error_response("Formato de embedding invÃ¡lido")
            
            if not query_embedding or len(query_embedding) == 0:
                return self._build_error_response("Falha ao gerar embedding da query")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # âœ… V4.0: CLASSIFICAR INTENT E OBTER CONFIGURAÃ‡Ã•ES OTIMIZADAS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            llm_config, rag_config = None, None
            if self.llm:
                try:
                    classifier = IntentClassifier(llm=self.llm, logger=self.logger)
                    classification_result = classifier.classify(query=query, context={})
                    
                    # Obter configuraÃ§Ãµes otimizadas baseadas na intenÃ§Ã£o
                    llm_config, rag_config = get_configs_for_intent(classification_result.primary_intent.value)
                    
                    self.logger.info({
                        'event': 'v4_configs_applied',
                        'intent': classification_result.primary_intent.value,
                        'temperature': llm_config.temperature,
                        'max_tokens': llm_config.max_tokens,
                        'rag_threshold': rag_config.similarity_threshold,  # âœ… V4.1: Corrigido nome do atributo
                        'rag_max_chunks': rag_config.max_chunks
                    })
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Falha ao classificar intent, usando configs default: {e}")
            
            # Usar configuraÃ§Ãµes otimizadas ou defaults (âœ… V4.1: Corrigido atributo)
            rag_threshold = rag_config.similarity_threshold if rag_config else 0.3
            rag_limit = rag_config.max_chunks if rag_config else 10
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 4. BUSCAR CHUNKS SIMILARES NOS DADOS (com configs otimizados)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug(f"Buscando chunks similares (threshold={rag_threshold}, limit={rag_limit})...")
            similar_chunks = self._search_similar_data(
                query_embedding=query_embedding,
                threshold=rag_threshold,  # âœ… V4.0: Threshold otimizado (0.6-0.65 vs 0.3)
                limit=rag_limit  # âœ… V4.0: Max chunks otimizado (10)
            )
            
            # SALVAR CONTEXTO DE DADOS NA TABELA agent_context
            if self.has_memory and self._current_session_id and similar_chunks:
                data_context = {
                    "dataset_info": {
                        "total_chunks": len(similar_chunks),
                        "source_types": list(set(c.get('source_type', 'unknown') for c in similar_chunks)),
                        "embedding_provider": "sentence-transformer",
                        "last_query": query[:100],
                        "query_timestamp": datetime.now().isoformat()
                    },
                    "performance_metrics": {
                        "embedding_generation_time": "N/A",  # Poderia ser medido
                        "search_time": "N/A",  # Poderia ser medido
                        "chunks_found": len(similar_chunks)
                    }
                }
                
                try:
                    await self.remember_data_context(
                        data_info=data_context,
                        context_key="current_dataset_info"
                    )
                    self.logger.debug("âœ… Contexto de dados salvo na tabela agent_context")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Falha ao salvar contexto de dados: {e}")
            
            # Fallback inteligente: se chunks nÃ£o sÃ£o suficientes OU pergunta exige anÃ¡lise global
            if not similar_chunks or self._should_use_global_csv(query, similar_chunks):
                # Tentar extrair caminho do CSV dos chunks ou do contexto
                csv_path = None
                if context and 'csv_path' in context:
                    csv_path = context['csv_path']
                elif similar_chunks:
                    for chunk in similar_chunks:
                        if 'csv_path' in chunk:
                            csv_path = chunk['csv_path']
                            break
                # Se nÃ£o encontrar, buscar na pasta processados
                if not csv_path:
                    try:
                        from src.settings import EDA_DATA_DIR_PROCESSADO
                        import os
                        import pandas as pd
                        csv_files = list(EDA_DATA_DIR_PROCESSADO.glob('*.csv'))
                        if csv_files:
                            csv_path = str(max(csv_files, key=lambda p: p.stat().st_mtime))
                    except Exception as e:
                        self.logger.warning(f"NÃ£o foi possÃ­vel localizar CSV para fallback: {e}")
                if csv_path:
                    self.logger.info(f"âš¡ Fallback: anÃ¡lise global do CSV ({csv_path}) para pergunta '{query[:60]}...'")
                    try:
                        resposta_csv = self._analisar_completo_csv(csv_path, query)
                        return self._build_response(
                            resposta_csv,
                            metadata={
                                "method": "global_csv_fallback",
                                "csv_path": csv_path,
                                "chunks_found": len(similar_chunks)
                            }
                        )
                    except Exception as e:
                        self.logger.error(f"Erro no fallback global CSV: {e}")
                        return self._build_error_response(f"Erro ao processar CSV global: {e}")
                else:
                    self.logger.error("âŒ Fallback global: CSV nÃ£o encontrado.")
                    return self._build_error_response("CSV original nÃ£o encontrado para anÃ¡lise global.")
            
            self.logger.info(f"âœ… Encontrados {len(similar_chunks)} chunks relevantes")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• VERIFICAR SE VISUALIZAÃ‡ÃƒO FOI SOLICITADA (MESMO COM CHUNKS)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            viz_requested = bool(context and context.get('visualization_requested'))
            if viz_requested:
                self.logger.info("ğŸ“Š VisualizaÃ§Ã£o solicitada - gerando grÃ¡ficos...")
                try:
                    import pandas as pd
                    from pathlib import Path
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE - ACESSO DIRETO AO CSV
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # JUSTIFICATIVA:
                    # 1. Tabela embeddings contÃ©m chunks de anÃ¡lises estatÃ­sticas (Markdown)
                    # 2. Histogramas requerem dados tabulares completos (285k linhas Ã— 31 colunas)
                    # 3. Embeddar cada linha seria ineficiente: ~$50-100 de custo + overhead
                    # 4. PadrÃ£o de mercado: LangChain, LlamaIndex, OpenAI Code Interpreter
                    #    fazem leitura direta de CSV para anÃ¡lises quantitativas
                    # 
                    # IMPLEMENTAÃ‡ÃƒO FUTURA:
                    # - TODO: Adicionar chunks raw_data na tabela embeddings durante ingestÃ£o
                    # - TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
                    # 
                    # AUDITORIA:
                    # - Log completo de acesso registrado
                    # - Metadados incluÃ­dos na resposta
                    # - Acesso read-only sem modificaÃ§Ã£o de dados
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    
                    from src.settings import EDA_DATA_DIR_PROCESSADO
                    # Buscar CSV mais recente em data/processado/
                    csv_files = list(EDA_DATA_DIR_PROCESSADO.glob("*.csv"))
                    if not csv_files:
                        self.logger.error("âŒ Nenhum arquivo CSV encontrado em data/processado/")
                        self.logger.info("âš ï¸ Continuando com resposta textual sem visualizaÃ§Ãµes")
                    else:
                        # Pegar o arquivo mais recente (Ãºltimo modificado)
                        csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
                        csv_size_mb = csv_path.stat().st_size / 1_000_000
                        self.logger.warning(
                            "âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE: Acesso direto ao CSV para visualizaÃ§Ã£o",
                            extra={
                                "event_type": "direct_csv_access",
                                "user_query": query[:100],
                                "csv_path": str(csv_path),
                                "csv_size_mb": round(csv_size_mb, 2),
                                "access_reason": "histogram_generation",
                                "session_id": self._current_session_id,
                                "agent_name": self.name,
                                "timestamp": datetime.now().isoformat(),
                                "conformidade_status": "exception_approved",
                                "alternative_implementation": "future_raw_data_embeddings",
                                "cost_saved_estimate_usd": 50.0
                            }
                        )
                        viz_df = pd.read_csv(csv_path)
                        self.logger.info(
                            f"âœ… CSV carregado para visualizaÃ§Ã£o: {viz_df.shape[0]:,} linhas Ã— {viz_df.shape[1]} colunas | "
                            f"Tamanho: {csv_size_mb:.2f} MB"
                        )
                        # Delegar para agente de visualizaÃ§Ã£o
                        # Removido: agente obsoleto csv_analysis_agent.py
                        vis_context = context.copy() if context else {}
                        vis_context['reconstructed_df'] = viz_df
                        vis_result = self._handle_visualization_query(query, vis_context)
                        if vis_result.get('metadata', {}).get('visualization_success'):
                            # Combinar resposta de visualizaÃ§Ã£o com anÃ¡lise textual dos chunks
                            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
                            context_str = "\n\n".join(context_texts[:5])
                            text_response = await self._generate_llm_response_langchain(
                                query=query,
                                context_data=context_str,
                                memory_context=memory_context,
                                chunks_metadata=similar_chunks
                            )
                            # Combinar resposta textual com informaÃ§Ã£o sobre grÃ¡ficos
                            graficos_info = vis_result.get('metadata', {}).get('graficos_gerados', [])
                            if graficos_info:
                                graficos_msg = f"\n\nğŸ“Š **VisualizaÃ§Ãµes Geradas:**\n"
                                for gf in graficos_info:
                                    graficos_msg += f"â€¢ {gf}\n"
                                combined_response = text_response + graficos_msg
                            else:
                                combined_response = text_response
                            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                            # Salvar interaÃ§Ã£o com metadados de conformidade
                            if self.has_memory:
                                await self.remember_interaction(
                                    query=query,
                                    response=combined_response,
                                    processing_time_ms=processing_time_ms,
                                    confidence=1.0,
                                    model_used="rag_v2_with_visualizations",
                                    metadata={
                                        "chunks_found": len(similar_chunks),
                                        "visualization_success": True,
                                        "graficos_gerados": len(graficos_info),
                                        "conformidade_exception": {
                                            "type": "direct_csv_access",
                                            "reason": "visualization_requires_raw_data",
                                            "csv_path": str(csv_path),
                                            "csv_size_mb": round(csv_size_mb, 2),
                                            "access_timestamp": datetime.now().isoformat(),
                                            "approved": True,
                                            "alternative_future": "raw_data_embeddings_implementation",
                                            "industry_standard": "LangChain/LlamaIndex/OpenAI_pattern",
                                            "cost_saved_usd": 50.0,
                                            "read_only": True
                                        }
                                    }
                                )
                            return self._build_response(
                                combined_response,
                                metadata={
                                    **vis_result.get('metadata', {}),
                                    "chunks_found": len(similar_chunks),
                                    "method": "rag_vectorial_v2_with_viz",
                                    "processing_time_ms": processing_time_ms,
                                    "conformidade_exception": {
                                        "type": "direct_csv_access",
                                        "reason": "visualization_requires_raw_data",
                                        "csv_path": str(csv_path),
                                        "csv_size_mb": round(csv_size_mb, 2),
                                        "approved": True,
                                        "industry_standard": True,
                                        "read_only": True,
                                        "documentation": "See comments in rag_data_agent.py lines 318-335"
                                    }
                                }
                            )
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao gerar visualizaÃ§Ãµes: {e}", exc_info=True)
                    # Continuar com resposta textual normal se visualizaÃ§Ã£o falhar
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 5. GERAR RESPOSTA COM LANGCHAIN + CONTEXTO HISTÃ“RICO
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
            context_str = "\n\n".join(context_texts[:5])  # Top 5 mais relevantes
            
            self.logger.debug("Usando LangChain LLM para gerar resposta...")
            response_text = await self._generate_llm_response_langchain(
                query=query,
                context_data=context_str,
                memory_context=memory_context,
                chunks_metadata=similar_chunks
            )
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            avg_similarity = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. SALVAR INTERAÃ‡ÃƒO NA MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=response_text,
                    processing_time_ms=processing_time_ms,
                    confidence=avg_similarity,
                    model_used="langchain_gemini" if self.llm else "fallback",
                    metadata={
                        "chunks_found": len(similar_chunks),
                        "chunks_used": min(5, len(similar_chunks)),
                        "avg_similarity": avg_similarity,
                        "top_similarity": similar_chunks[0]['similarity'],
                        "has_history": len(memory_context.get('recent_conversations', [])) > 0
                    }
                )
                self.logger.debug("âœ… InteraÃ§Ã£o salva na memÃ³ria persistente")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 7. RETORNAR RESPOSTA COM METADADOS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            return self._build_response(
                response_text,
                metadata={
                    "chunks_found": len(similar_chunks),
                    "chunks_used": min(5, len(similar_chunks)),
                    "avg_similarity": avg_similarity,
                    "method": "rag_vectorial_v2",
                    "top_similarity": similar_chunks[0]['similarity'] if similar_chunks else 0,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao processar query: {str(e)}", exc_info=True)
            return self._build_error_response(f"Erro no processamento: {str(e)}")
    
    def _search_similar_data(
        self,
        query_embedding: List[float],
        threshold: float = 0.5,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Busca chunks similares nos dados usando match_embeddings RPC.
        
        Args:
            query_embedding: Embedding da query
            threshold: Threshold de similaridade (0.0 - 1.0)
            limit: NÃºmero mÃ¡ximo de resultados
            
        Returns:
            Lista de chunks similares com metadata
        """
        try:
            # Chamar funÃ§Ã£o RPC match_embeddings
            response = supabase.rpc(
                'match_embeddings',
                {
                    'query_embedding': query_embedding,
                    'similarity_threshold': threshold,
                    'match_count': limit
                }
            ).execute()
            
            if not response.data:
                self.logger.warning("Nenhum chunk similar encontrado")
                return []
            
            self.logger.debug(f"Encontrados {len(response.data)} chunks similares")
            # Parsing defensivo dos embeddings
            from src.embeddings.vector_store import parse_embedding_from_api, VECTOR_DIMENSIONS
            parsed_chunks = []
            for chunk in response.data:
                embedding_raw = chunk.get('embedding')
                try:
                    chunk['embedding'] = parse_embedding_from_api(embedding_raw, VECTOR_DIMENSIONS)
                except Exception as e:
                    self.logger.warning(f"Falha ao parsear embedding do chunk: {e}")
                    chunk['embedding'] = None
                parsed_chunks.append(chunk)
            return parsed_chunks
            
        except Exception as e:
            self.logger.error(f"Erro na busca vetorial: {str(e)}")
            return []
    
    async def _generate_llm_response_langchain(
        self,
        query: str,
        context_data: str,
        memory_context: dict,
        chunks_metadata: list
    ) -> str:
        try:
            # Preparar contexto histÃ³rico da conversa
            history_context = ""
            if memory_context.get('recent_messages') and len(memory_context['recent_messages']) > 0:
                history_context = "\n\n**Contexto da Conversa Anterior:**\n"
                for msg in memory_context['recent_messages'][-6:]:  # Ãšltimas 6 mensagens (3 pares user/assistant)
                    msg_type = msg.get('type', 'unknown')
                    content = msg.get('content', '')[:200]  # Limitar a 200 chars
                    if msg_type == 'user':
                        history_context += f"- UsuÃ¡rio perguntou: {content}\n"
                    elif msg_type == 'assistant':
                        history_context += f"- Assistente respondeu: {content}\n"
                history_context += "\n"

            # Preparar prompt DINÃ‚MICO baseado no tipo de query
            query_lower = query.lower()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ”¥ V3.0: ORQUESTRAÃ‡ÃƒO INTELIGENTE VIA LLM (ZERO HARD-CODING)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 
            # ANTES (V2.0): ~240 linhas de cascata if/elif com keywords hardcoded
            # DEPOIS (V3.0): ClassificaÃ§Ã£o semÃ¢ntica + orquestraÃ§Ã£o modular
            # 
            # BenefÃ­cios:
            # - âœ… Reconhece QUALQUER sinÃ´nimo (nÃ£o limitado a lista fixa)
            # - âœ… Processa queries mistas simultaneamente
            # - âœ… ExtensÃ­vel (novos tipos sem modificar cÃ³digo)
            # - âœ… ManutenÃ­vel (cÃ³digo limpo e modular)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Detectar se Ã© query sobre HISTÃ“RICO (caso especial)
            is_history_query = any(term in query_lower for term in [
                'pergunta anterior', 'perguntei antes', 'falamos sobre',
                'conversamos sobre', 'vocÃª disse', 'previous question', 'asked before'
            ])
            
            if is_history_query:
                # Query sobre HISTÃ“RICO - usar memÃ³ria conversacional
                self.logger.info("ğŸ“œ Query sobre histÃ³rico conversacional detectada")
                
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© responder sobre o HISTÃ“RICO da conversa. "
                    "Use o contexto da conversa anterior fornecido para responder. "
                    "Seja claro e objetivo, referenciando exatamente o que foi discutido."
                )
                user_prompt = (
                    f"{history_context}"
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Consulte o histÃ³rico da conversa acima\n"
                    "- Responda referenciando exatamente o que foi perguntado/respondido anteriormente\n"
                    "- Se nÃ£o houver histÃ³rico suficiente, informe claramente\n"
                    "- Finalize com: 'Posso esclarecer mais alguma coisa sobre nossa conversa?'\n\n"
                    "**Resposta:**"
                )
                
                # Usar LLM diretamente para query de histÃ³rico
                if self.llm and LANGCHAIN_AVAILABLE:
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    response = await asyncio.to_thread(self.llm.invoke, messages)
                    final_response = response.content
                else:
                    final_response = "HistÃ³rico nÃ£o disponÃ­vel (LLM indisponÃ­vel)"
            
            else:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ğŸ”¥ V3.0: ORQUESTRAÃ‡ÃƒO ANALÃTICA MODULAR
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    self.logger.info("ğŸ”¥ Executando V3.0: AnalysisOrchestrator")
                    
                    # Carregar DataFrame do CSV se disponÃ­vel
                    # Note: 'context' estÃ¡ definido no parÃ¢metro do mÃ©todo process()
                    df = None
                    if context and 'csv_data' in context:  # type: ignore[has-type]
                        import pandas as pd
                        csv_path = context['csv_data'].get('path')  # type: ignore[has-type]
                        if csv_path:
                            try:
                                df = pd.read_csv(csv_path)
                                self.logger.info(f"ğŸ“Š DataFrame carregado: {df.shape}")
                            except Exception as e:
                                self.logger.error(f"Erro ao carregar CSV: {e}")
                    
                    # Se DataFrame disponÃ­vel, usar orchestrator V3
                    if df is not None and not df.empty:
                        final_response = self._build_analytical_response_v3(
                            query=query,
                            df=df,
                            context_data=context_data,
                            history_context=history_context
                        )
                    else:
                        # Fallback: resposta baseada apenas em chunks (sem anÃ¡lise executada)
                        self.logger.warning("âš ï¸ DataFrame nÃ£o disponÃ­vel - usando fallback chunks-only")
                        final_response = self._fallback_basic_response(
                            query=query,
                            context_data=context_data,
                            history_context=history_context
                        )
                
                except Exception as e:
                    self.logger.error(f"âŒ Erro no fluxo V3.0: {e}", exc_info=True)
                    # Fallback final
                    final_response = self._fallback_basic_response(
                        query=query,
                        context_data=context_data,
                        history_context=history_context
                    )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # RESPOSTA FINAL
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. SALVAR NA MEMÃ“RIA E RETORNAR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Note: start_time estÃ¡ definido no inÃ­cio do mÃ©todo (linha ~821)
            # Note: similar_chunks estÃ¡ definido apÃ³s busca vetorial (linha ~889)
            # Pylance false positive: essas variÃ¡veis ESTÃƒO no escopo do mÃ©todo
            processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000  # type: ignore[has-type]
            
            # Calcular mÃ©tricas se similar_chunks disponÃ­vel
            chunks_count = len(similar_chunks) if similar_chunks else 0  # type: ignore[has-type]
            avg_sim = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks) if similar_chunks else 0.0  # type: ignore[has-type]
            top_sim = similar_chunks[0]['similarity'] if similar_chunks else 0.0  # type: ignore[has-type]
            
            # Salvar interaÃ§Ã£o na memÃ³ria persistente
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=final_response,
                    processing_time_ms=processing_time_ms,
                    confidence=0.85,
                    model_used="rag_v3_orchestrated",
                    metadata={
                        "chunks_found": chunks_count,
                        "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                        "method": "rag_vectorial_v3",
                        "architecture": "modular_orchestrated",
                        "zero_hardcoding": True
                    }
                )
            
            # Retornar resposta formatada
            return self._build_response(
                final_response,
                metadata={
                    "chunks_found": chunks_count,
                    "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                    "avg_similarity": avg_sim,
                    "method": "rag_vectorial_v3",
                    "architecture": "modular_orchestrated",
                    "top_similarity": top_sim,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao gerar resposta LLM: {str(e)}", exc_info=True)
            return self._format_raw_data_response(query, chunks_metadata)
    
    def _format_raw_data_response(
        self,
        query: str,
        chunks_metadata: List[Dict]
        ) -> str:
        """
        Fallback: usa agente de sÃ­ntese para consolidar dados se LLM falhar.
        """
        # Extrair apenas o texto dos chunks
        chunks = [chunk.get('chunk_text', '') for chunk in chunks_metadata]
        
        # Chamar agente de sÃ­ntese para consolidar
        from src.agent.rag_synthesis_agent import synthesize_response
        try:
            return synthesize_response(chunks, query, use_llm=False)
        except Exception as e:
            self.logger.error(f"Erro no agente de sÃ­ntese: {e}")
            # Fallback extremo: resposta estruturada mÃ­nima
            return f"""## Resposta para: {query}

**Status:** âš ï¸ Erro na sÃ­ntese

NÃ£o foi possÃ­vel processar completamente a consulta devido a um erro tÃ©cnico.
Por favor, reformule sua pergunta ou entre em contato com o suporte.

_Erro: {str(e)}_"""
    
    def _build_error_response(self, error_msg: str) -> Dict[str, Any]:
        """ConstrÃ³i resposta de erro padronizada."""
        return self._build_response(
            f"âŒ {error_msg}",
            metadata={"error": True, "method": "rag_vectorial_v2"}
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO SÃNCRONO WRAPPER (para compatibilidade retroativa)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process_sync(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Wrapper sÃ­ncrono para compatibilidade com cÃ³digo legado.
        
        âš ï¸ DEPRECATED: Use process() async quando possÃ­vel.
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.process(query, context))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO DE CARREGAMENTO CSV (mantido da versÃ£o anterior)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def load_csv_to_embeddings(
        self,
        csv_path: str,
        chunk_size: int = 1000,
        overlap: int = 100
    ) -> Dict[str, Any]:
        """
        Carrega CSV para a tabela embeddings.
        
        Args:
            csv_path: Caminho do arquivo CSV
            chunk_size: Tamanho dos chunks
            overlap: Overlap entre chunks
            
        Returns:
            Status do carregamento
        """
        try:
            self.logger.info(f"ğŸ“‚ Carregando CSV: {csv_path}")
            
            import pandas as pd
            from src.embeddings.chunker import CSVChunker
            
            # Ler CSV
            df = pd.read_csv(csv_path)
            self.logger.info(f"âœ… CSV lido: {len(df)} linhas, {len(df.columns)} colunas")
            
            # Criar chunks
            chunker = CSVChunker(chunk_size=chunk_size, overlap=overlap)
            chunks = chunker.chunk_dataframe(df)
            self.logger.info(f"âœ… Criados {len(chunks)} chunks")
            
            # Gerar embeddings e salvar
            inserted_count = 0
            for i, chunk in enumerate(chunks):
                try:
                    # Gerar embedding
                    embedding = self.embedding_gen.generate_embedding(chunk['text'])
                    
                    # Salvar na tabela embeddings
                    insert_data = {
                        'chunk_text': chunk['text'],
                        'embedding': embedding,
                        'metadata': {
                            'source': csv_path,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'created_at': datetime.now().isoformat()
                        }
                    }
                    
                    result = supabase.table('embeddings').insert(insert_data).execute()
                    
                    if result.data:
                        inserted_count += 1
                        if (i + 1) % 10 == 0:
                            self.logger.info(f"Progresso: {i+1}/{len(chunks)} chunks inseridos")
                
                except Exception as chunk_error:
                    self.logger.warning(f"Erro no chunk {i}: {chunk_error}")
                    continue
            
            self.logger.info(f"âœ… Carregamento concluÃ­do: {inserted_count}/{len(chunks)} chunks inseridos")
            
            return self._build_response(
                f"âœ… CSV carregado com sucesso: {inserted_count} chunks inseridos na base vetorial",
                metadata={
                    'csv_path': csv_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'chunks_created': len(chunks),
                    'chunks_inserted': inserted_count
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao carregar CSV: {str(e)}")
            return self._build_error_response(f"Falha ao carregar CSV: {str(e)}")
