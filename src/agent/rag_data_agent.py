"""
Agente de AnÃ¡lise de Dados via RAG Vetorial Puro com MemÃ³ria Persistente e LangChain.

VERSÃƒO 2.0 - REFATORADA:
- âœ… MemÃ³ria persistente em Supabase (tabelas agent_sessions, agent_conversations, agent_context)
- âœ… LangChain integrado nativamente (ChatOpenAI, ChatGoogleGenerativeAI)
- âœ… MÃ©todos async para performance
- âœ… Contexto conversacional entre interaÃ§Ãµes
- âœ… Busca vetorial pura (sem keywords hardcoded)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  EXCEÃ‡ÃƒO DE CONFORMIDADE: ACESSO DIRETO A CSV PARA VISUALIZAÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTEXTO:
- Tabela 'embeddings' armazena chunks de anÃ¡lises estatÃ­sticas (Markdown)
- VisualizaÃ§Ãµes (histogramas) requerem dados tabulares completos (285k linhas)
- Embeddar cada linha seria ineficiente: ~$50-100 custo + overhead desnecessÃ¡rio

SOLUÃ‡ÃƒO IMPLEMENTADA:
- Quando visualizaÃ§Ã£o Ã© solicitada, acessa CSV diretamente via pd.read_csv()
- Acesso Ã© READ-ONLY, sem modificaÃ§Ã£o de dados
- Log completo de auditoria registrado (linhas 318-350)
- Metadados de conformidade incluÃ­dos em todas as respostas

JUSTIFICATIVA (ADERENTE A BOAS PRÃTICAS DE MERCADO):
1. PadrÃ£o da indÃºstria: LangChain CSV Agents, LlamaIndex, OpenAI Code Interpreter
2. SeparaÃ§Ã£o de responsabilidades: RAG para busca semÃ¢ntica, CSV para dados tabulares
3. Custo-benefÃ­cio: evita armazenamento/processamento desnecessÃ¡rio
4. Performance: leitura direta Ã© mais rÃ¡pida que reconstituiÃ§Ã£o de embeddings

IMPLEMENTAÃ‡ÃƒO FUTURA (Opcional):
- TODO: Adicionar chunks 'raw_data' na tabela embeddings durante ingestÃ£o
- TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
- TODO: Adicionar configuraÃ§Ã£o para escolher entre direct-access vs embeddings

AUDITORIA E COMPLIANCE:
- âœ… Log detalhado com event_type, timestamp, session_id, csv_path, size
- âœ… Metadados em response.metadata['conformidade_exception']
- âœ… DocumentaÃ§Ã£o clara da exceÃ§Ã£o e justificativa
- âœ… AprovaÃ§Ã£o registrada (approved=True)

REFERÃŠNCIAS:
- LangChain CSV Agent: https://python.langchain.com/docs/integrations/toolkits/csv
- OpenAI Code Interpreter: https://openai.com/blog/code-interpreter
- Hybrid RAG Architectures: https://docs.llamaindex.ai/en/stable/examples/query_engine/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import logging
from typing import Any, Dict, List, Optional
import json
from datetime import datetime
import asyncio
import pandas as pd

from agent.base_agent import BaseAgent, AgentError
from vectorstore.supabase_client import supabase
from embeddings.generator import EmbeddingGenerator
from utils.logging_config import get_logger
from analysis.intent_classifier import IntentClassifier, AnalysisIntent
from analysis.orchestrator import AnalysisOrchestrator

# Imports LangChain
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_experimental.tools import PythonREPLTool
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    AIMessage = None
    PythonREPLTool = None
    print(f"âš ï¸ LangChain nÃ£o disponÃ­vel: {e}")


class RAGDataAgent(BaseAgent):
    def _interpretar_pergunta_llm(self, pergunta: str, df):
        """
        âœ… V3.0: Utiliza IntentClassifier para classificaÃ§Ã£o semÃ¢ntica SEM HARD-CODING.
        
        Retorna instruÃ§Ãµes analÃ­ticas baseadas na intenÃ§Ã£o classificada pela LLM.
        Cada instruÃ§Ã£o Ã© um dict: {'acao': ..., 'colunas': [...], 'params': {}, 'justificativa': str}
        """
        if not self.llm:
            # Fallback: retorna instruÃ§Ã£o genÃ©rica
            return [{'acao': 'estatÃ­sticas gerais', 'colunas': list(df.columns), 'params': {}, 
                    'justificativa': 'LLM indisponÃ­vel, fornecendo estatÃ­sticas gerais.'}]
        
        try:
            # ğŸ”¥ V3.0: Usar IntentClassifier para classificaÃ§Ã£o semÃ¢ntica
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            # Classificar intenÃ§Ã£o da pergunta
            context = {
                'available_columns': list(df.columns),
                'dataframe_info': f"Shape: {df.shape}, Colunas numÃ©ricas: {df.select_dtypes(include=['number']).columns.tolist()}"
            }
            
            classification_result = classifier.classify(query=pergunta, context=context)
            
            # Log da classificaÃ§Ã£o
            self.logger.info({
                'event': 'intent_classification',
                'primary_intent': classification_result.primary_intent.value,
                'secondary_intents': [intent.value for intent in classification_result.secondary_intents],
                'confidence': classification_result.confidence,
                'reasoning': classification_result.reasoning
            })
            
            # ğŸ¯ Mapear intenÃ§Ãµes para instruÃ§Ãµes analÃ­ticas
            instrucoes = []
            
            # Processar intenÃ§Ã£o primÃ¡ria
            primary_action = self._intent_to_action(
                classification_result.primary_intent, 
                df, 
                classification_result.reasoning
            )
            if primary_action:
                instrucoes.append(primary_action)
            
            # Processar intenÃ§Ãµes secundÃ¡rias se existirem
            for secondary_intent in classification_result.secondary_intents:
                secondary_action = self._intent_to_action(
                    secondary_intent, 
                    df, 
                    f"IntenÃ§Ã£o secundÃ¡ria detectada: {secondary_intent.value}"
                )
                if secondary_action and secondary_action not in instrucoes:
                    instrucoes.append(secondary_action)
            
            # Garantir pelo menos uma instruÃ§Ã£o
            if not instrucoes:
                instrucoes = [{
                    'acao': 'estatÃ­sticas gerais',
                    'colunas': list(df.columns),
                    'params': {},
                    'justificativa': 'Nenhuma intenÃ§Ã£o especÃ­fica detectada, fornecendo visÃ£o geral.'
                }]
            
            self.logger.info({
                'event': 'instructions_generated',
                'num_instructions': len(instrucoes),
                'actions': [ins['acao'] for ins in instrucoes]
            })
            
            return instrucoes
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao interpretar pergunta com IntentClassifier: {e}")
            # Fallback para interpretaÃ§Ã£o bÃ¡sica via LLM direta
            return self._fallback_interpretation(pergunta, df)
    
    def _intent_to_action(self, intent: AnalysisIntent, df, justificativa: str) -> Optional[Dict]:
        """
        ğŸ¯ Converte AnalysisIntent em instruÃ§Ã£o analÃ­tica.
        Mapeia tipos de intenÃ§Ã£o para aÃ§Ãµes especÃ­ficas.
        """
        # Selecionar colunas numÃ©ricas por padrÃ£o
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        all_cols = list(df.columns)
        
        intent_map = {
            AnalysisIntent.STATISTICAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.FREQUENCY: {
                'acao': 'frequency_analysis',
                'colunas': all_cols,
                'params': {'top_n': 10},
                'justificativa': justificativa
            },
            AnalysisIntent.TEMPORAL: {
                'acao': 'temporal_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.CLUSTERING: {
                'acao': 'clustering_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {'n_clusters': 3},
                'justificativa': justificativa
            },
            AnalysisIntent.CORRELATION: {
                'acao': 'correlation_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.OUTLIERS: {
                'acao': 'outlier_detection',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.COMPARISON: {
                'acao': 'comparison_analysis',
                'colunas': all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.VISUALIZATION: {
                'acao': 'visualization_request',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.GENERAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            }
        }
        
        return intent_map.get(intent)
    
    def _fallback_interpretation(self, pergunta: str, df) -> List[Dict]:
        """
        Fallback: interpretaÃ§Ã£o bÃ¡sica via LLM direta quando IntentClassifier falha.
        MantÃ©m apenas lÃ³gica essencial, SEM hard-coding de keywords.
        """
        prompt = (
            "VocÃª Ã© um especialista em anÃ¡lise de dados.\n"
            "Interprete a pergunta do usuÃ¡rio e retorne uma lista JSON de instruÃ§Ãµes analÃ­ticas.\n"
            "Formato de cada instruÃ§Ã£o: {'acao': str, 'colunas': [str], 'params': {}, 'justificativa': str}\n"
            f"Pergunta: {pergunta}\n"
            f"Colunas disponÃ­veis: {list(df.columns)}\n"
            "Responda APENAS com o JSON, sem explicaÃ§Ãµes."
        )
        
        try:
            response = self.llm.invoke(prompt)
            import json
            instrucoes = json.loads(response.content)
            return instrucoes if isinstance(instrucoes, list) else [instrucoes]
        except Exception as e:
            self.logger.error(f"Fallback interpretation falhou: {e}")
            return [{
                'acao': 'estatÃ­sticas gerais',
                'colunas': list(df.columns),
                'params': {},
                'justificativa': 'InterpretaÃ§Ã£o padrÃ£o devido a erro.'
            }]
    
    def _build_analytical_response_v3(
        self,
        query: str,
        df: pd.DataFrame,
        context_data: str,
        history_context: str = ""
    ) -> str:
        """
        ğŸ”¥ V3.0: ConstrÃ³i resposta analÃ­tica usando AnalysisOrchestrator.
        
        Substitui ~240 linhas de cascata if/elif por orquestraÃ§Ã£o inteligente.
        
        Args:
            query: Pergunta do usuÃ¡rio
            df: DataFrame carregado
            context_data: Chunks analÃ­ticos do CSV
            history_context: HistÃ³rico conversacional
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            self.logger.info("ğŸ”¥ Usando V3.0: AnalysisOrchestrator")
            
            # Classificar intenÃ§Ã£o via IntentClassifier
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            context_info = {
                'available_columns': list(df.columns),
                'dataframe_shape': df.shape,
                'has_history': bool(history_context)
            }
            
            intent_result = classifier.classify(query, context=context_info)
            
            # Converter IntentClassificationResult para dict de confianÃ§a
            intent_dict = {intent_result.primary_intent.value.upper(): intent_result.confidence}
            
            # Adicionar intenÃ§Ãµes secundÃ¡rias
            for secondary in intent_result.secondary_intents:
                intent_dict[secondary.value.upper()] = 0.75  # ConfianÃ§a padrÃ£o para secundÃ¡rias
            
            self.logger.info(f"IntenÃ§Ãµes detectadas: {intent_dict}")
            
            # Criar orquestrador e executar anÃ¡lises
            orchestrator = AnalysisOrchestrator(llm=self.llm, logger=self.logger)
            
            orchestration_result = orchestrator.orchestrate_v3_direct(
                intent_result=intent_dict,
                df=df,
                confidence_threshold=0.6
            )
            
            # Construir resposta formatada
            response = self._format_orchestrated_response(
                query=query,
                orchestration_result=orchestration_result,
                context_data=context_data,
                history_context=history_context,
                intent_result=intent_result
            )
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ Erro no V3 orchestrator: {e}", exc_info=True)
            # Fallback para resposta bÃ¡sica
            return self._fallback_basic_response(query, context_data, history_context)
    
    def _format_orchestrated_response(
        self,
        query: str,
        orchestration_result: Dict[str, Any],
        context_data: str,
        history_context: str,
        intent_result
    ) -> str:
        """
        Formata resultado orquestrado em resposta humanizada.
        
        Args:
            query: Pergunta original
            orchestration_result: Resultado do orchestrator.orchestrate_v3_direct()
            context_data: Chunks analÃ­ticos
            history_context: HistÃ³rico
            intent_result: ClassificaÃ§Ã£o de intenÃ§Ã£o
            
        Returns:
            Resposta formatada em Markdown
        """
        try:
            # Construir prompt para LLM formatar resposta final
            from langchain.schema import SystemMessage, HumanMessage
            
            system_prompt = """
VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© apresentar resultados analÃ­ticos de forma clara e estruturada.

VocÃª receberÃ¡:
1. Pergunta do usuÃ¡rio
2. Resultados de anÃ¡lises executadas (JSON estruturado)
3. Chunks analÃ­ticos do CSV (contexto adicional)
4. HistÃ³rico conversacional (se houver)

Sua resposta deve:
- Iniciar com: "Pergunta feita: [pergunta]"
- Apresentar resultados de forma humanizada e estruturada
- Usar tabelas Markdown quando apropriado
- Destacar insights relevantes
- Finalizar com: "Se precisar de mais detalhes, Ã© sÃ³ perguntar!"
"""
            
            results_summary = []
            for analyzer_name, result in orchestration_result.get('results', {}).items():
                if isinstance(result, dict) and 'error' not in result:
                    results_summary.append(f"**{analyzer_name.title()}**: AnÃ¡lise executada com sucesso")
                elif isinstance(result, dict) and 'error' in result:
                    results_summary.append(f"**{analyzer_name.title()}**: Erro - {result['error']}")
            
            user_prompt = f"""
**Pergunta do UsuÃ¡rio:**
{query}

{history_context}

**Resultados das AnÃ¡lises Executadas:**
{chr(10).join(results_summary)}

**Detalhes Completos (JSON):**
```json
{json.dumps(orchestration_result.get('results', {}), indent=2, ensure_ascii=False)}
```

**Chunks AnalÃ­ticos do CSV (contexto adicional):**
{context_data[:1000]}...

**IntenÃ§Ã£o Detectada:**
- Principal: {intent_result.primary_intent.value}
- ConfianÃ§a: {intent_result.confidence:.1%}
- Justificativa: {intent_result.reasoning}

Apresente os resultados de forma clara, humanizada e estruturada.
"""
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            response = self.llm.invoke(messages)
            return response.content
            
        except Exception as e:
            self.logger.error(f"Erro ao formatar resposta orquestrada: {e}")
            # Fallback: retornar JSON bruto formatado
            import json
            return f"""
Pergunta feita: {query}

**AnÃ¡lises Executadas:**
{chr(10).join([f"- {k}" for k in orchestration_result.get('results', {}).keys()])}

**Resultados:**
```json
{json.dumps(orchestration_result, indent=2, ensure_ascii=False)}
```

Se precisar de mais detalhes, Ã© sÃ³ perguntar!
"""
    
    def _fallback_basic_response(
        self,
        query: str,
        context_data: str,
        history_context: str
    ) -> str:
        """
        Fallback bÃ¡sico quando V3 orchestrator falha completamente.
        """
        from langchain.schema import SystemMessage, HumanMessage
        
        system_prompt = "VocÃª Ã© um agente EDA. Responda Ã  pergunta usando os chunks fornecidos."
        
        user_prompt = f"""
Pergunta: {query}

{history_context}

Chunks analÃ­ticos:
{context_data}

Responda de forma clara e estruturada.
"""
        
        try:
            messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_prompt)]
            response = self.llm.invoke(messages)
            return response.content
        except Exception as e:
            return f"Erro ao processar anÃ¡lise: {str(e)}"

    def _executar_instrucao(self, df, instrucao):
        """
        Executa uma instruÃ§Ã£o analÃ­tica sobre o DataFrame.
        Suporta mÃ©tricas nativas pandas/NumPy e delega compostas Ã  LLM.
        """
        acao = instrucao.get('acao','')
        colunas = instrucao.get('colunas', list(df.columns))
        params = instrucao.get('params', {})
        try:
            # Normalizar nome da aÃ§Ã£o para comparaÃ§Ã£o (tolerante a maiÃºsculas/minÃºsculas)
            acao_norm = str(acao).strip().lower()
            # MÃ©tricas nativas pandas
            if acao_norm in ('mÃ©dia', 'media', 'mean'):
                return df[colunas].mean().to_frame(name='MÃ©dia')
            if acao_norm in ('mediana', 'median'):
                return df[colunas].median().to_frame(name='Mediana')
            if acao_norm in ('moda', 'mode'):
                return df[colunas].mode().T
            if acao_norm in ('desvio padrÃ£o', 'desvio padrao', 'std', 'standard deviation'):
                return df[colunas].std().to_frame(name='Desvio padrÃ£o')
            if acao_norm in ('variÃ¢ncia', 'variancia', 'variance', 'var'):
                # VariÃ¢ncia populacional/por padrÃ£o pandas var() usa ddof=1 (amostral). Mantemos default.
                return df[colunas].var().to_frame(name='VariÃ¢ncia')
            if acao_norm in ('intervalo', 'minmax', 'min_max', 'mÃ­nimo', 'mÃ¡ximo'):
                resultado = df[colunas].agg(['min','max']).T
                resultado.columns = ['MÃ­nimo','MÃ¡ximo']
                return resultado
            if acao_norm in ('estatÃ­sticas gerais', 'estatisticas gerais', 'describe', 'summary', 'resumo'):
                return df[colunas].describe().T
            # MÃ©tricas compostas ou extraordinÃ¡rias: delega Ã  LLM com execuÃ§Ã£o SEGURA
            if self.llm and PythonREPLTool:
                try:
                    # ğŸ”’ SEGURANÃ‡A: Usar PythonREPLTool com sandbox isolado
                    python_repl = PythonREPLTool()
                    
                    prompt = (
                        f"Receba instruÃ§Ã£o analÃ­tica: {instrucao}.\n"
                        f"DataFrame jÃ¡ estÃ¡ disponÃ­vel como variÃ¡vel 'df'.\n"
                        f"Colunas disponÃ­veis: {list(df.columns)}.\n"
                        "Gere cÃ³digo Python (pandas/numpy) que:\n"
                        "1. Execute a mÃ©trica pedida\n"
                        "2. Armazene o resultado em uma variÃ¡vel chamada 'resultado'\n"
                        "3. Retorne 'resultado' na Ãºltima linha\n"
                        "Retorne APENAS o cÃ³digo, sem explicaÃ§Ãµes, sem markdown.\n"
                        "Exemplo: resultado = df['coluna'].mean()"
                    )
                    
                    response = self.llm.invoke(prompt)
                    code = response.content.strip()
                    
                    # Remove markdown code blocks se presentes
                    if code.startswith("```python"):
                        code = code.split("```python")[1].split("```")[0].strip()
                    elif code.startswith("```"):
                        code = code.split("```")[1].split("```")[0].strip()
                    
                    # Log cÃ³digo antes de executar (auditoria)
                    self.logger.info(f"ğŸ”’ Executando cÃ³digo seguro via PythonREPLTool:\n{code[:200]}...")
                    
                    # Preparar contexto com DataFrame
                    import pandas as pd
                    import numpy as np
                    globals_context = {'df': df, 'pd': pd, 'np': np}
                    
                    # ğŸ”’ ExecuÃ§Ã£o segura via PythonREPLTool (sandbox isolado)
                    # Nota: PythonREPLTool executa em ambiente isolado sem acesso ao filesystem
                    resultado = python_repl.run(code, globals=globals_context)
                    
                    self.logger.info(f"âœ… CÃ³digo executado com sucesso via PythonREPLTool")
                    return resultado
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao executar cÃ³digo via PythonREPLTool: {e}")
                    self.logger.debug(f"CÃ³digo problemÃ¡tico: {code}")
                    return None
            else:
                self.logger.warning("LLM ou PythonREPLTool nÃ£o disponÃ­vel para mÃ©tricas compostas")
                return None
        except Exception as e:
            self.logger.warning(f"Falha ao executar instruÃ§Ã£o: {instrucao} | Erro: {e}")
            return None

    def _analisar_completo_csv(self, csv_path: str, pergunta: str, override_temporal_col: str = None,
                               temporal_col_names: list = None, accepted_types: tuple = None) -> str:
        """
        Executa anÃ¡lise temporal robusta e modular usando arquitetura refatorada V2.0.
        
        ARQUITETURA MODULAR:
        - DetecÃ§Ã£o via TemporalColumnDetector (src/analysis/temporal_detection.py)
        - AnÃ¡lise via TemporalAnalyzer (src/analysis/temporal_analyzer.py)
        - Fallback para anÃ¡lise estatÃ­stica geral quando nÃ£o houver colunas temporais
        
        CritÃ©rios de detecÃ§Ã£o:
        - Override manual (prioritÃ¡rio)
        - Tipo datetime64 nativo
        - Nomes comuns parametrizÃ¡veis (case-insensitive)
        - ConversÃ£o de strings temporais
        - SequÃªncias numÃ©ricas temporais (modo agressivo)
        
        ParÃ¢metros:
            - override_temporal_col: forÃ§a uso de coluna especÃ­fica (ou None para auto)
            - temporal_col_names: lista de nomes comuns (default: ["time", "date", "timestamp", "data", "datetime"])
            - accepted_types: DEPRECATED - mantido para backward compatibility
            
        Returns:
            String formatada em Markdown com anÃ¡lises temporais e/ou estatÃ­sticas
        """
        import pandas as pd
        from analysis.temporal_detection import TemporalColumnDetector, TemporalDetectionConfig
        from analysis.temporal_analyzer import TemporalAnalyzer
        
        # Carregar dados
        df = pd.read_csv(csv_path)
        
        logger = self.logger if hasattr(self, 'logger') else logging.getLogger(__name__)
        logger.info({
            'event': 'inicio_analise_csv_v2',
            'csv_path': csv_path,
            'shape': df.shape,
            'override_temporal_col': override_temporal_col
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 1: DETECÃ‡ÃƒO DE COLUNAS TEMPORAIS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Configurar detector com parÃ¢metros customizÃ¡veis
        detection_config = TemporalDetectionConfig()
        if temporal_col_names:
            detection_config.common_names = temporal_col_names
        
        detector = TemporalColumnDetector(config=detection_config)
        
        try:
            detection_results = detector.detect(df, override_column=override_temporal_col)
            temporal_cols = detector.get_detected_columns(detection_results)
            detection_summary = detector.get_detection_summary(detection_results)
            
            logger.info({
                'event': 'deteccao_temporal_concluida',
                'colunas_detectadas': temporal_cols,
                'total_colunas': len(df.columns),
                'taxa_deteccao': detection_summary['detection_rate'],
                'metodos_usados': detection_summary['methods_used']
            })
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o de colunas temporais: {e}", exc_info=True)
            temporal_cols = []
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 2: ANÃLISE TEMPORAL (se colunas detectadas)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if temporal_cols:
            logger.info(f"Executando anÃ¡lise temporal em {len(temporal_cols)} coluna(s)")
            
            analyzer = TemporalAnalyzer(logger=logger)
            respostas = []
            
            for col in temporal_cols:
                try:
                    # Executar anÃ¡lise temporal avanÃ§ada
                    result = analyzer.analyze(df, col, enable_advanced=True)
                    
                    # Gerar relatÃ³rio Markdown
                    respostas.append(result.to_markdown())
                    
                    logger.info({
                        'event': 'analise_temporal_coluna_concluida',
                        'coluna': col,
                        'trend_type': result.trend.get('type'),
                        'anomalies_count': result.anomalies.get('count', 0),
                        'seasonality_detected': result.seasonality.get('detected', False)
                    })
                except Exception as e:
                    logger.error(f"Erro ao analisar coluna temporal '{col}': {e}", exc_info=True)
                    respostas.append(
                        f"## Erro na AnÃ¡lise: {col}\n\n"
                        f"NÃ£o foi possÃ­vel completar a anÃ¡lise temporal da coluna '{col}': {str(e)}\n"
                    )
            
            # Adicionar sumÃ¡rio executivo da detecÃ§Ã£o
            header = (
                f"# AnÃ¡lise Temporal Completa\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"**Colunas analisadas:** {len(temporal_cols)} de {len(df.columns)} colunas totais\n\n"
                f"**Taxa de detecÃ§Ã£o:** {detection_summary['detection_rate']:.1%}\n\n"
                f"**MÃ©todos de detecÃ§Ã£o utilizados:** {', '.join(detection_summary['methods_used'].keys())}\n\n"
                "---\n\n"
            )
            
            return header + "\n\n---\n\n".join(respostas)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 3: FALLBACK - ANÃLISE ESTATÃSTICA GERAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        logger.info({
            'event': 'fallback_analise_geral',
            'motivo': 'nenhuma_coluna_temporal_detectada'
        })
        
        # Interpretar intenÃ§Ã£o da pergunta via LLM
        instrucoes = self._interpretar_pergunta_llm(pergunta, df)
        
        # Executar instruÃ§Ãµes e consolidar resultados
        resultados = []
        for instrucao in instrucoes:
            resultado = self._executar_instrucao(df, instrucao)
            if resultado is not None:
                justificativa = instrucao.get('justificativa', '')
                if hasattr(resultado, 'to_markdown'):
                    resultados.append(
                        f"**{instrucao.get('acao', 'MÃ©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{resultado.to_markdown()}"
                    )
                else:
                    resultados.append(
                        f"**{instrucao.get('acao', 'MÃ©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{str(resultado)}"
                    )
        
        if resultados:
            header = (
                f"# AnÃ¡lise EstatÃ­stica Geral\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"*Nenhuma coluna temporal detectada. Executando anÃ¡lise estatÃ­stica padrÃ£o.*\n\n"
                "---\n\n"
            )
            return header + "\n\n".join(resultados)
        else:
            # Fallback final: estatÃ­sticas gerais descritivas
            logger.warning("Fallback final: retornando describe() do DataFrame")
            return (
                f"# EstatÃ­sticas Descritivas\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"{df.describe().T.to_markdown()}"
            )

    def _should_use_global_csv(self, query: str, chunks_metadata: List[Dict]) -> bool:
        """
        Decide se Ã© necessÃ¡rio recorrer ao CSV completo para responder a pergunta.
        CritÃ©rios:
        - Pergunta exige anÃ¡lise de todas as colunas/linhas (ex: intervalo de todas variÃ¡veis)
        - Chunks nÃ£o possuem dados suficientes (ex: subset de colunas)
        """
        # Detecta termos que indicam anÃ¡lise global
        termos_globais = ["todas as variÃ¡veis", "todas as colunas", "intervalo de cada variÃ¡vel", "intervalo de todas", "intervalo completo", "todas as linhas", "anÃ¡lise completa"]
        if any(t in query.lower() for t in termos_globais):
            return True
        # Detecta se chunks nÃ£o cobrem todas as colunas
        if chunks_metadata:
            # Extrai colunas presentes nos chunks
            colunas_chunks = set()
            for chunk in chunks_metadata:
                if 'columns' in chunk:
                    colunas_chunks.update(chunk['columns'])
            # Se nÃºmero de colunas for pequeno, pode indicar subset
            if len(colunas_chunks) < 5:
                return True
        return False

    def reset_memory(self, session_id: str = None):
        """
        Reseta a memÃ³ria/contexto do agente para a sessÃ£o informada.
        """
        self.memory = {}
        if session_id:
            self.session_id = session_id
        self.logger.info(f"MemÃ³ria/contexto resetados para sessÃ£o: {session_id}")
    """
    Agente que responde perguntas sobre dados usando RAG vetorial + memÃ³ria persistente + LangChain.
    
    Fluxo V2.0:
    1. Inicializa sessÃ£o de memÃ³ria (se nÃ£o existir)
    2. Recupera contexto conversacional anterior
    3. Gera embedding da pergunta
    4. Busca chunks similares nos DADOS usando match_embeddings()
    5. Usa LangChain LLM para interpretar chunks + contexto histÃ³rico
    6. Salva interaÃ§Ã£o na memÃ³ria persistente
    7. Retorna resposta contextualizada
    
    SEM keywords hardcoded, SEM classificaÃ§Ã£o manual, SEM listas fixas.
    COM memÃ³ria persistente, COM LangChain, COM contexto conversacional.
    """
    
    def __init__(self):
        super().__init__(
            name="rag_data_analyzer",
            description="Analisa dados usando busca vetorial semÃ¢ntica pura com memÃ³ria persistente",
            enable_memory=True  # âœ… CRÃTICO: Habilita memÃ³ria persistente
        )
        self.logger = get_logger("agent.rag_data")
        self.embedding_gen = EmbeddingGenerator()
        
        # Inicializar LLM LangChain
        self._init_langchain_llm()
        
        self.logger.info("âœ… RAGDataAgent V2.0 inicializado - RAG vetorial + memÃ³ria + LangChain")
    
    def _init_langchain_llm(self):
        """Inicializa LLM do LangChain com fallback."""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning("âš ï¸ LangChain nÃ£o disponÃ­vel - usando fallback")
            self.llm = None
            return
        
        try:
            # Tentar Google Gemini primeiro (melhor custo-benefÃ­cio)
            from src.settings import GOOGLE_API_KEY
            if GOOGLE_API_KEY:
                self.llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-flash",
                    temperature=0.3,
                    max_tokens=2000,
                    google_api_key=GOOGLE_API_KEY
                )
                self.logger.info("âœ… LLM LangChain inicializado: Google Gemini")
                return
        except Exception as e:
            self.logger.warning(f"Google Gemini nÃ£o disponÃ­vel: {e}")
        
        try:
            # Fallback: OpenAI
            from src.settings import OPENAI_API_KEY
            if OPENAI_API_KEY:
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.3,
                    max_tokens=2000,
                    openai_api_key=OPENAI_API_KEY
                )
                self.logger.info("âœ… LLM LangChain inicializado: OpenAI GPT-4o-mini")
                return
        except Exception as e:
            self.logger.warning(f"OpenAI nÃ£o disponÃ­vel: {e}")
        
        self.llm = None
        self.logger.warning("âš ï¸ Nenhum LLM LangChain disponÃ­vel - usando fallback manual")
    
    async def process(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa query do usuÃ¡rio usando RAG vetorial + memÃ³ria persistente.
        
        VERSÃƒO ASYNC com memÃ³ria persistente.
        
        Args:
            query: Pergunta do usuÃ¡rio
            context: Contexto adicional (opcional)
            session_id: ID da sessÃ£o para memÃ³ria persistente
            
        Returns:
            Resposta baseada em busca vetorial + contexto histÃ³rico
        """
        start_time = datetime.now()
        
        try:
            self.logger.info(f"ğŸ” Processando query via RAG V2.0: {query[:80]}...")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 1. INICIALIZAR MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if not self._current_session_id:
                if session_id:
                    await self.init_memory_session(session_id)
                else:
                    session_id = await self.init_memory_session()
                self.logger.info(f"âœ… SessÃ£o de memÃ³ria inicializada: {session_id}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 2. RECUPERAR CONTEXTO CONVERSACIONAL ANTERIOR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # FILTRAR CONTEXTO: manter apenas campos relevantes para anÃ¡lise
            memory_context = {}
            if context:
                filtered_context = {}
                if 'chunks' in context:
                    filtered_context['chunks'] = context['chunks']
                if 'csv_data' in context:
                    filtered_context['csv_data'] = context['csv_data']
                # âœ… PRESERVAR FLAGS DE VISUALIZAÃ‡ÃƒO
                if 'visualization_requested' in context:
                    filtered_context['visualization_requested'] = context['visualization_requested']
                if 'visualization_type' in context:
                    filtered_context['visualization_type'] = context['visualization_type']
                if 'fallback_sample_limit' in context:
                    filtered_context['fallback_sample_limit'] = context['fallback_sample_limit']
                if 'reconstructed_df' in context:
                    filtered_context['reconstructed_df'] = context['reconstructed_df']
                context = filtered_context
            # NÃƒO recuperar contexto de memÃ³ria para queries de intervalo
            interval_terms = ['intervalo', 'mÃ­nimo', 'mÃ¡ximo', 'range', 'amplitude']
            if any(term in query.lower() for term in interval_terms):
                memory_context = {}  # Ignorar histÃ³rico/memÃ³ria
            elif self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(
                    f"âœ… Contexto de memÃ³ria recuperado: "
                    f"{len(memory_context.get('recent_messages', []))} mensagens anteriores"
                )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 3. GERAR EMBEDDING DA QUERY
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug("Gerando embedding da query...")
            embedding_result = self.embedding_gen.generate_embedding(query)
            
            # Extrair lista de floats do resultado
            if isinstance(embedding_result, list):
                query_embedding = embedding_result
            elif hasattr(embedding_result, 'embedding'):
                query_embedding = embedding_result.embedding
            else:
                return self._build_error_response("Formato de embedding invÃ¡lido")
            
            if not query_embedding or len(query_embedding) == 0:
                return self._build_error_response("Falha ao gerar embedding da query")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 4. BUSCAR CHUNKS SIMILARES NOS DADOS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug("Buscando chunks similares nos dados...")
            similar_chunks = self._search_similar_data(
                query_embedding=query_embedding,
                threshold=0.3,  # Threshold igual ao RAGAgent para capturar chunks analÃ­ticos
                limit=10
            )
            
            # SALVAR CONTEXTO DE DADOS NA TABELA agent_context
            if self.has_memory and self._current_session_id and similar_chunks:
                data_context = {
                    "dataset_info": {
                        "total_chunks": len(similar_chunks),
                        "source_types": list(set(c.get('source_type', 'unknown') for c in similar_chunks)),
                        "embedding_provider": "sentence-transformer",
                        "last_query": query[:100],
                        "query_timestamp": datetime.now().isoformat()
                    },
                    "performance_metrics": {
                        "embedding_generation_time": "N/A",  # Poderia ser medido
                        "search_time": "N/A",  # Poderia ser medido
                        "chunks_found": len(similar_chunks)
                    }
                }
                
                try:
                    await self.remember_data_context(
                        data_info=data_context,
                        context_key="current_dataset_info"
                    )
                    self.logger.debug("âœ… Contexto de dados salvo na tabela agent_context")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Falha ao salvar contexto de dados: {e}")
            
            # Fallback inteligente: se chunks nÃ£o sÃ£o suficientes OU pergunta exige anÃ¡lise global
            if not similar_chunks or self._should_use_global_csv(query, similar_chunks):
                # Tentar extrair caminho do CSV dos chunks ou do contexto
                csv_path = None
                if context and 'csv_path' in context:
                    csv_path = context['csv_path']
                elif similar_chunks:
                    for chunk in similar_chunks:
                        if 'csv_path' in chunk:
                            csv_path = chunk['csv_path']
                            break
                # Se nÃ£o encontrar, buscar na pasta processados
                if not csv_path:
                    try:
                        from src.settings import EDA_DATA_DIR_PROCESSADO
                        import os
                        import pandas as pd
                        csv_files = list(EDA_DATA_DIR_PROCESSADO.glob('*.csv'))
                        if csv_files:
                            csv_path = str(max(csv_files, key=lambda p: p.stat().st_mtime))
                    except Exception as e:
                        self.logger.warning(f"NÃ£o foi possÃ­vel localizar CSV para fallback: {e}")
                if csv_path:
                    self.logger.info(f"âš¡ Fallback: anÃ¡lise global do CSV ({csv_path}) para pergunta '{query[:60]}...'")
                    try:
                        resposta_csv = self._analisar_completo_csv(csv_path, query)
                        return self._build_response(
                            resposta_csv,
                            metadata={
                                "method": "global_csv_fallback",
                                "csv_path": csv_path,
                                "chunks_found": len(similar_chunks)
                            }
                        )
                    except Exception as e:
                        self.logger.error(f"Erro no fallback global CSV: {e}")
                        return self._build_error_response(f"Erro ao processar CSV global: {e}")
                else:
                    self.logger.error("âŒ Fallback global: CSV nÃ£o encontrado.")
                    return self._build_error_response("CSV original nÃ£o encontrado para anÃ¡lise global.")
            
            self.logger.info(f"âœ… Encontrados {len(similar_chunks)} chunks relevantes")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• VERIFICAR SE VISUALIZAÃ‡ÃƒO FOI SOLICITADA (MESMO COM CHUNKS)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            viz_requested = bool(context and context.get('visualization_requested'))
            if viz_requested:
                self.logger.info("ğŸ“Š VisualizaÃ§Ã£o solicitada - gerando grÃ¡ficos...")
                try:
                    import pandas as pd
                    from pathlib import Path
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE - ACESSO DIRETO AO CSV
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # JUSTIFICATIVA:
                    # 1. Tabela embeddings contÃ©m chunks de anÃ¡lises estatÃ­sticas (Markdown)
                    # 2. Histogramas requerem dados tabulares completos (285k linhas Ã— 31 colunas)
                    # 3. Embeddar cada linha seria ineficiente: ~$50-100 de custo + overhead
                    # 4. PadrÃ£o de mercado: LangChain, LlamaIndex, OpenAI Code Interpreter
                    #    fazem leitura direta de CSV para anÃ¡lises quantitativas
                    # 
                    # IMPLEMENTAÃ‡ÃƒO FUTURA:
                    # - TODO: Adicionar chunks raw_data na tabela embeddings durante ingestÃ£o
                    # - TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
                    # 
                    # AUDITORIA:
                    # - Log completo de acesso registrado
                    # - Metadados incluÃ­dos na resposta
                    # - Acesso read-only sem modificaÃ§Ã£o de dados
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    
                    from src.settings import EDA_DATA_DIR_PROCESSADO
                    # Buscar CSV mais recente em data/processado/
                    csv_files = list(EDA_DATA_DIR_PROCESSADO.glob("*.csv"))
                    if not csv_files:
                        self.logger.error("âŒ Nenhum arquivo CSV encontrado em data/processado/")
                        self.logger.info("âš ï¸ Continuando com resposta textual sem visualizaÃ§Ãµes")
                    else:
                        # Pegar o arquivo mais recente (Ãºltimo modificado)
                        csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
                        csv_size_mb = csv_path.stat().st_size / 1_000_000
                        self.logger.warning(
                            "âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE: Acesso direto ao CSV para visualizaÃ§Ã£o",
                            extra={
                                "event_type": "direct_csv_access",
                                "user_query": query[:100],
                                "csv_path": str(csv_path),
                                "csv_size_mb": round(csv_size_mb, 2),
                                "access_reason": "histogram_generation",
                                "session_id": self._current_session_id,
                                "agent_name": self.name,
                                "timestamp": datetime.now().isoformat(),
                                "conformidade_status": "exception_approved",
                                "alternative_implementation": "future_raw_data_embeddings",
                                "cost_saved_estimate_usd": 50.0
                            }
                        )
                        viz_df = pd.read_csv(csv_path)
                        self.logger.info(
                            f"âœ… CSV carregado para visualizaÃ§Ã£o: {viz_df.shape[0]:,} linhas Ã— {viz_df.shape[1]} colunas | "
                            f"Tamanho: {csv_size_mb:.2f} MB"
                        )
                        # Delegar para agente de visualizaÃ§Ã£o
                        # Removido: agente obsoleto csv_analysis_agent.py
                        vis_context = context.copy() if context else {}
                        vis_context['reconstructed_df'] = viz_df
                        vis_result = self._handle_visualization_query(query, vis_context)
                        if vis_result.get('metadata', {}).get('visualization_success'):
                            # Combinar resposta de visualizaÃ§Ã£o com anÃ¡lise textual dos chunks
                            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
                            context_str = "\n\n".join(context_texts[:5])
                            text_response = await self._generate_llm_response_langchain(
                                query=query,
                                context_data=context_str,
                                memory_context=memory_context,
                                chunks_metadata=similar_chunks
                            )
                            # Combinar resposta textual com informaÃ§Ã£o sobre grÃ¡ficos
                            graficos_info = vis_result.get('metadata', {}).get('graficos_gerados', [])
                            if graficos_info:
                                graficos_msg = f"\n\nğŸ“Š **VisualizaÃ§Ãµes Geradas:**\n"
                                for gf in graficos_info:
                                    graficos_msg += f"â€¢ {gf}\n"
                                combined_response = text_response + graficos_msg
                            else:
                                combined_response = text_response
                            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                            # Salvar interaÃ§Ã£o com metadados de conformidade
                            if self.has_memory:
                                await self.remember_interaction(
                                    query=query,
                                    response=combined_response,
                                    processing_time_ms=processing_time_ms,
                                    confidence=1.0,
                                    model_used="rag_v2_with_visualizations",
                                    metadata={
                                        "chunks_found": len(similar_chunks),
                                        "visualization_success": True,
                                        "graficos_gerados": len(graficos_info),
                                        "conformidade_exception": {
                                            "type": "direct_csv_access",
                                            "reason": "visualization_requires_raw_data",
                                            "csv_path": str(csv_path),
                                            "csv_size_mb": round(csv_size_mb, 2),
                                            "access_timestamp": datetime.now().isoformat(),
                                            "approved": True,
                                            "alternative_future": "raw_data_embeddings_implementation",
                                            "industry_standard": "LangChain/LlamaIndex/OpenAI_pattern",
                                            "cost_saved_usd": 50.0,
                                            "read_only": True
                                        }
                                    }
                                )
                            return self._build_response(
                                combined_response,
                                metadata={
                                    **vis_result.get('metadata', {}),
                                    "chunks_found": len(similar_chunks),
                                    "method": "rag_vectorial_v2_with_viz",
                                    "processing_time_ms": processing_time_ms,
                                    "conformidade_exception": {
                                        "type": "direct_csv_access",
                                        "reason": "visualization_requires_raw_data",
                                        "csv_path": str(csv_path),
                                        "csv_size_mb": round(csv_size_mb, 2),
                                        "approved": True,
                                        "industry_standard": True,
                                        "read_only": True,
                                        "documentation": "See comments in rag_data_agent.py lines 318-335"
                                    }
                                }
                            )
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao gerar visualizaÃ§Ãµes: {e}", exc_info=True)
                    # Continuar com resposta textual normal se visualizaÃ§Ã£o falhar
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 5. GERAR RESPOSTA COM LANGCHAIN + CONTEXTO HISTÃ“RICO
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
            context_str = "\n\n".join(context_texts[:5])  # Top 5 mais relevantes
            
            self.logger.debug("Usando LangChain LLM para gerar resposta...")
            response_text = await self._generate_llm_response_langchain(
                query=query,
                context_data=context_str,
                memory_context=memory_context,
                chunks_metadata=similar_chunks
            )
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            avg_similarity = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. SALVAR INTERAÃ‡ÃƒO NA MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=response_text,
                    processing_time_ms=processing_time_ms,
                    confidence=avg_similarity,
                    model_used="langchain_gemini" if self.llm else "fallback",
                    metadata={
                        "chunks_found": len(similar_chunks),
                        "chunks_used": min(5, len(similar_chunks)),
                        "avg_similarity": avg_similarity,
                        "top_similarity": similar_chunks[0]['similarity'],
                        "has_history": len(memory_context.get('recent_conversations', [])) > 0
                    }
                )
                self.logger.debug("âœ… InteraÃ§Ã£o salva na memÃ³ria persistente")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 7. RETORNAR RESPOSTA COM METADADOS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            return self._build_response(
                response_text,
                metadata={
                    "chunks_found": len(similar_chunks),
                    "chunks_used": min(5, len(similar_chunks)),
                    "avg_similarity": avg_similarity,
                    "method": "rag_vectorial_v2",
                    "top_similarity": similar_chunks[0]['similarity'] if similar_chunks else 0,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao processar query: {str(e)}", exc_info=True)
            return self._build_error_response(f"Erro no processamento: {str(e)}")
    
    def _search_similar_data(
        self,
        query_embedding: List[float],
        threshold: float = 0.5,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Busca chunks similares nos dados usando match_embeddings RPC.
        
        Args:
            query_embedding: Embedding da query
            threshold: Threshold de similaridade (0.0 - 1.0)
            limit: NÃºmero mÃ¡ximo de resultados
            
        Returns:
            Lista de chunks similares com metadata
        """
        try:
            # Chamar funÃ§Ã£o RPC match_embeddings
            response = supabase.rpc(
                'match_embeddings',
                {
                    'query_embedding': query_embedding,
                    'similarity_threshold': threshold,
                    'match_count': limit
                }
            ).execute()
            
            if not response.data:
                self.logger.warning("Nenhum chunk similar encontrado")
                return []
            
            self.logger.debug(f"Encontrados {len(response.data)} chunks similares")
            # Parsing defensivo dos embeddings
            from src.embeddings.vector_store import parse_embedding_from_api, VECTOR_DIMENSIONS
            parsed_chunks = []
            for chunk in response.data:
                embedding_raw = chunk.get('embedding')
                try:
                    chunk['embedding'] = parse_embedding_from_api(embedding_raw, VECTOR_DIMENSIONS)
                except Exception as e:
                    self.logger.warning(f"Falha ao parsear embedding do chunk: {e}")
                    chunk['embedding'] = None
                parsed_chunks.append(chunk)
            return parsed_chunks
            
        except Exception as e:
            self.logger.error(f"Erro na busca vetorial: {str(e)}")
            return []
    
    async def _generate_llm_response_langchain(
        self,
        query: str,
        context_data: str,
        memory_context: dict,
        chunks_metadata: list
    ) -> str:
        try:
            # Preparar contexto histÃ³rico da conversa
            history_context = ""
            if memory_context.get('recent_messages') and len(memory_context['recent_messages']) > 0:
                history_context = "\n\n**Contexto da Conversa Anterior:**\n"
                for msg in memory_context['recent_messages'][-6:]:  # Ãšltimas 6 mensagens (3 pares user/assistant)
                    msg_type = msg.get('type', 'unknown')
                    content = msg.get('content', '')[:200]  # Limitar a 200 chars
                    if msg_type == 'user':
                        history_context += f"- UsuÃ¡rio perguntou: {content}\n"
                    elif msg_type == 'assistant':
                        history_context += f"- Assistente respondeu: {content}\n"
                history_context += "\n"

            # Preparar prompt DINÃ‚MICO baseado no tipo de query
            query_lower = query.lower()
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ”¥ V3.0: ORQUESTRAÃ‡ÃƒO INTELIGENTE VIA LLM (ZERO HARD-CODING)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 
            # ANTES (V2.0): ~240 linhas de cascata if/elif com keywords hardcoded
            # DEPOIS (V3.0): ClassificaÃ§Ã£o semÃ¢ntica + orquestraÃ§Ã£o modular
            # 
            # BenefÃ­cios:
            # - âœ… Reconhece QUALQUER sinÃ´nimo (nÃ£o limitado a lista fixa)
            # - âœ… Processa queries mistas simultaneamente
            # - âœ… ExtensÃ­vel (novos tipos sem modificar cÃ³digo)
            # - âœ… ManutenÃ­vel (cÃ³digo limpo e modular)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Detectar se Ã© query sobre HISTÃ“RICO (caso especial)
            is_history_query = any(term in query_lower for term in [
                'pergunta anterior', 'perguntei antes', 'falamos sobre',
                'conversamos sobre', 'vocÃª disse', 'previous question', 'asked before'
            ])
            
            if is_history_query:
                # Query sobre HISTÃ“RICO - usar memÃ³ria conversacional
                self.logger.info("ğŸ“œ Query sobre histÃ³rico conversacional detectada")
                
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© responder sobre o HISTÃ“RICO da conversa. "
                    "Use o contexto da conversa anterior fornecido para responder. "
                    "Seja claro e objetivo, referenciando exatamente o que foi discutido."
                )
                user_prompt = (
                    f"{history_context}"
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Consulte o histÃ³rico da conversa acima\n"
                    "- Responda referenciando exatamente o que foi perguntado/respondido anteriormente\n"
                    "- Se nÃ£o houver histÃ³rico suficiente, informe claramente\n"
                    "- Finalize com: 'Posso esclarecer mais alguma coisa sobre nossa conversa?'\n\n"
                    "**Resposta:**"
                )
                
                # Usar LLM diretamente para query de histÃ³rico
                if self.llm and LANGCHAIN_AVAILABLE:
                    messages = [
                        SystemMessage(content=system_prompt),
                        HumanMessage(content=user_prompt)
                    ]
                    response = await asyncio.to_thread(self.llm.invoke, messages)
                    final_response = response.content
                else:
                    final_response = "HistÃ³rico nÃ£o disponÃ­vel (LLM indisponÃ­vel)"
            
            else:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # ğŸ”¥ V3.0: ORQUESTRAÃ‡ÃƒO ANALÃTICA MODULAR
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                try:
                    self.logger.info("ğŸ”¥ Executando V3.0: AnalysisOrchestrator")
                    
                    # Carregar DataFrame do CSV se disponÃ­vel
                    df = None
                    if context and 'csv_data' in context:
                        import pandas as pd
                        csv_path = context['csv_data'].get('path')
                        if csv_path:
                            try:
                                df = pd.read_csv(csv_path)
                                self.logger.info(f"ğŸ“Š DataFrame carregado: {df.shape}")
                            except Exception as e:
                                self.logger.error(f"Erro ao carregar CSV: {e}")
                    
                    # Se DataFrame disponÃ­vel, usar orchestrator V3
                    if df is not None and not df.empty:
                        final_response = self._build_analytical_response_v3(
                            query=query,
                            df=df,
                            context_data=context_data,
                            history_context=history_context
                        )
                    else:
                        # Fallback: resposta baseada apenas em chunks (sem anÃ¡lise executada)
                        self.logger.warning("âš ï¸ DataFrame nÃ£o disponÃ­vel - usando fallback chunks-only")
                        final_response = self._fallback_basic_response(
                            query=query,
                            context_data=context_data,
                            history_context=history_context
                        )
                
                except Exception as e:
                    self.logger.error(f"âŒ Erro no fluxo V3.0: {e}", exc_info=True)
                    # Fallback final
                    final_response = self._fallback_basic_response(
                        query=query,
                        context_data=context_data,
                        history_context=history_context
                    )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # RESPOSTA FINAL
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. SALVAR NA MEMÃ“RIA E RETORNAR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            processing_time_ms = (datetime.now() - start_time).total_seconds() * 1000
            
            # Calcular mÃ©tricas se similar_chunks disponÃ­vel
            chunks_count = len(similar_chunks) if similar_chunks else 0
            avg_sim = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks) if similar_chunks else 0.0
            top_sim = similar_chunks[0]['similarity'] if similar_chunks else 0.0
            
            # Salvar interaÃ§Ã£o na memÃ³ria persistente
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=final_response,
                    processing_time_ms=processing_time_ms,
                    confidence=0.85,
                    model_used="rag_v3_orchestrated",
                    metadata={
                        "chunks_found": chunks_count,
                        "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                        "method": "rag_vectorial_v3",
                        "architecture": "modular_orchestrated",
                        "zero_hardcoding": True
                    }
                )
            
            # Retornar resposta formatada
            return self._build_response(
                final_response,
                metadata={
                    "chunks_found": chunks_count,
                    "chunks_used": min(5, chunks_count) if chunks_count > 0 else 0,
                    "avg_similarity": avg_sim,
                    "method": "rag_vectorial_v3",
                    "architecture": "modular_orchestrated",
                    "top_similarity": top_sim,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao gerar resposta LLM: {str(e)}", exc_info=True)
            return self._format_raw_data_response(query, chunks_metadata)
    
    def _format_raw_data_response(
        self,
        query: str,
        chunks_metadata: List[Dict]
        ) -> str:
        """
        Fallback: usa agente de sÃ­ntese para consolidar dados se LLM falhar.
        """
        # Extrair apenas o texto dos chunks
        chunks = [chunk.get('chunk_text', '') for chunk in chunks_metadata]
        
        # Chamar agente de sÃ­ntese para consolidar
        from src.agent.rag_synthesis_agent import synthesize_response
        try:
            return synthesize_response(chunks, query, use_llm=False)
        except Exception as e:
            self.logger.error(f"Erro no agente de sÃ­ntese: {e}")
            # Fallback extremo: resposta estruturada mÃ­nima
            return f"""## Resposta para: {query}

**Status:** âš ï¸ Erro na sÃ­ntese

NÃ£o foi possÃ­vel processar completamente a consulta devido a um erro tÃ©cnico.
Por favor, reformule sua pergunta ou entre em contato com o suporte.

_Erro: {str(e)}_"""
    
    def _build_error_response(self, error_msg: str) -> Dict[str, Any]:
        """ConstrÃ³i resposta de erro padronizada."""
        return self._build_response(
            f"âŒ {error_msg}",
            metadata={"error": True, "method": "rag_vectorial_v2"}
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO SÃNCRONO WRAPPER (para compatibilidade retroativa)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process_sync(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Wrapper sÃ­ncrono para compatibilidade com cÃ³digo legado.
        
        âš ï¸ DEPRECATED: Use process() async quando possÃ­vel.
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.process(query, context))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO DE CARREGAMENTO CSV (mantido da versÃ£o anterior)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def load_csv_to_embeddings(
        self,
        csv_path: str,
        chunk_size: int = 1000,
        overlap: int = 100
    ) -> Dict[str, Any]:
        """
        Carrega CSV para a tabela embeddings.
        
        Args:
            csv_path: Caminho do arquivo CSV
            chunk_size: Tamanho dos chunks
            overlap: Overlap entre chunks
            
        Returns:
            Status do carregamento
        """
        try:
            self.logger.info(f"ğŸ“‚ Carregando CSV: {csv_path}")
            
            import pandas as pd
            from src.embeddings.chunker import CSVChunker
            
            # Ler CSV
            df = pd.read_csv(csv_path)
            self.logger.info(f"âœ… CSV lido: {len(df)} linhas, {len(df.columns)} colunas")
            
            # Criar chunks
            chunker = CSVChunker(chunk_size=chunk_size, overlap=overlap)
            chunks = chunker.chunk_dataframe(df)
            self.logger.info(f"âœ… Criados {len(chunks)} chunks")
            
            # Gerar embeddings e salvar
            inserted_count = 0
            for i, chunk in enumerate(chunks):
                try:
                    # Gerar embedding
                    embedding = self.embedding_gen.generate_embedding(chunk['text'])
                    
                    # Salvar na tabela embeddings
                    insert_data = {
                        'chunk_text': chunk['text'],
                        'embedding': embedding,
                        'metadata': {
                            'source': csv_path,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'created_at': datetime.now().isoformat()
                        }
                    }
                    
                    result = supabase.table('embeddings').insert(insert_data).execute()
                    
                    if result.data:
                        inserted_count += 1
                        if (i + 1) % 10 == 0:
                            self.logger.info(f"Progresso: {i+1}/{len(chunks)} chunks inseridos")
                
                except Exception as chunk_error:
                    self.logger.warning(f"Erro no chunk {i}: {chunk_error}")
                    continue
            
            self.logger.info(f"âœ… Carregamento concluÃ­do: {inserted_count}/{len(chunks)} chunks inseridos")
            
            return self._build_response(
                f"âœ… CSV carregado com sucesso: {inserted_count} chunks inseridos na base vetorial",
                metadata={
                    'csv_path': csv_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'chunks_created': len(chunks),
                    'chunks_inserted': inserted_count
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao carregar CSV: {str(e)}")
            return self._build_error_response(f"Falha ao carregar CSV: {str(e)}")
