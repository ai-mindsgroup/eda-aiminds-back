"""
Agente de AnÃ¡lise de Dados via RAG Vetorial Puro com MemÃ³ria Persistente e LangChain.

VERSÃƒO 2.0 - REFATORADA:
- âœ… MemÃ³ria persistente em Supabase (tabelas agent_sessions, agent_conversations, agent_context)
- âœ… LangChain integrado nativamente (ChatOpenAI, ChatGoogleGenerativeAI)
- âœ… MÃ©todos async para performance
- âœ… Contexto conversacional entre interaÃ§Ãµes
- âœ… Busca vetorial pura (sem keywords hardcoded)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸  EXCEÃ‡ÃƒO DE CONFORMIDADE: ACESSO DIRETO A CSV PARA VISUALIZAÃ‡Ã•ES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONTEXTO:
- Tabela 'embeddings' armazena chunks de anÃ¡lises estatÃ­sticas (Markdown)
- VisualizaÃ§Ãµes (histogramas) requerem dados tabulares completos (285k linhas)
- Embeddar cada linha seria ineficiente: ~$50-100 custo + overhead desnecessÃ¡rio

SOLUÃ‡ÃƒO IMPLEMENTADA:
- Quando visualizaÃ§Ã£o Ã© solicitada, acessa CSV diretamente via pd.read_csv()
- Acesso Ã© READ-ONLY, sem modificaÃ§Ã£o de dados
- Log completo de auditoria registrado (linhas 318-350)
- Metadados de conformidade incluÃ­dos em todas as respostas

JUSTIFICATIVA (ADERENTE A BOAS PRÃTICAS DE MERCADO):
1. PadrÃ£o da indÃºstria: LangChain CSV Agents, LlamaIndex, OpenAI Code Interpreter
2. SeparaÃ§Ã£o de responsabilidades: RAG para busca semÃ¢ntica, CSV para dados tabulares
3. Custo-benefÃ­cio: evita armazenamento/processamento desnecessÃ¡rio
4. Performance: leitura direta Ã© mais rÃ¡pida que reconstituiÃ§Ã£o de embeddings

IMPLEMENTAÃ‡ÃƒO FUTURA (Opcional):
- TODO: Adicionar chunks 'raw_data' na tabela embeddings durante ingestÃ£o
- TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
- TODO: Adicionar configuraÃ§Ã£o para escolher entre direct-access vs embeddings

AUDITORIA E COMPLIANCE:
- âœ… Log detalhado com event_type, timestamp, session_id, csv_path, size
- âœ… Metadados em response.metadata['conformidade_exception']
- âœ… DocumentaÃ§Ã£o clara da exceÃ§Ã£o e justificativa
- âœ… AprovaÃ§Ã£o registrada (approved=True)

REFERÃŠNCIAS:
- LangChain CSV Agent: https://python.langchain.com/docs/integrations/toolkits/csv
- OpenAI Code Interpreter: https://openai.com/blog/code-interpreter
- Hybrid RAG Architectures: https://docs.llamaindex.ai/en/stable/examples/query_engine/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import logging
from typing import Any, Dict, List, Optional
import json
from datetime import datetime
import asyncio

from agent.base_agent import BaseAgent, AgentError
from vectorstore.supabase_client import supabase
from embeddings.generator import EmbeddingGenerator
from utils.logging_config import get_logger
from analysis.intent_classifier import IntentClassifier, AnalysisIntent

# Imports LangChain
try:
    from langchain_openai import ChatOpenAI
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain.schema import HumanMessage, SystemMessage, AIMessage
    from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain.chains import ConversationChain
    from langchain.memory import ConversationBufferMemory
    from langchain_experimental.tools import PythonREPLTool
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    ChatOpenAI = None
    ChatGoogleGenerativeAI = None
    HumanMessage = None
    SystemMessage = None
    AIMessage = None
    PythonREPLTool = None
    print(f"âš ï¸ LangChain nÃ£o disponÃ­vel: {e}")


class RAGDataAgent(BaseAgent):
    def _interpretar_pergunta_llm(self, pergunta: str, df):
        """
        âœ… V3.0: Utiliza IntentClassifier para classificaÃ§Ã£o semÃ¢ntica SEM HARD-CODING.
        
        Retorna instruÃ§Ãµes analÃ­ticas baseadas na intenÃ§Ã£o classificada pela LLM.
        Cada instruÃ§Ã£o Ã© um dict: {'acao': ..., 'colunas': [...], 'params': {}, 'justificativa': str}
        """
        if not self.llm:
            # Fallback: retorna instruÃ§Ã£o genÃ©rica
            return [{'acao': 'estatÃ­sticas gerais', 'colunas': list(df.columns), 'params': {}, 
                    'justificativa': 'LLM indisponÃ­vel, fornecendo estatÃ­sticas gerais.'}]
        
        try:
            # ğŸ”¥ V3.0: Usar IntentClassifier para classificaÃ§Ã£o semÃ¢ntica
            classifier = IntentClassifier(llm=self.llm, logger=self.logger)
            
            # Classificar intenÃ§Ã£o da pergunta
            context = {
                'available_columns': list(df.columns),
                'dataframe_info': f"Shape: {df.shape}, Colunas numÃ©ricas: {df.select_dtypes(include=['number']).columns.tolist()}"
            }
            
            classification_result = classifier.classify(query=pergunta, context=context)
            
            # Log da classificaÃ§Ã£o
            self.logger.info({
                'event': 'intent_classification',
                'primary_intent': classification_result.primary_intent.value,
                'secondary_intents': [intent.value for intent in classification_result.secondary_intents],
                'confidence': classification_result.confidence,
                'reasoning': classification_result.reasoning
            })
            
            # ğŸ¯ Mapear intenÃ§Ãµes para instruÃ§Ãµes analÃ­ticas
            instrucoes = []
            
            # Processar intenÃ§Ã£o primÃ¡ria
            primary_action = self._intent_to_action(
                classification_result.primary_intent, 
                df, 
                classification_result.reasoning
            )
            if primary_action:
                instrucoes.append(primary_action)
            
            # Processar intenÃ§Ãµes secundÃ¡rias se existirem
            for secondary_intent in classification_result.secondary_intents:
                secondary_action = self._intent_to_action(
                    secondary_intent, 
                    df, 
                    f"IntenÃ§Ã£o secundÃ¡ria detectada: {secondary_intent.value}"
                )
                if secondary_action and secondary_action not in instrucoes:
                    instrucoes.append(secondary_action)
            
            # Garantir pelo menos uma instruÃ§Ã£o
            if not instrucoes:
                instrucoes = [{
                    'acao': 'estatÃ­sticas gerais',
                    'colunas': list(df.columns),
                    'params': {},
                    'justificativa': 'Nenhuma intenÃ§Ã£o especÃ­fica detectada, fornecendo visÃ£o geral.'
                }]
            
            self.logger.info({
                'event': 'instructions_generated',
                'num_instructions': len(instrucoes),
                'actions': [ins['acao'] for ins in instrucoes]
            })
            
            return instrucoes
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao interpretar pergunta com IntentClassifier: {e}")
            # Fallback para interpretaÃ§Ã£o bÃ¡sica via LLM direta
            return self._fallback_interpretation(pergunta, df)
    
    def _intent_to_action(self, intent: AnalysisIntent, df, justificativa: str) -> Optional[Dict]:
        """
        ğŸ¯ Converte AnalysisIntent em instruÃ§Ã£o analÃ­tica.
        Mapeia tipos de intenÃ§Ã£o para aÃ§Ãµes especÃ­ficas.
        """
        # Selecionar colunas numÃ©ricas por padrÃ£o
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        all_cols = list(df.columns)
        
        intent_map = {
            AnalysisIntent.STATISTICAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.FREQUENCY: {
                'acao': 'frequency_analysis',
                'colunas': all_cols,
                'params': {'top_n': 10},
                'justificativa': justificativa
            },
            AnalysisIntent.TEMPORAL: {
                'acao': 'temporal_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.CLUSTERING: {
                'acao': 'clustering_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {'n_clusters': 3},
                'justificativa': justificativa
            },
            AnalysisIntent.CORRELATION: {
                'acao': 'correlation_analysis',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.OUTLIERS: {
                'acao': 'outlier_detection',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.COMPARISON: {
                'acao': 'comparison_analysis',
                'colunas': all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.VISUALIZATION: {
                'acao': 'visualization_request',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            },
            AnalysisIntent.GENERAL: {
                'acao': 'estatÃ­sticas gerais',
                'colunas': numeric_cols if numeric_cols else all_cols,
                'params': {},
                'justificativa': justificativa
            }
        }
        
        return intent_map.get(intent)
    
    def _fallback_interpretation(self, pergunta: str, df) -> List[Dict]:
        """
        Fallback: interpretaÃ§Ã£o bÃ¡sica via LLM direta quando IntentClassifier falha.
        MantÃ©m apenas lÃ³gica essencial, SEM hard-coding de keywords.
        """
        prompt = (
            "VocÃª Ã© um especialista em anÃ¡lise de dados.\n"
            "Interprete a pergunta do usuÃ¡rio e retorne uma lista JSON de instruÃ§Ãµes analÃ­ticas.\n"
            "Formato de cada instruÃ§Ã£o: {'acao': str, 'colunas': [str], 'params': {}, 'justificativa': str}\n"
            f"Pergunta: {pergunta}\n"
            f"Colunas disponÃ­veis: {list(df.columns)}\n"
            "Responda APENAS com o JSON, sem explicaÃ§Ãµes."
        )
        
        try:
            response = self.llm.invoke(prompt)
            import json
            instrucoes = json.loads(response.content)
            return instrucoes if isinstance(instrucoes, list) else [instrucoes]
        except Exception as e:
            self.logger.error(f"Fallback interpretation falhou: {e}")
            return [{
                'acao': 'estatÃ­sticas gerais',
                'colunas': list(df.columns),
                'params': {},
                'justificativa': 'InterpretaÃ§Ã£o padrÃ£o devido a erro.'
            }]

    def _executar_instrucao(self, df, instrucao):
        """
        Executa uma instruÃ§Ã£o analÃ­tica sobre o DataFrame.
        Suporta mÃ©tricas nativas pandas/NumPy e delega compostas Ã  LLM.
        """
        acao = instrucao.get('acao','')
        colunas = instrucao.get('colunas', list(df.columns))
        params = instrucao.get('params', {})
        try:
            # Normalizar nome da aÃ§Ã£o para comparaÃ§Ã£o (tolerante a maiÃºsculas/minÃºsculas)
            acao_norm = str(acao).strip().lower()
            # MÃ©tricas nativas pandas
            if acao_norm in ('mÃ©dia', 'media', 'mean'):
                return df[colunas].mean().to_frame(name='MÃ©dia')
            if acao_norm in ('mediana', 'median'):
                return df[colunas].median().to_frame(name='Mediana')
            if acao_norm in ('moda', 'mode'):
                return df[colunas].mode().T
            if acao_norm in ('desvio padrÃ£o', 'desvio padrao', 'std', 'standard deviation'):
                return df[colunas].std().to_frame(name='Desvio padrÃ£o')
            if acao_norm in ('variÃ¢ncia', 'variancia', 'variance', 'var'):
                # VariÃ¢ncia populacional/por padrÃ£o pandas var() usa ddof=1 (amostral). Mantemos default.
                return df[colunas].var().to_frame(name='VariÃ¢ncia')
            if acao_norm in ('intervalo', 'minmax', 'min_max', 'mÃ­nimo', 'mÃ¡ximo'):
                resultado = df[colunas].agg(['min','max']).T
                resultado.columns = ['MÃ­nimo','MÃ¡ximo']
                return resultado
            if acao_norm in ('estatÃ­sticas gerais', 'estatisticas gerais', 'describe', 'summary', 'resumo'):
                return df[colunas].describe().T
            # MÃ©tricas compostas ou extraordinÃ¡rias: delega Ã  LLM com execuÃ§Ã£o SEGURA
            if self.llm and PythonREPLTool:
                try:
                    # ğŸ”’ SEGURANÃ‡A: Usar PythonREPLTool com sandbox isolado
                    python_repl = PythonREPLTool()
                    
                    prompt = (
                        f"Receba instruÃ§Ã£o analÃ­tica: {instrucao}.\n"
                        f"DataFrame jÃ¡ estÃ¡ disponÃ­vel como variÃ¡vel 'df'.\n"
                        f"Colunas disponÃ­veis: {list(df.columns)}.\n"
                        "Gere cÃ³digo Python (pandas/numpy) que:\n"
                        "1. Execute a mÃ©trica pedida\n"
                        "2. Armazene o resultado em uma variÃ¡vel chamada 'resultado'\n"
                        "3. Retorne 'resultado' na Ãºltima linha\n"
                        "Retorne APENAS o cÃ³digo, sem explicaÃ§Ãµes, sem markdown.\n"
                        "Exemplo: resultado = df['coluna'].mean()"
                    )
                    
                    response = self.llm.invoke(prompt)
                    code = response.content.strip()
                    
                    # Remove markdown code blocks se presentes
                    if code.startswith("```python"):
                        code = code.split("```python")[1].split("```")[0].strip()
                    elif code.startswith("```"):
                        code = code.split("```")[1].split("```")[0].strip()
                    
                    # Log cÃ³digo antes de executar (auditoria)
                    self.logger.info(f"ğŸ”’ Executando cÃ³digo seguro via PythonREPLTool:\n{code[:200]}...")
                    
                    # Preparar contexto com DataFrame
                    import pandas as pd
                    import numpy as np
                    globals_context = {'df': df, 'pd': pd, 'np': np}
                    
                    # ğŸ”’ ExecuÃ§Ã£o segura via PythonREPLTool (sandbox isolado)
                    # Nota: PythonREPLTool executa em ambiente isolado sem acesso ao filesystem
                    resultado = python_repl.run(code, globals=globals_context)
                    
                    self.logger.info(f"âœ… CÃ³digo executado com sucesso via PythonREPLTool")
                    return resultado
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao executar cÃ³digo via PythonREPLTool: {e}")
                    self.logger.debug(f"CÃ³digo problemÃ¡tico: {code}")
                    return None
            else:
                self.logger.warning("LLM ou PythonREPLTool nÃ£o disponÃ­vel para mÃ©tricas compostas")
                return None
        except Exception as e:
            self.logger.warning(f"Falha ao executar instruÃ§Ã£o: {instrucao} | Erro: {e}")
            return None

    def _analisar_completo_csv(self, csv_path: str, pergunta: str, override_temporal_col: str = None,
                               temporal_col_names: list = None, accepted_types: tuple = None) -> str:
        """
        Executa anÃ¡lise temporal robusta e modular usando arquitetura refatorada V2.0.
        
        ARQUITETURA MODULAR:
        - DetecÃ§Ã£o via TemporalColumnDetector (src/analysis/temporal_detection.py)
        - AnÃ¡lise via TemporalAnalyzer (src/analysis/temporal_analyzer.py)
        - Fallback para anÃ¡lise estatÃ­stica geral quando nÃ£o houver colunas temporais
        
        CritÃ©rios de detecÃ§Ã£o:
        - Override manual (prioritÃ¡rio)
        - Tipo datetime64 nativo
        - Nomes comuns parametrizÃ¡veis (case-insensitive)
        - ConversÃ£o de strings temporais
        - SequÃªncias numÃ©ricas temporais (modo agressivo)
        
        ParÃ¢metros:
            - override_temporal_col: forÃ§a uso de coluna especÃ­fica (ou None para auto)
            - temporal_col_names: lista de nomes comuns (default: ["time", "date", "timestamp", "data", "datetime"])
            - accepted_types: DEPRECATED - mantido para backward compatibility
            
        Returns:
            String formatada em Markdown com anÃ¡lises temporais e/ou estatÃ­sticas
        """
        import pandas as pd
        from analysis.temporal_detection import TemporalColumnDetector, TemporalDetectionConfig
        from analysis.temporal_analyzer import TemporalAnalyzer
        
        # Carregar dados
        df = pd.read_csv(csv_path)
        
        logger = self.logger if hasattr(self, 'logger') else logging.getLogger(__name__)
        logger.info({
            'event': 'inicio_analise_csv_v2',
            'csv_path': csv_path,
            'shape': df.shape,
            'override_temporal_col': override_temporal_col
        })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 1: DETECÃ‡ÃƒO DE COLUNAS TEMPORAIS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # Configurar detector com parÃ¢metros customizÃ¡veis
        detection_config = TemporalDetectionConfig()
        if temporal_col_names:
            detection_config.common_names = temporal_col_names
        
        detector = TemporalColumnDetector(config=detection_config)
        
        try:
            detection_results = detector.detect(df, override_column=override_temporal_col)
            temporal_cols = detector.get_detected_columns(detection_results)
            detection_summary = detector.get_detection_summary(detection_results)
            
            logger.info({
                'event': 'deteccao_temporal_concluida',
                'colunas_detectadas': temporal_cols,
                'total_colunas': len(df.columns),
                'taxa_deteccao': detection_summary['detection_rate'],
                'metodos_usados': detection_summary['methods_used']
            })
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o de colunas temporais: {e}", exc_info=True)
            temporal_cols = []
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 2: ANÃLISE TEMPORAL (se colunas detectadas)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        if temporal_cols:
            logger.info(f"Executando anÃ¡lise temporal em {len(temporal_cols)} coluna(s)")
            
            analyzer = TemporalAnalyzer(logger=logger)
            respostas = []
            
            for col in temporal_cols:
                try:
                    # Executar anÃ¡lise temporal avanÃ§ada
                    result = analyzer.analyze(df, col, enable_advanced=True)
                    
                    # Gerar relatÃ³rio Markdown
                    respostas.append(result.to_markdown())
                    
                    logger.info({
                        'event': 'analise_temporal_coluna_concluida',
                        'coluna': col,
                        'trend_type': result.trend.get('type'),
                        'anomalies_count': result.anomalies.get('count', 0),
                        'seasonality_detected': result.seasonality.get('detected', False)
                    })
                except Exception as e:
                    logger.error(f"Erro ao analisar coluna temporal '{col}': {e}", exc_info=True)
                    respostas.append(
                        f"## Erro na AnÃ¡lise: {col}\n\n"
                        f"NÃ£o foi possÃ­vel completar a anÃ¡lise temporal da coluna '{col}': {str(e)}\n"
                    )
            
            # Adicionar sumÃ¡rio executivo da detecÃ§Ã£o
            header = (
                f"# AnÃ¡lise Temporal Completa\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"**Colunas analisadas:** {len(temporal_cols)} de {len(df.columns)} colunas totais\n\n"
                f"**Taxa de detecÃ§Ã£o:** {detection_summary['detection_rate']:.1%}\n\n"
                f"**MÃ©todos de detecÃ§Ã£o utilizados:** {', '.join(detection_summary['methods_used'].keys())}\n\n"
                "---\n\n"
            )
            
            return header + "\n\n---\n\n".join(respostas)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ETAPA 3: FALLBACK - ANÃLISE ESTATÃSTICA GERAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        logger.info({
            'event': 'fallback_analise_geral',
            'motivo': 'nenhuma_coluna_temporal_detectada'
        })
        
        # Interpretar intenÃ§Ã£o da pergunta via LLM
        instrucoes = self._interpretar_pergunta_llm(pergunta, df)
        
        # Executar instruÃ§Ãµes e consolidar resultados
        resultados = []
        for instrucao in instrucoes:
            resultado = self._executar_instrucao(df, instrucao)
            if resultado is not None:
                justificativa = instrucao.get('justificativa', '')
                if hasattr(resultado, 'to_markdown'):
                    resultados.append(
                        f"**{instrucao.get('acao', 'MÃ©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{resultado.to_markdown()}"
                    )
                else:
                    resultados.append(
                        f"**{instrucao.get('acao', 'MÃ©trica')}**\n"
                        f"{justificativa}\n\n"
                        f"{str(resultado)}"
                    )
        
        if resultados:
            header = (
                f"# AnÃ¡lise EstatÃ­stica Geral\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"*Nenhuma coluna temporal detectada. Executando anÃ¡lise estatÃ­stica padrÃ£o.*\n\n"
                "---\n\n"
            )
            return header + "\n\n".join(resultados)
        else:
            # Fallback final: estatÃ­sticas gerais descritivas
            logger.warning("Fallback final: retornando describe() do DataFrame")
            return (
                f"# EstatÃ­sticas Descritivas\n\n"
                f"**Dataset:** `{csv_path}`\n\n"
                f"{df.describe().T.to_markdown()}"
            )

    def _should_use_global_csv(self, query: str, chunks_metadata: List[Dict]) -> bool:
        """
        Decide se Ã© necessÃ¡rio recorrer ao CSV completo para responder a pergunta.
        CritÃ©rios:
        - Pergunta exige anÃ¡lise de todas as colunas/linhas (ex: intervalo de todas variÃ¡veis)
        - Chunks nÃ£o possuem dados suficientes (ex: subset de colunas)
        """
        # Detecta termos que indicam anÃ¡lise global
        termos_globais = ["todas as variÃ¡veis", "todas as colunas", "intervalo de cada variÃ¡vel", "intervalo de todas", "intervalo completo", "todas as linhas", "anÃ¡lise completa"]
        if any(t in query.lower() for t in termos_globais):
            return True
        # Detecta se chunks nÃ£o cobrem todas as colunas
        if chunks_metadata:
            # Extrai colunas presentes nos chunks
            colunas_chunks = set()
            for chunk in chunks_metadata:
                if 'columns' in chunk:
                    colunas_chunks.update(chunk['columns'])
            # Se nÃºmero de colunas for pequeno, pode indicar subset
            if len(colunas_chunks) < 5:
                return True
        return False

    def reset_memory(self, session_id: str = None):
        """
        Reseta a memÃ³ria/contexto do agente para a sessÃ£o informada.
        """
        self.memory = {}
        if session_id:
            self.session_id = session_id
        self.logger.info(f"MemÃ³ria/contexto resetados para sessÃ£o: {session_id}")
    """
    Agente que responde perguntas sobre dados usando RAG vetorial + memÃ³ria persistente + LangChain.
    
    Fluxo V2.0:
    1. Inicializa sessÃ£o de memÃ³ria (se nÃ£o existir)
    2. Recupera contexto conversacional anterior
    3. Gera embedding da pergunta
    4. Busca chunks similares nos DADOS usando match_embeddings()
    5. Usa LangChain LLM para interpretar chunks + contexto histÃ³rico
    6. Salva interaÃ§Ã£o na memÃ³ria persistente
    7. Retorna resposta contextualizada
    
    SEM keywords hardcoded, SEM classificaÃ§Ã£o manual, SEM listas fixas.
    COM memÃ³ria persistente, COM LangChain, COM contexto conversacional.
    """
    
    def __init__(self):
        super().__init__(
            name="rag_data_analyzer",
            description="Analisa dados usando busca vetorial semÃ¢ntica pura com memÃ³ria persistente",
            enable_memory=True  # âœ… CRÃTICO: Habilita memÃ³ria persistente
        )
        self.logger = get_logger("agent.rag_data")
        self.embedding_gen = EmbeddingGenerator()
        
        # Inicializar LLM LangChain
        self._init_langchain_llm()
        
        self.logger.info("âœ… RAGDataAgent V2.0 inicializado - RAG vetorial + memÃ³ria + LangChain")
    
    def _init_langchain_llm(self):
        """Inicializa LLM do LangChain com fallback."""
        if not LANGCHAIN_AVAILABLE:
            self.logger.warning("âš ï¸ LangChain nÃ£o disponÃ­vel - usando fallback")
            self.llm = None
            return
        
        try:
            # Tentar Google Gemini primeiro (melhor custo-benefÃ­cio)
            from src.settings import GOOGLE_API_KEY
            if GOOGLE_API_KEY:
                self.llm = ChatGoogleGenerativeAI(
                    model="gemini-1.5-flash",
                    temperature=0.3,
                    max_tokens=2000,
                    google_api_key=GOOGLE_API_KEY
                )
                self.logger.info("âœ… LLM LangChain inicializado: Google Gemini")
                return
        except Exception as e:
            self.logger.warning(f"Google Gemini nÃ£o disponÃ­vel: {e}")
        
        try:
            # Fallback: OpenAI
            from src.settings import OPENAI_API_KEY
            if OPENAI_API_KEY:
                self.llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.3,
                    max_tokens=2000,
                    openai_api_key=OPENAI_API_KEY
                )
                self.logger.info("âœ… LLM LangChain inicializado: OpenAI GPT-4o-mini")
                return
        except Exception as e:
            self.logger.warning(f"OpenAI nÃ£o disponÃ­vel: {e}")
        
        self.llm = None
        self.logger.warning("âš ï¸ Nenhum LLM LangChain disponÃ­vel - usando fallback manual")
    
    async def process(
        self, 
        query: str, 
        context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Processa query do usuÃ¡rio usando RAG vetorial + memÃ³ria persistente.
        
        VERSÃƒO ASYNC com memÃ³ria persistente.
        
        Args:
            query: Pergunta do usuÃ¡rio
            context: Contexto adicional (opcional)
            session_id: ID da sessÃ£o para memÃ³ria persistente
            
        Returns:
            Resposta baseada em busca vetorial + contexto histÃ³rico
        """
        start_time = datetime.now()
        
        try:
            self.logger.info(f"ğŸ” Processando query via RAG V2.0: {query[:80]}...")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 1. INICIALIZAR MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if not self._current_session_id:
                if session_id:
                    await self.init_memory_session(session_id)
                else:
                    session_id = await self.init_memory_session()
                self.logger.info(f"âœ… SessÃ£o de memÃ³ria inicializada: {session_id}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 2. RECUPERAR CONTEXTO CONVERSACIONAL ANTERIOR
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # FILTRAR CONTEXTO: manter apenas campos relevantes para anÃ¡lise
            memory_context = {}
            if context:
                filtered_context = {}
                if 'chunks' in context:
                    filtered_context['chunks'] = context['chunks']
                if 'csv_data' in context:
                    filtered_context['csv_data'] = context['csv_data']
                # âœ… PRESERVAR FLAGS DE VISUALIZAÃ‡ÃƒO
                if 'visualization_requested' in context:
                    filtered_context['visualization_requested'] = context['visualization_requested']
                if 'visualization_type' in context:
                    filtered_context['visualization_type'] = context['visualization_type']
                if 'fallback_sample_limit' in context:
                    filtered_context['fallback_sample_limit'] = context['fallback_sample_limit']
                if 'reconstructed_df' in context:
                    filtered_context['reconstructed_df'] = context['reconstructed_df']
                context = filtered_context
            # NÃƒO recuperar contexto de memÃ³ria para queries de intervalo
            interval_terms = ['intervalo', 'mÃ­nimo', 'mÃ¡ximo', 'range', 'amplitude']
            if any(term in query.lower() for term in interval_terms):
                memory_context = {}  # Ignorar histÃ³rico/memÃ³ria
            elif self.has_memory and self._current_session_id:
                memory_context = await self.recall_conversation_context()
                self.logger.debug(
                    f"âœ… Contexto de memÃ³ria recuperado: "
                    f"{len(memory_context.get('recent_messages', []))} mensagens anteriores"
                )
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 3. GERAR EMBEDDING DA QUERY
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug("Gerando embedding da query...")
            embedding_result = self.embedding_gen.generate_embedding(query)
            
            # Extrair lista de floats do resultado
            if isinstance(embedding_result, list):
                query_embedding = embedding_result
            elif hasattr(embedding_result, 'embedding'):
                query_embedding = embedding_result.embedding
            else:
                return self._build_error_response("Formato de embedding invÃ¡lido")
            
            if not query_embedding or len(query_embedding) == 0:
                return self._build_error_response("Falha ao gerar embedding da query")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 4. BUSCAR CHUNKS SIMILARES NOS DADOS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.logger.debug("Buscando chunks similares nos dados...")
            similar_chunks = self._search_similar_data(
                query_embedding=query_embedding,
                threshold=0.3,  # Threshold igual ao RAGAgent para capturar chunks analÃ­ticos
                limit=10
            )
            
            # SALVAR CONTEXTO DE DADOS NA TABELA agent_context
            if self.has_memory and self._current_session_id and similar_chunks:
                data_context = {
                    "dataset_info": {
                        "total_chunks": len(similar_chunks),
                        "source_types": list(set(c.get('source_type', 'unknown') for c in similar_chunks)),
                        "embedding_provider": "sentence-transformer",
                        "last_query": query[:100],
                        "query_timestamp": datetime.now().isoformat()
                    },
                    "performance_metrics": {
                        "embedding_generation_time": "N/A",  # Poderia ser medido
                        "search_time": "N/A",  # Poderia ser medido
                        "chunks_found": len(similar_chunks)
                    }
                }
                
                try:
                    await self.remember_data_context(
                        data_info=data_context,
                        context_key="current_dataset_info"
                    )
                    self.logger.debug("âœ… Contexto de dados salvo na tabela agent_context")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Falha ao salvar contexto de dados: {e}")
            
            # Fallback inteligente: se chunks nÃ£o sÃ£o suficientes OU pergunta exige anÃ¡lise global
            if not similar_chunks or self._should_use_global_csv(query, similar_chunks):
                # Tentar extrair caminho do CSV dos chunks ou do contexto
                csv_path = None
                if context and 'csv_path' in context:
                    csv_path = context['csv_path']
                elif similar_chunks:
                    for chunk in similar_chunks:
                        if 'csv_path' in chunk:
                            csv_path = chunk['csv_path']
                            break
                # Se nÃ£o encontrar, buscar na pasta processados
                if not csv_path:
                    try:
                        from src.settings import EDA_DATA_DIR_PROCESSADO
                        import os
                        import pandas as pd
                        csv_files = list(EDA_DATA_DIR_PROCESSADO.glob('*.csv'))
                        if csv_files:
                            csv_path = str(max(csv_files, key=lambda p: p.stat().st_mtime))
                    except Exception as e:
                        self.logger.warning(f"NÃ£o foi possÃ­vel localizar CSV para fallback: {e}")
                if csv_path:
                    self.logger.info(f"âš¡ Fallback: anÃ¡lise global do CSV ({csv_path}) para pergunta '{query[:60]}...'")
                    try:
                        resposta_csv = self._analisar_completo_csv(csv_path, query)
                        return self._build_response(
                            resposta_csv,
                            metadata={
                                "method": "global_csv_fallback",
                                "csv_path": csv_path,
                                "chunks_found": len(similar_chunks)
                            }
                        )
                    except Exception as e:
                        self.logger.error(f"Erro no fallback global CSV: {e}")
                        return self._build_error_response(f"Erro ao processar CSV global: {e}")
                else:
                    self.logger.error("âŒ Fallback global: CSV nÃ£o encontrado.")
                    return self._build_error_response("CSV original nÃ£o encontrado para anÃ¡lise global.")
            
            self.logger.info(f"âœ… Encontrados {len(similar_chunks)} chunks relevantes")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ†• VERIFICAR SE VISUALIZAÃ‡ÃƒO FOI SOLICITADA (MESMO COM CHUNKS)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            viz_requested = bool(context and context.get('visualization_requested'))
            if viz_requested:
                self.logger.info("ğŸ“Š VisualizaÃ§Ã£o solicitada - gerando grÃ¡ficos...")
                try:
                    import pandas as pd
                    from pathlib import Path
                    
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE - ACESSO DIRETO AO CSV
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    # JUSTIFICATIVA:
                    # 1. Tabela embeddings contÃ©m chunks de anÃ¡lises estatÃ­sticas (Markdown)
                    # 2. Histogramas requerem dados tabulares completos (285k linhas Ã— 31 colunas)
                    # 3. Embeddar cada linha seria ineficiente: ~$50-100 de custo + overhead
                    # 4. PadrÃ£o de mercado: LangChain, LlamaIndex, OpenAI Code Interpreter
                    #    fazem leitura direta de CSV para anÃ¡lises quantitativas
                    # 
                    # IMPLEMENTAÃ‡ÃƒO FUTURA:
                    # - TODO: Adicionar chunks raw_data na tabela embeddings durante ingestÃ£o
                    # - TODO: Implementar reconstituiÃ§Ã£o de DataFrame a partir de embeddings
                    # 
                    # AUDITORIA:
                    # - Log completo de acesso registrado
                    # - Metadados incluÃ­dos na resposta
                    # - Acesso read-only sem modificaÃ§Ã£o de dados
                    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                    
                    from src.settings import EDA_DATA_DIR_PROCESSADO
                    # Buscar CSV mais recente em data/processado/
                    csv_files = list(EDA_DATA_DIR_PROCESSADO.glob("*.csv"))
                    if not csv_files:
                        self.logger.error("âŒ Nenhum arquivo CSV encontrado em data/processado/")
                        self.logger.info("âš ï¸ Continuando com resposta textual sem visualizaÃ§Ãµes")
                    else:
                        # Pegar o arquivo mais recente (Ãºltimo modificado)
                        csv_path = max(csv_files, key=lambda p: p.stat().st_mtime)
                        csv_size_mb = csv_path.stat().st_size / 1_000_000
                        self.logger.warning(
                            "âš ï¸ EXCEÃ‡ÃƒO DE CONFORMIDADE: Acesso direto ao CSV para visualizaÃ§Ã£o",
                            extra={
                                "event_type": "direct_csv_access",
                                "user_query": query[:100],
                                "csv_path": str(csv_path),
                                "csv_size_mb": round(csv_size_mb, 2),
                                "access_reason": "histogram_generation",
                                "session_id": self._current_session_id,
                                "agent_name": self.name,
                                "timestamp": datetime.now().isoformat(),
                                "conformidade_status": "exception_approved",
                                "alternative_implementation": "future_raw_data_embeddings",
                                "cost_saved_estimate_usd": 50.0
                            }
                        )
                        viz_df = pd.read_csv(csv_path)
                        self.logger.info(
                            f"âœ… CSV carregado para visualizaÃ§Ã£o: {viz_df.shape[0]:,} linhas Ã— {viz_df.shape[1]} colunas | "
                            f"Tamanho: {csv_size_mb:.2f} MB"
                        )
                        # Delegar para agente de visualizaÃ§Ã£o
                        # Removido: agente obsoleto csv_analysis_agent.py
                        vis_context = context.copy() if context else {}
                        vis_context['reconstructed_df'] = viz_df
                        vis_result = self._handle_visualization_query(query, vis_context)
                        if vis_result.get('metadata', {}).get('visualization_success'):
                            # Combinar resposta de visualizaÃ§Ã£o com anÃ¡lise textual dos chunks
                            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
                            context_str = "\n\n".join(context_texts[:5])
                            text_response = await self._generate_llm_response_langchain(
                                query=query,
                                context_data=context_str,
                                memory_context=memory_context,
                                chunks_metadata=similar_chunks
                            )
                            # Combinar resposta textual com informaÃ§Ã£o sobre grÃ¡ficos
                            graficos_info = vis_result.get('metadata', {}).get('graficos_gerados', [])
                            if graficos_info:
                                graficos_msg = f"\n\nğŸ“Š **VisualizaÃ§Ãµes Geradas:**\n"
                                for gf in graficos_info:
                                    graficos_msg += f"â€¢ {gf}\n"
                                combined_response = text_response + graficos_msg
                            else:
                                combined_response = text_response
                            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
                            # Salvar interaÃ§Ã£o com metadados de conformidade
                            if self.has_memory:
                                await self.remember_interaction(
                                    query=query,
                                    response=combined_response,
                                    processing_time_ms=processing_time_ms,
                                    confidence=1.0,
                                    model_used="rag_v2_with_visualizations",
                                    metadata={
                                        "chunks_found": len(similar_chunks),
                                        "visualization_success": True,
                                        "graficos_gerados": len(graficos_info),
                                        "conformidade_exception": {
                                            "type": "direct_csv_access",
                                            "reason": "visualization_requires_raw_data",
                                            "csv_path": str(csv_path),
                                            "csv_size_mb": round(csv_size_mb, 2),
                                            "access_timestamp": datetime.now().isoformat(),
                                            "approved": True,
                                            "alternative_future": "raw_data_embeddings_implementation",
                                            "industry_standard": "LangChain/LlamaIndex/OpenAI_pattern",
                                            "cost_saved_usd": 50.0,
                                            "read_only": True
                                        }
                                    }
                                )
                            return self._build_response(
                                combined_response,
                                metadata={
                                    **vis_result.get('metadata', {}),
                                    "chunks_found": len(similar_chunks),
                                    "method": "rag_vectorial_v2_with_viz",
                                    "processing_time_ms": processing_time_ms,
                                    "conformidade_exception": {
                                        "type": "direct_csv_access",
                                        "reason": "visualization_requires_raw_data",
                                        "csv_path": str(csv_path),
                                        "csv_size_mb": round(csv_size_mb, 2),
                                        "approved": True,
                                        "industry_standard": True,
                                        "read_only": True,
                                        "documentation": "See comments in rag_data_agent.py lines 318-335"
                                    }
                                }
                            )
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao gerar visualizaÃ§Ãµes: {e}", exc_info=True)
                    # Continuar com resposta textual normal se visualizaÃ§Ã£o falhar
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 5. GERAR RESPOSTA COM LANGCHAIN + CONTEXTO HISTÃ“RICO
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            context_texts = [chunk['chunk_text'] for chunk in similar_chunks]
            context_str = "\n\n".join(context_texts[:5])  # Top 5 mais relevantes
            
            self.logger.debug("Usando LangChain LLM para gerar resposta...")
            response_text = await self._generate_llm_response_langchain(
                query=query,
                context_data=context_str,
                memory_context=memory_context,
                chunks_metadata=similar_chunks
            )
            
            processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)
            avg_similarity = sum(c['similarity'] for c in similar_chunks) / len(similar_chunks)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 6. SALVAR INTERAÃ‡ÃƒO NA MEMÃ“RIA PERSISTENTE
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if self.has_memory:
                await self.remember_interaction(
                    query=query,
                    response=response_text,
                    processing_time_ms=processing_time_ms,
                    confidence=avg_similarity,
                    model_used="langchain_gemini" if self.llm else "fallback",
                    metadata={
                        "chunks_found": len(similar_chunks),
                        "chunks_used": min(5, len(similar_chunks)),
                        "avg_similarity": avg_similarity,
                        "top_similarity": similar_chunks[0]['similarity'],
                        "has_history": len(memory_context.get('recent_conversations', [])) > 0
                    }
                )
                self.logger.debug("âœ… InteraÃ§Ã£o salva na memÃ³ria persistente")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # 7. RETORNAR RESPOSTA COM METADADOS
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            return self._build_response(
                response_text,
                metadata={
                    "chunks_found": len(similar_chunks),
                    "chunks_used": min(5, len(similar_chunks)),
                    "avg_similarity": avg_similarity,
                    "method": "rag_vectorial_v2",
                    "top_similarity": similar_chunks[0]['similarity'] if similar_chunks else 0,
                    "processing_time_ms": processing_time_ms,
                    "has_memory": self.has_memory,
                    "session_id": self._current_session_id,
                    "previous_interactions": len(memory_context.get('recent_conversations', []))
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao processar query: {str(e)}", exc_info=True)
            return self._build_error_response(f"Erro no processamento: {str(e)}")
    
    def _search_similar_data(
        self,
        query_embedding: List[float],
        threshold: float = 0.5,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Busca chunks similares nos dados usando match_embeddings RPC.
        
        Args:
            query_embedding: Embedding da query
            threshold: Threshold de similaridade (0.0 - 1.0)
            limit: NÃºmero mÃ¡ximo de resultados
            
        Returns:
            Lista de chunks similares com metadata
        """
        try:
            # Chamar funÃ§Ã£o RPC match_embeddings
            response = supabase.rpc(
                'match_embeddings',
                {
                    'query_embedding': query_embedding,
                    'similarity_threshold': threshold,
                    'match_count': limit
                }
            ).execute()
            
            if not response.data:
                self.logger.warning("Nenhum chunk similar encontrado")
                return []
            
            self.logger.debug(f"Encontrados {len(response.data)} chunks similares")
            # Parsing defensivo dos embeddings
            from src.embeddings.vector_store import parse_embedding_from_api, VECTOR_DIMENSIONS
            parsed_chunks = []
            for chunk in response.data:
                embedding_raw = chunk.get('embedding')
                try:
                    chunk['embedding'] = parse_embedding_from_api(embedding_raw, VECTOR_DIMENSIONS)
                except Exception as e:
                    self.logger.warning(f"Falha ao parsear embedding do chunk: {e}")
                    chunk['embedding'] = None
                parsed_chunks.append(chunk)
            return parsed_chunks
            
        except Exception as e:
            self.logger.error(f"Erro na busca vetorial: {str(e)}")
            return []
    
    async def _generate_llm_response_langchain(
        self,
        query: str,
        context_data: str,
        memory_context: dict,
        chunks_metadata: list
    ) -> str:
        try:
            # Preparar contexto histÃ³rico da conversa
            history_context = ""
            if memory_context.get('recent_messages') and len(memory_context['recent_messages']) > 0:
                history_context = "\n\n**Contexto da Conversa Anterior:**\n"
                for msg in memory_context['recent_messages'][-6:]:  # Ãšltimas 6 mensagens (3 pares user/assistant)
                    msg_type = msg.get('type', 'unknown')
                    content = msg.get('content', '')[:200]  # Limitar a 200 chars
                    if msg_type == 'user':
                        history_context += f"- UsuÃ¡rio perguntou: {content}\n"
                    elif msg_type == 'assistant':
                        history_context += f"- Assistente respondeu: {content}\n"
                history_context += "\n"

            # Preparar prompt DINÃ‚MICO baseado no tipo de query
            query_lower = query.lower()
            
            # Detectar tipo de query para customizar prompt
            # TIPO 1: Perguntas sobre o HISTÃ“RICO/CONTEXTO da conversa
            if any(term in query_lower for term in ['pergunta anterior', 'perguntei antes', 'falamos sobre', 'conversamos sobre', 'vocÃª disse', 'previous question', 'asked before']):
                # Query sobre HISTÃ“RICO - nÃ£o precisa de chunks, apenas memÃ³ria
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© responder sobre o HISTÃ“RICO da conversa. "
                    "Use o contexto da conversa anterior fornecido para responder. "
                    "Seja claro e objetivo, referenciando exatamente o que foi discutido."
                )
                user_prompt = (
                    f"{history_context}"  # PRINCIPAL fonte de informaÃ§Ã£o
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Consulte o histÃ³rico da conversa acima\n"
                    "- Responda referenciando exatamente o que foi perguntado/respondido anteriormente\n"
                    "- Se nÃ£o houver histÃ³rico suficiente, informe claramente\n"
                    "- Finalize com: 'Posso esclarecer mais alguma coisa sobre nossa conversa?'\n\n"
                    "**Resposta:**"
                )
            # TIPO 2: Perguntas sobre VARIABILIDADE
            elif any(term in query_lower for term in ['variabilidade', 'desvio padrÃ£o', 'variÃ¢ncia', 'variance', 'std', 'standard deviation']):
                # Query sobre VARIABILIDADE
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© responder sobre a VARIABILIDADE dos dados (desvio padrÃ£o, variÃ¢ncia, coeficiente de variaÃ§Ã£o). "
                    "Use os chunks analÃ­ticos fornecidos E o histÃ³rico da conversa para contextualizar a resposta. "
                    "Responda de forma clara, humanizada e estruturada."
                )
                user_prompt = (
                    f"{history_context}"  # INCLUIR histÃ³rico
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    f"**CHUNKS ANALÃTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Se houver histÃ³rico relevante, mencione brevemente o contexto\n"
                    "- Apresente mÃ©tricas de variabilidade: desvio padrÃ£o e variÃ¢ncia para as principais variÃ¡veis\n"
                    "- Agrupe V1 a V28 com estatÃ­sticas agregadas\n"
                    "- Destaque variÃ¡veis com alta vs baixa variabilidade\n"
                    "- Use formato R$ para valores monetÃ¡rios\n"
                    "- Finalize com: 'Se precisar de mais detalhes, Ã© sÃ³ perguntar!'\n\n"
                    "**Resposta:**"
                )
            # TIPO 3: Perguntas sobre INTERVALOS
            elif any(term in query_lower for term in ['intervalo', 'mÃ­nimo', 'mÃ¡ximo', 'range', 'amplitude']):
                # Query sobre INTERVALOS
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© responder EXCLUSIVAMENTE sobre o INTERVALO (mÃ­nimo e mÃ¡ximo) de cada variÃ¡vel presente nos chunks analÃ­ticos do CSV carregado. "
                    "Ignore completamente qualquer contexto extra, histÃ³rico, memÃ³ria ou dados residuais que nÃ£o estejam nos chunks analÃ­ticos. "
                    "NÃƒO inclua estatÃ­sticas, outliers, grÃ¡ficos ou qualquer dado nÃ£o solicitado. Responda de forma clara, objetiva e apenas com o solicitado. "
                    "Para perguntas comuns (saudaÃ§Ãµes, hora, etc.), responda de forma simples e natural, sem anÃ¡lise. Se a pergunta for sobre outro tipo de anÃ¡lise, siga as instruÃ§Ãµes especÃ­ficas do usuÃ¡rio."
                )
                user_prompt = (
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    f"**CHUNKS ANALÃTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Leia e interprete SOMENTE os chunks fornecidos\n"
                    "- Extraia SOMENTE os valores mÃ­nimo e mÃ¡ximo de cada variÃ¡vel\n"
                    "- NÃƒO inclua anÃ¡lise de outliers, desvio padrÃ£o, variÃ¢ncia ou estatÃ­sticas extras\n"
                    "- Formate a resposta em tabela Markdown\n"
                    "- Se nÃ£o houver informaÃ§Ã£o suficiente, informe claramente\n\n"
                    "**Resposta:**"
                )
            # TIPO 4: Perguntas sobre TIPOS DE DADOS
            elif any(term in query_lower for term in ['tipos', 'tipo de dado', 'numÃ©rico', 'categÃ³rico', 'categÃ³rica', 'categorical', 'numerical']):
                # Query sobre TIPOS DE DADOS
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Sua tarefa Ã© identificar e classificar os TIPOS DE DADOS (numÃ©ricos vs categÃ³ricos) presentes no dataset. "
                    "Use APENAS os chunks analÃ­ticos fornecidos - nÃ£o invente ou infira informaÃ§Ãµes. "
                    "Responda de forma clara, humanizada e estruturada."
                )
                user_prompt = (
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    f"**CHUNKS ANALÃTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Adicione mensagem amigÃ¡vel: 'OlÃ¡! Aqui estÃ¡ uma anÃ¡lise dos tipos de variÃ¡veis presentes no seu conjunto de dados:'\n"
                    "- Divida a resposta em 2 seÃ§Ãµes: 'VariÃ¡veis NumÃ©ricas' e 'VariÃ¡veis CategÃ³ricas'\n"
                    "- Para variÃ¡veis numÃ©ricas: agrupe V1 a V28 como 'V1 a V28: VariÃ¡veis numÃ©ricas agrupadas, todas apresentam alta variabilidade.'\n"
                    "- Liste tambÃ©m: Time, Amount\n"
                    "- Para variÃ¡veis categÃ³ricas: descreva Class como 'Class: VariÃ¡vel categÃ³rica com valores possÃ­veis 0 e 1. Utilizada para indicar fraude ou nÃ£o fraude.'\n"
                    "- NÃƒO mencione frequÃªncia de valores para Class\n"
                    "- Adicione estatÃ­sticas apenas para Amount (MÃ©dia, Desvio padrÃ£o, MÃ­nimo, MÃ¡ximo) com formato R$ XX.XX\n"
                    "- Finalize com: 'Se precisar de mais detalhes ou quiser analisar outra variÃ¡vel, Ã© sÃ³ perguntar!'\n\n"
                    "**Resposta:**"
                )
            # TIPO 5: Perguntas sobre FREQUÃŠNCIA (valores mais/menos frequentes)
            elif any(term in query_lower for term in ['frequente', 'frequentes', 'frequÃªncia', 'comum', 'raro', 'raros', 'moda', 'contagem', 'value_counts', 'mais ocorre', 'menos ocorre']):
                # Query sobre FREQUÃŠNCIA
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado em anÃ¡lise de frequÃªncia. "
                    "Sua tarefa Ã© identificar e reportar QUANTAS VEZES cada valor aparece no dataset. "
                    "Use APENAS os dados fornecidos nos chunks analÃ­ticos. NÃƒO invente nÃºmeros. "
                    "ATENÃ‡ÃƒO: Na tabela 'Colunas CategÃ³ricas', a coluna 'FrequÃªncia' contÃ©m o NÃšMERO DE OCORRÃŠNCIAS do valor mais frequente. "
                    "Exemplo: Se vir '| Class | 0 | 284315 |', significa que o valor '0' aparece 284.315 vezes no dataset."
                )
                user_prompt = (
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    f"**CHUNKS ANALÃTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRUÃ‡Ã•ES CRÃTICAS DE INTERPRETAÃ‡ÃƒO:**\n"
                    "1. **Leia a tabela 'Colunas CategÃ³ricas' corretamente**:\n"
                    "   - Formato: | Coluna | Valor Mais Frequente | FrequÃªncia | Valores Ãšnicos | Valores Nulos |\n"
                    "   - A coluna 'FrequÃªncia' Ã© o NÃšMERO DE VEZES que o 'Valor Mais Frequente' aparece\n"
                    "   - Exemplo: | Class | 0 | 284315 | â†’ O valor '0' aparece 284.315 vezes\n\n"
                    "2. **Calcule o valor menos frequente**:\n"
                    "   - Se houver 2 valores Ãºnicos (ex: Class com valores 0 e 1)\n"
                    "   - E o total de registros Ã© conhecido (ex: 284.807)\n"
                    "   - Menos frequente = Total de registros - FrequÃªncia do mais frequente\n"
                    "   - Exemplo: Class valor '1' = 284.807 - 284.315 = 492 vezes\n\n"
                    "3. **Para COLUNAS NUMÃ‰RICAS**:\n"
                    "   - Leia a tabela 'Colunas NumÃ©ricas' e encontre a coluna 'Moda'\n"
                    "   - A moda Ã© o valor numÃ©rico que mais se repete\n"
                    "   - Explique que para variÃ¡veis contÃ­nuas, muitos valores aparecem apenas 1 vez\n\n"
                    "**FORMATO DE RESPOSTA OBRIGATÃ“RIO:**\n"
                    "Inicie com: 'Pergunta feita: [pergunta]'\n\n"
                    "Adicione: 'Analisando a frequÃªncia dos valores no dataset:'\n\n"
                    "**ğŸ”¢ Colunas CategÃ³ricas:**\n\n"
                    "Para cada coluna categÃ³rica encontrada, mostre:\n"
                    "- **Coluna [Nome]**: \n"
                    "  * Valor mais frequente: [valor] (aparece [X] vezes)\n"
                    "  * Valor menos frequente: [valor] (aparece [Y] vezes) [SE PUDER CALCULAR]\n\n"
                    "**ğŸ“Š Colunas NumÃ©ricas:**\n\n"
                    "Para colunas numÃ©ricas, mostre a moda estatÃ­stica:\n"
                    "- **Coluna [Nome]**: Moda = [valor] (valor que mais se repete)\n\n"
                    "Adicione explicaÃ§Ã£o:\n"
                    "'Para variÃ¡veis numÃ©ricas contÃ­nuas (como V1-V28, Amount), a maioria dos valores aparece apenas 1 vez. "
                    "A moda estatÃ­stica indica o valor que mais se repete, mas para anÃ¡lise mais detalhada, considere perguntar sobre distribuiÃ§Ã£o ou intervalos.'\n\n"
                    "Finalize: 'Se precisar de mais detalhes ou anÃ¡lise de distribuiÃ§Ã£o, Ã© sÃ³ perguntar!'\n\n"
                    "**âš ï¸ REGRAS CRÃTICAS:**\n"
                    "- NÃƒO diga 'aparece 0 vezes' quando o nÃºmero na tabela Ã© POSITIVO\n"
                    "- NÃƒO confunda a coluna 'FrequÃªncia' com o valor da variÃ¡vel\n"
                    "- NÃƒO mostre mÃ­nimo/mÃ¡ximo quando a pergunta Ã© sobre frequÃªncia\n"
                    "- USE os nÃºmeros EXATOS da tabela de chunks fornecidos\n\n"
                    "**Resposta:**"
                )
            
            # TIPO 6: Perguntas sobre CLUSTERING/AGRUPAMENTOS
            elif any(term in query_lower for term in ['cluster', 'clusters', 'agrupamento', 'agrupamentos', 'grupos', 'kmeans', 'k-means', 'dbscan', 'hierÃ¡rquico', 'hierarquico', 'segmentaÃ§Ã£o', 'segmentacao']):
                # ğŸ”¬ EXECUÃ‡ÃƒO REAL DE CLUSTERING usando PythonDataAnalyzer
                self.logger.info("ğŸ”¬ Detectada pergunta sobre clustering - executando anÃ¡lise KMeans real...")
                
                try:
                    from src.tools.python_analyzer import python_analyzer
                    
                    # Executar clustering real nos dados
                    clustering_result = python_analyzer.calculate_clustering_analysis(n_clusters=3)
                    
                    if "error" in clustering_result:
                        # Se houve erro, informar ao usuÃ¡rio
                        error_msg = clustering_result.get("error", "Erro desconhecido")
                        suggestion = clustering_result.get("suggestion", "")
                        
                        return (
                            f"Pergunta feita: {query}\n\n"
                            f"âŒ **NÃ£o foi possÃ­vel realizar anÃ¡lise de clustering:**\n"
                            f"{error_msg}\n\n"
                            f"{suggestion}\n\n"
                            "Se precisar de mais detalhes, Ã© sÃ³ perguntar!"
                        )
                    
                    # Construir contexto enriquecido com resultados reais do clustering
                    clustering_context = clustering_result.get("interpretation", "")
                    cluster_distribution = clustering_result.get("cluster_distribution", {})
                    cluster_percentages = clustering_result.get("cluster_percentages", {})
                    numeric_vars = clustering_result.get("numeric_variables_used", [])
                    is_balanced = clustering_result.get("is_balanced", False)
                    
                    # Construir prompt com dados REAIS do clustering
                    system_prompt = (
                        "VocÃª Ã© um agente EDA especializado em anÃ¡lise de clustering. "
                        "Acabei de executar anÃ¡lise de clustering KMeans REAL nos dados. "
                        "Sua tarefa Ã© apresentar os resultados de forma clara e estruturada. "
                        "Use APENAS os resultados reais fornecidos. NÃƒO invente nÃºmeros."
                    )
                    
                    user_prompt = (
                        f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                        f"**RESULTADOS REAIS DO CLUSTERING EXECUTADO:**\n\n"
                        f"**Algoritmo:** KMeans com {clustering_result.get('n_clusters', 3)} clusters\n"
                        f"**Total de pontos analisados:** {clustering_result.get('total_points', 0):,}\n"
                        f"**VariÃ¡veis numÃ©ricas utilizadas:** {len(numeric_vars)} variÃ¡veis\n"
                        f"  - Exemplos: {', '.join(numeric_vars[:5])}{'...' if len(numeric_vars) > 5 else ''}\n\n"
                        f"**DistribuiÃ§Ã£o dos Clusters:**\n"
                    )
                    
                    # Adicionar distribuiÃ§Ã£o real dos clusters
                    for cluster_id in sorted(cluster_distribution.keys()):
                        count = cluster_distribution[cluster_id]
                        pct = cluster_percentages[cluster_id]
                        user_prompt += f"- Cluster {cluster_id}: {count:,} pontos ({pct:.1f}%)\n"
                    
                    user_prompt += f"\n**Balanceamento:** {'Clusters balanceados' if is_balanced else 'Clusters desbalanceados'}\n\n"
                    
                    user_prompt += (
                        f"**CHUNKS ANALÃTICOS DO CSV (contexto adicional):**\n{context_data}\n\n"
                        "**FORMATO DE RESPOSTA OBRIGATÃ“RIO:**\n"
                        "Inicie com: 'Pergunta feita: [pergunta]'\n\n"
                        "Adicione: 'Para responder se hÃ¡ agrupamentos (clusters) nos dados, executei anÃ¡lise de clustering KMeans:'\n\n"
                        "**ğŸ”¬ AnÃ¡lise de Clustering (KMeans):**\n\n"
                        "**VariÃ¡veis Utilizadas:**\n"
                        f"- {len(numeric_vars)} variÃ¡veis numÃ©ricas: {', '.join(numeric_vars[:8])}{'...' if len(numeric_vars) > 8 else ''}\n\n"
                        "**Resultado do Clustering (k=3):**\n"
                    )
                    
                    # Adicionar novamente para o LLM formatar
                    for cluster_id in sorted(cluster_distribution.keys()):
                        count = cluster_distribution[cluster_id]
                        pct = cluster_percentages[cluster_id]
                        user_prompt += f"- Cluster {cluster_id}: {count:,} pontos ({pct:.1f}%)\n"
                    
                    user_prompt += (
                        "\n**âœ… ConclusÃ£o:**\n"
                        f"- SIM, os dados apresentam {len(cluster_distribution)} agrupamentos distintos\n"
                        f"- Os clusters sÃ£o {'balanceados' if is_balanced else 'desbalanceados'}\n"
                        "- [Adicione insight interpretativo sobre o significado desses agrupamentos]\n\n"
                        "**ğŸ’¡ RecomendaÃ§Ãµes:**\n"
                        "- Para visualizar os clusters, pergunte: 'mostre grÃ¡fico de dispersÃ£o dos clusters'\n"
                        "- Para anÃ¡lise PCA 2D/3D, pergunte: 'aplique PCA nos dados'\n"
                        "- Para estatÃ­sticas por cluster, pergunte: 'qual a mÃ©dia de cada cluster?'\n\n"
                        "Finalize: 'Se desejar aprofundar na anÃ¡lise de clustering, Ã© sÃ³ perguntar!'\n\n"
                        "**Resposta:**"
                    )
                    
                except Exception as e:
                    self.logger.error(f"âŒ Erro ao executar clustering: {str(e)}", exc_info=True)
                    # Fallback: usar prompt genÃ©rico informando limitaÃ§Ã£o
                    system_prompt = (
                        "VocÃª Ã© um agente EDA especializado. "
                        "Houve um erro tÃ©cnico ao tentar executar anÃ¡lise de clustering real nos dados. "
                        "Explique ao usuÃ¡rio que a anÃ¡lise de clustering requer execuÃ§Ã£o de algoritmos especÃ­ficos."
                    )
                    user_prompt = (
                        f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                        f"**STATUS:** Erro tÃ©cnico ao executar KMeans: {str(e)}\n\n"
                        "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                        "- Informe que houve uma limitaÃ§Ã£o tÃ©cnica temporÃ¡ria\n"
                        "- Explique que clustering requer execuÃ§Ã£o de algoritmos (KMeans, DBSCAN, etc.)\n"
                        "- Sugira tentar novamente ou perguntar sobre outras anÃ¡lises\n\n"
                        "**Resposta:**"
                    )

            
            # TIPO 7: Query genÃ©rica - incluir histÃ³rico de conversa
            else:
                # Query genÃ©rica - incluir histÃ³rico de conversa
                system_prompt = (
                    "VocÃª Ã© um agente EDA especializado. Responda Ã  pergunta do usuÃ¡rio usando os chunks analÃ­ticos fornecidos E o histÃ³rico da conversa quando relevante. "
                    "Seja claro, objetivo, estruturado e humanizado. NÃ£o invente dados ou informaÃ§Ãµes."
                )
                user_prompt = (
                    f"{history_context}"  # INCLUIR histÃ³rico aqui!
                    f"**Pergunta do UsuÃ¡rio:**\n{query}\n\n"
                    f"**CHUNKS ANALÃTICOS DO CSV CARREGADO:**\n{context_data}\n\n"
                    "**INSTRUÃ‡Ã•ES DE RESPOSTA:**\n"
                    "- Use as informaÃ§Ãµes dos chunks fornecidos E o histÃ³rico da conversa quando relevante\n"
                    "- Se a pergunta se refere a algo mencionado anteriormente, considere o contexto\n"
                    "- Inicie com: 'Pergunta feita: [pergunta]'\n"
                    "- Responda de forma clara, humanizada e estruturada\n"
                    "- Se nÃ£o houver informaÃ§Ã£o suficiente, informe claramente\n"
                    "- Finalize com: 'Se precisar de mais detalhes, Ã© sÃ³ perguntar!'\n\n"
                    "**Resposta:**"
                )
            
            # Usar LangChain LLM se disponÃ­vel
            if self.llm and LANGCHAIN_AVAILABLE:
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]
                
                response = await asyncio.to_thread(self.llm.invoke, messages)
                return response.content
            
            # Fallback: usar LLM Manager customizado
            else:
                from src.llm.manager import LLMManager, LLMConfig
                llm_manager = LLMManager()
                
                # Construir prompt Ãºnico (LLMManager.chat espera string, nÃ£o messages)
                full_prompt = f"{system_prompt}\n\n{user_prompt}"
                llm_response = llm_manager.chat(
                    full_prompt,
                    config=LLMConfig(
                        temperature=0.3,
                        max_tokens=2000
                    )
                )
                
                # Verificar se houve erro (LLMResponse tem atributo .error)
                if not llm_response.success or llm_response.error:
                    return self._format_raw_data_response(query, chunks_metadata)
                
                # Extrair conteÃºdo (LLMResponse tem atributo .content)
                response_text = llm_response.content
                
                if not response_text:
                    return self._format_raw_data_response(query, chunks_metadata)
                
                return response_text
            
        except Exception as e:
            self.logger.error(f"Erro ao gerar resposta LLM: {str(e)}", exc_info=True)
            return self._format_raw_data_response(query, chunks_metadata)
    
    def _format_raw_data_response(
        self,
        query: str,
        chunks_metadata: List[Dict]
        ) -> str:
        """
        Fallback: usa agente de sÃ­ntese para consolidar dados se LLM falhar.
        """
        # Extrair apenas o texto dos chunks
        chunks = [chunk.get('chunk_text', '') for chunk in chunks_metadata]
        
        # Chamar agente de sÃ­ntese para consolidar
        from src.agent.rag_synthesis_agent import synthesize_response
        try:
            return synthesize_response(chunks, query, use_llm=False)
        except Exception as e:
            self.logger.error(f"Erro no agente de sÃ­ntese: {e}")
            # Fallback extremo: resposta estruturada mÃ­nima
            return f"""## Resposta para: {query}

**Status:** âš ï¸ Erro na sÃ­ntese

NÃ£o foi possÃ­vel processar completamente a consulta devido a um erro tÃ©cnico.
Por favor, reformule sua pergunta ou entre em contato com o suporte.

_Erro: {str(e)}_"""
    
    def _build_error_response(self, error_msg: str) -> Dict[str, Any]:
        """ConstrÃ³i resposta de erro padronizada."""
        return self._build_response(
            f"âŒ {error_msg}",
            metadata={"error": True, "method": "rag_vectorial_v2"}
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO SÃNCRONO WRAPPER (para compatibilidade retroativa)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def process_sync(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Wrapper sÃ­ncrono para compatibilidade com cÃ³digo legado.
        
        âš ï¸ DEPRECATED: Use process() async quando possÃ­vel.
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.process(query, context))
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MÃ‰TODO DE CARREGAMENTO CSV (mantido da versÃ£o anterior)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    async def load_csv_to_embeddings(
        self,
        csv_path: str,
        chunk_size: int = 1000,
        overlap: int = 100
    ) -> Dict[str, Any]:
        """
        Carrega CSV para a tabela embeddings.
        
        Args:
            csv_path: Caminho do arquivo CSV
            chunk_size: Tamanho dos chunks
            overlap: Overlap entre chunks
            
        Returns:
            Status do carregamento
        """
        try:
            self.logger.info(f"ğŸ“‚ Carregando CSV: {csv_path}")
            
            import pandas as pd
            from src.embeddings.chunker import CSVChunker
            
            # Ler CSV
            df = pd.read_csv(csv_path)
            self.logger.info(f"âœ… CSV lido: {len(df)} linhas, {len(df.columns)} colunas")
            
            # Criar chunks
            chunker = CSVChunker(chunk_size=chunk_size, overlap=overlap)
            chunks = chunker.chunk_dataframe(df)
            self.logger.info(f"âœ… Criados {len(chunks)} chunks")
            
            # Gerar embeddings e salvar
            inserted_count = 0
            for i, chunk in enumerate(chunks):
                try:
                    # Gerar embedding
                    embedding = self.embedding_gen.generate_embedding(chunk['text'])
                    
                    # Salvar na tabela embeddings
                    insert_data = {
                        'chunk_text': chunk['text'],
                        'embedding': embedding,
                        'metadata': {
                            'source': csv_path,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'created_at': datetime.now().isoformat()
                        }
                    }
                    
                    result = supabase.table('embeddings').insert(insert_data).execute()
                    
                    if result.data:
                        inserted_count += 1
                        if (i + 1) % 10 == 0:
                            self.logger.info(f"Progresso: {i+1}/{len(chunks)} chunks inseridos")
                
                except Exception as chunk_error:
                    self.logger.warning(f"Erro no chunk {i}: {chunk_error}")
                    continue
            
            self.logger.info(f"âœ… Carregamento concluÃ­do: {inserted_count}/{len(chunks)} chunks inseridos")
            
            return self._build_response(
                f"âœ… CSV carregado com sucesso: {inserted_count} chunks inseridos na base vetorial",
                metadata={
                    'csv_path': csv_path,
                    'total_rows': len(df),
                    'total_columns': len(df.columns),
                    'chunks_created': len(chunks),
                    'chunks_inserted': inserted_count
                }
            )
            
        except Exception as e:
            self.logger.error(f"âŒ Erro ao carregar CSV: {str(e)}")
            return self._build_error_response(f"Falha ao carregar CSV: {str(e)}")
