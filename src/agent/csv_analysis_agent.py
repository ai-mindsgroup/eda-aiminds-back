"""
‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  DEPRECATION WARNING  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è

ESTE ARQUIVO EST√Å OBSOLETO E SER√Å REMOVIDO EM VERS√ïES FUTURAS.

Use src/agent/rag_data_agent.py ao inv√©s deste arquivo.

MOTIVO DA DEPRECA√á√ÉO:
- rag_data_agent.py implementa busca vetorial PURA sem keywords hardcoded
- csv_analysis_agent.py viola princ√≠pios RAG com detec√ß√£o por palavras-chave
- Sistema deve ser 100% gen√©rico e agn√≥stico ao dataset

IMPACTO:
- Este arquivo ainda funciona mas N√ÉO deve ser usado em novos desenvolvimentos
- Testes que usam EmbeddingsAnalysisAgent devem migrar para RAGDataAgent
- A remo√ß√£o definitiva est√° planejada para pr√≥xima vers√£o major

DATA DE DEPRECA√á√ÉO: 05/10/2025
‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è  ‚ö†Ô∏è
"""
from __future__ import annotations

"""Agente especializado em an√°lise de dados via tabela embeddings.

Este agente combina:
- Consulta exclusiva √† tabela embeddings do Supabase
- An√°lise inteligente de dados estruturados armazenados como embeddings
- LLM para interpreta√ß√£o e insights baseados em embeddings
- Gera√ß√£o de an√°lises sem acesso direto a arquivos CSV

NOTA CR√çTICA: Este agente N√ÉO acessa arquivos CSV diretamente.
Todos os dados v√™m da tabela embeddings do Supabase.
"""
import json
import pandas as pd
import numpy as np
from typing import Any, Dict, List, Optional, Union
from datetime import datetime

try:
    from src.vectorstore.supabase_client import supabase
    SUPABASE_AVAILABLE = True
except (ImportError, RuntimeError) as e:
    SUPABASE_AVAILABLE = False
    supabase = None
    print(f"‚ö†Ô∏è Supabase n√£o dispon√≠vel: {str(e)[:100]}...")

from src.agent.base_agent import BaseAgent, AgentError
# query_classifier removido - arquivo obsoleto deletado
# ‚ö†Ô∏è NOTA: Este arquivo est√° DEPRECATED e cont√©m c√≥digo quebrado (refer√™ncias a QueryType)
# Mantido apenas para compatibilidade tempor√°ria. Use RAGDataAgent ao inv√©s.


class EmbeddingsAnalysisAgent(BaseAgent):
    """Agente para an√°lise inteligente de dados via embeddings.
    
    CONFORMIDADE: Este agente acessa APENAS a tabela embeddings do Supabase.
    Jamais l√™ arquivos CSV diretamente para responder consultas.
    """
    
    def __init__(self):
        super().__init__(
            name="embeddings_analyzer",
            description="Especialista em an√°lise de dados via tabela embeddings do Supabase",
            enable_memory=True  # Habilita sistema de mem√≥ria
        )
        self.current_embeddings: List[Dict[str, Any]] = []
        self.dataset_metadata: Dict[str, Any] = {}
        
        # Cache de an√°lises em mem√≥ria local (otimiza√ß√£o)
        self._analysis_cache: Dict[str, Any] = {}
        self._patterns_cache: Dict[str, Any] = {}
        
        # REMOVED: RAGQueryClassifier n√£o existe no codebase (corrigido em 2025-01-06)
        # self.query_classifier = RAGQueryClassifier()
        self.query_classifier = None  # Fallback seguro para permitir inicializa√ß√£o
        
        if not SUPABASE_AVAILABLE:
            raise AgentError(self.name, "Supabase n√£o dispon√≠vel - necess√°rio para acesso a embeddings")
        
        self.logger.info("Agente de an√°lise via embeddings inicializado com sistema de mem√≥ria")
    
    def _validate_embeddings_access_only(self) -> None:
        """Valida que o agente s√≥ acessa embeddings, nunca CSV diretamente."""
        if hasattr(self, 'current_df') or hasattr(self, 'current_file_path'):
            raise AgentError(
                self.name, 
                "VIOLA√á√ÉO CR√çTICA: Tentativa de acesso direto a CSV detectada"
            )
    
    def _classify_query_by_keywords(self, query: str):
        """Classifica√ß√£o b√°sica via keywords (fallback quando RAGQueryClassifier indispon√≠vel).
        
        Args:
            query: Pergunta do usu√°rio
            
        Returns:
            QueryType correspondente
        """
        from src.agent.orchestrator_agent import QueryType
        
        query_lower = query.lower()
        
        # Mapeamento de keywords para tipos
        keywords_map = {
            QueryType.VISUALIZATION: ['gr√°fico', 'grafico', 'histograma', 'distribui√ß√£o', 'plot', 'visualizar', 'mostrar'],
            QueryType.CORRELATION: ['correla√ß√£o', 'correlacao', 'rela√ß√£o', 'relacao', 'associa√ß√£o', 'associacao'],
            QueryType.VARIABILITY: ['variabilidade', 'varia√ß√£o', 'variacao', 'desvio', 'dispers√£o', 'dispersao'],
            QueryType.CENTRAL_TENDENCY: ['m√©dia', 'media', 'mediana', 'moda', 'central'],
            QueryType.DISTRIBUTION: ['distribui√ß√£o', 'distribuicao', 'frequ√™ncia', 'frequencia'],
            QueryType.OUTLIERS: ['outlier', 'discrepante', 'an√¥malo', 'anomalo', 'fora da curva'],
            QueryType.INTERVAL: ['intervalo', 'faixa', 'range', 'm√≠nimo', 'minimo', 'm√°ximo', 'maximo'],
            QueryType.COUNT: ['quantos', 'quantas', 'quantidade', 'contar', 'n√∫mero', 'numero'],
            QueryType.SUMMARY: ['resumo', 'vis√£o geral', 'visao geral', 'overview', 'sum√°rio', 'sumario'],
        }
        
        # Procurar palavras-chave
        for qtype, keywords in keywords_map.items():
            if any(kw in query_lower for kw in keywords):
                return qtype
        
        # Fallback para ANALYSIS se n√£o encontrar match espec√≠fico
        return QueryType.ANALYSIS
    
    def load_from_embeddings(self, 
                           dataset_filter: Optional[str] = None,
                           limit: int = 1000) -> Dict[str, Any]:
        """Carrega dados da tabela embeddings do Supabase para an√°lise.
        
        Args:
            dataset_filter: Filtro opcional por dataset espec√≠fico
            limit: Limite de embeddings para carregar
        
        Returns:
            Informa√ß√µes sobre os dados carregados dos embeddings
        """
        self._validate_embeddings_access_only()
        
        try:
            self.logger.info(f"Carregando dados da tabela embeddings (limite: {limit})")
            
            # Consultar tabela embeddings
            query = supabase.table('embeddings').select('chunk_text, metadata, created_at')
            
            if dataset_filter:
                query = query.eq('metadata->>source', dataset_filter)
            
            response = query.limit(limit).execute()
            
            if not response.data:
                return self._build_response(
                    "‚ùå Nenhum embedding encontrado na base de dados",
                    metadata={'embeddings_count': 0}
                )
            
            self.current_embeddings = response.data
            
            # Extrair metadados do dataset
            self.dataset_metadata = self._extract_dataset_metadata()
            
            # An√°lise inicial dos embeddings
            analysis = self._analyze_embeddings_data()
            
            return self._build_response(
                f"‚úÖ Dados carregados: {len(self.current_embeddings)} embeddings encontrados",
                metadata=analysis
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao carregar embeddings: {str(e)}")
            raise AgentError(self.name, f"Falha ao carregar dados dos embeddings: {str(e)}")
    
    def _extract_dataset_metadata(self) -> Dict[str, Any]:
        """Extrai metadados dos embeddings carregados."""
        if not self.current_embeddings:
            return {}
        
        sources = set()
        providers = set()
        chunk_types = set()
        dates = []
        
        for embedding in self.current_embeddings:
            metadata = embedding.get('metadata', {})
            sources.add(metadata.get('source', 'unknown'))
            providers.add(metadata.get('provider', 'unknown'))
            chunk_types.add(metadata.get('chunk_type', 'unknown'))
            
            if embedding.get('created_at'):
                dates.append(embedding['created_at'])
        
        return {
            'sources': list(sources),
            'providers': list(providers), 
            'chunk_types': list(chunk_types),
            'date_range': {
                'earliest': min(dates) if dates else None,
                'latest': max(dates) if dates else None
            },
            'total_embeddings': len(self.current_embeddings)
        }
    
    def _analyze_embeddings_data(self) -> Dict[str, Any]:
        """An√°lise dos dados dos embeddings carregados."""
        if not self.current_embeddings:
            return {}
        
        # Analisar conte√∫do dos chunks
        chunk_texts = [emb['chunk_text'] for emb in self.current_embeddings]
        
        # Tentar detectar estrutura de dados CSV nos chunks
        detected_columns = set()
        numeric_patterns = []
        
        for chunk_text in chunk_texts[:50]:  # Analisar amostra
            # Buscar padr√µes de colunas/campos
            if ',' in chunk_text or '|' in chunk_text or '\t' in chunk_text:
                # Poss√≠vel dados tabulares
                lines = chunk_text.split('\n')
                for line in lines[:3]:  # Primeiras linhas
                    if ',' in line:
                        parts = line.split(',')
                        for part in parts:
                            part = part.strip()
                            if part and len(part) < 50:  # Poss√≠vel nome de coluna
                                detected_columns.add(part)
        
        return {
            'embeddings_count': len(self.current_embeddings),
            'dataset_metadata': self.dataset_metadata,
            'detected_columns': list(detected_columns)[:20],  # Limite
            'content_analysis': {
                'avg_chunk_length': np.mean([len(text) for text in chunk_texts]),
                'total_content_length': sum(len(text) for text in chunk_texts)
            }
        }
    
    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta sobre dados via embeddings.
        
        Args:
            query: Pergunta ou comando sobre os dados
            context: Contexto adicional (pode incluir dataset_filter)
        
        Returns:
            Resposta com an√°lise baseada em embeddings
        """
        self._validate_embeddings_access_only()
        
        try:
            # Verificar se precisa carregar embeddings
            if not self.current_embeddings:
                dataset_filter = context.get('dataset_filter') if context else None
                load_result = self.load_from_embeddings(dataset_filter=dataset_filter)
                if 'error' in load_result.get('metadata', {}):
                    return load_result
            
            if not self.current_embeddings:
                return self._build_response(
                    "‚ùå Nenhum embedding carregado. Verifique se h√° dados na tabela embeddings.",
                    metadata={"error": True, "conformidade": "embeddings_only"}
                )
            
            # ===================================================================
            # CLASSIFICA√á√ÉO INTELIGENTE VIA RAG (substituiu keywords hardcoded)
            # ===================================================================
            self.logger.info(f"üîç Classificando query via RAG: {query[:60]}...")
            
            # Guard: query_classifier pode n√£o existir (bug RAGQueryClassifier corrigido em 2025-01-06)
            if self.query_classifier is None:
                # Fallback: classifica√ß√£o b√°sica via keywords
                from src.agent.orchestrator_agent import QueryType
                classification_type = self._classify_query_by_keywords(query)
                class FallbackClassification:
                    def __init__(self, qtype):
                        self.query_type = qtype
                        self.confidence = 0.7
                        self.metadata = {'method': 'keyword_fallback'}
                classification = FallbackClassification(classification_type)
                self.logger.warning("‚ö†Ô∏è  query_classifier indispon√≠vel, usando fallback por keywords")
            else:
                classification = self.query_classifier.classify_query(query)
            
            self.logger.info(
                f"üìä Query classificada como: {classification.query_type.value} "
                f"(confian√ßa: {classification.confidence:.2f})"
            )
            
            # Adicionar metadados de classifica√ß√£o ao contexto
            if context is None:
                context = {}
            context['classification'] = {
                'type': classification.query_type.value,
                'confidence': classification.confidence,
                'method': classification.metadata.get('method', 'unknown')
            }
            
            # Rotear para o handler apropriado baseado na classifica√ß√£o RAG
            handler_map = {
                QueryType.VARIABILITY: self._handle_variability_query_from_embeddings,
                QueryType.INTERVAL: self._handle_statistics_query_from_embeddings,
                QueryType.CENTRAL_TENDENCY: self._handle_central_tendency_query_from_embeddings,
                QueryType.CORRELATION: self._handle_correlation_query_from_embeddings,
                QueryType.DISTRIBUTION: self._handle_distribution_query_from_embeddings,
                QueryType.OUTLIERS: self._handle_outliers_query_from_embeddings,
                QueryType.VISUALIZATION: self._handle_visualization_query,
                QueryType.SUMMARY: self._handle_summary_query_from_embeddings,
                QueryType.ANALYSIS: self._handle_analysis_query_from_embeddings,
                QueryType.SEARCH: self._handle_search_query_from_embeddings,
                QueryType.COUNT: self._handle_count_query_from_embeddings,
                QueryType.GENERAL: self._handle_general_query_from_embeddings,
            }
            
            handler = handler_map.get(classification.query_type)
            
            if handler is None:
                self.logger.warning(f"Handler n√£o encontrado para tipo: {classification.query_type}")
                handler = self._handle_general_query_from_embeddings
            
            # Executar handler
            response = handler(query, context)
            
            # Aprender com a query processada (melhoria cont√≠nua)
            try:
                # Guard: query_classifier pode n√£o existir
                if self.query_classifier is not None:
                    self.query_classifier.learn_from_query(
                        query=query,
                        correct_type=classification.query_type,
                        response=response.get('response', ''),
                        metadata={'confidence': classification.confidence}
                    )
                else:
                    self.logger.debug("query_classifier indispon√≠vel, pulando learn_from_query")
            except Exception as learn_error:
                self.logger.warning(f"Falha ao registrar aprendizado: {learn_error}")
            
            return response
                
        except Exception as e:
            self.logger.error(f"Erro ao processar consulta via embeddings: {str(e)}")
            return self._build_response(
                f"Erro ao processar consulta: {str(e)}",
                metadata={"error": True, "conformidade": "embeddings_only"}
            )
    
    # ========================================================================
    # M√âTODOS DE PROCESSAMENTO COM MEM√ìRIA
    # ========================================================================
    
    async def process_with_memory(self, query: str, context: Optional[Dict[str, Any]] = None,
                                session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Processa consulta utilizando sistema de mem√≥ria para cache de an√°lises.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
            session_id: ID da sess√£o
            
        Returns:
            Resposta processada com cache otimizado
        """
        import time
        start_time = time.time()
        
        try:
            # 1. Inicializar sess√£o de mem√≥ria se necess√°rio
            if session_id and self.has_memory:
                if not self._current_session_id or self._current_session_id != session_id:
                    await self.init_memory_session(session_id)
            elif not self._current_session_id and self.has_memory:
                await self.init_memory_session()
            
            # 2. Verificar cache de an√°lises espec√≠ficas
            analysis_key = self._generate_analysis_cache_key(query, context)
            cached_result = await self.recall_cached_analysis(analysis_key)
            
            if cached_result:
                self.logger.info(f"üì¶ An√°lise recuperada do cache: {analysis_key}")
                cached_result['metadata']['from_cache'] = True
                cached_result['metadata']['cache_key'] = analysis_key
                return cached_result
            
            # 3. Recuperar padr√µes de consulta aprendidos
            learned_patterns = await self.recall_learned_patterns()
            if learned_patterns:
                self.logger.debug(f"üß† Aplicando {len(learned_patterns)} padr√µes aprendidos")
                context = context or {}
                context['learned_patterns'] = learned_patterns
            
            # 4. Processar consulta normalmente
            result = self.process(query, context)
            
            # 5. Calcular tempo de processamento
            processing_time_ms = int((time.time() - start_time) * 1000)
            result.setdefault('metadata', {})['processing_time_ms'] = processing_time_ms
            
            # 6. Salvar resultado no cache se significativo
            if self._should_cache_analysis(result, processing_time_ms):
                await self.remember_analysis_result(analysis_key, result, expiry_hours=12)
                self.logger.debug(f"üíæ An√°lise salva no cache: {analysis_key}")
            
            # 7. Aprender padr√µes da consulta
            await self.learn_query_pattern(query, result)
            
            # 8. Salvar intera√ß√£o na mem√≥ria
            if self.has_memory and self._current_session_id:
                await self.remember_interaction(
                    query=query,
                    response=result.get('content', str(result)),
                    processing_time_ms=processing_time_ms,
                    metadata=result.get('metadata', {})
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento com mem√≥ria: {e}")
            # Fallback para processamento sem mem√≥ria
            return self.process(query, context)
    
    # ========================================================================
    # M√âTODOS DE MEM√ìRIA ESPEC√çFICOS
    # ========================================================================
    
    def _generate_analysis_cache_key(self, query: str, context: Optional[Dict[str, Any]]) -> str:
        """Gera chave √∫nica para cache de an√°lise."""
        import hashlib
        
        # Normaliza query para cache
        normalized_query = query.lower().strip()
        
        # Adiciona contexto relevante
        context_str = ""
        if context:
            relevant_context = {
                'dataset_filter': context.get('dataset_filter'),
                'limit': context.get('limit'),
                'embeddings_count': len(self.current_embeddings)
            }
            context_str = str(sorted(relevant_context.items()))
        
        # Gera hash
        cache_input = f"{normalized_query}_{context_str}"
        return f"analysis_{hashlib.md5(cache_input.encode()).hexdigest()[:12]}"
    
    def _should_cache_analysis(self, result: Dict[str, Any], processing_time_ms: int) -> bool:
        """Determina se uma an√°lise deve ser cacheada."""
        # Cachear se:
        # 1. Processamento demorado (> 500ms)
        # 2. Resultado significativo (tem metadados de an√°lise)
        # 3. N√£o √© erro
        
        if result.get('metadata', {}).get('error', False):
            return False
        
        if processing_time_ms > 500:
            return True
        
        metadata = result.get('metadata', {})
        has_analysis = any(key in metadata for key in [
            'embeddings_count', 'detected_columns', 'content_analysis'
        ])
        
        return has_analysis
    
    async def learn_query_pattern(self, query: str, result: Dict[str, Any]) -> None:
        """Aprende padr√µes de consulta para otimiza√ß√£o futura."""
        if not self.has_memory or not self._current_session_id:
            return
        
        try:
            # Extrai caracter√≠sticas da consulta
            query_features = {
                'length': len(query),
                'words': len(query.split()),
                'type': self._classify_query_type(query),
                'embeddings_used': len(self.current_embeddings),
                'success': not result.get('metadata', {}).get('error', False)
            }
            
            # Salva padr√£o de aprendizado
            pattern_data = {
                'query_sample': query[:100],  # Trunca para privacidade
                'features': query_features,
                'result_type': result.get('metadata', {}).get('query_type', 'unknown'),
                'timestamp': datetime.now().isoformat()
            }
            
            pattern_key = f"pattern_{self._classify_query_type(query)}"
            context_key = f"learning_patterns_{pattern_key}"
            
            # Salva na mem√≥ria de contexto
            await self.remember_data_context(pattern_data, context_key)
            
            self.logger.debug(f"Padr√£o de consulta aprendido: {pattern_key}")
            
        except Exception as e:
            self.logger.debug(f"Erro ao aprender padr√£o: {e}")
    
    async def recall_learned_patterns(self) -> List[Dict[str, Any]]:
        """Recupera padr√µes de consulta aprendidos."""
        if not self.has_memory or not self._current_session_id:
            return []
        
        try:
            # Recupera contexto de aprendizado
            context = await self.recall_conversation_context(hours=168)  # 1 semana
            
            patterns = []
            for key, data in context.get('data_context', {}).items():
                if key.startswith('learning_patterns_'):
                    patterns.append(data)
            
            return patterns
            
        except Exception as e:
            self.logger.debug(f"Erro ao recuperar padr√µes: {e}")
            return []
    
    def _classify_query_type(self, query: str) -> str:
        """Classifica tipo de consulta para aprendizado."""
        query_lower = query.lower()
        
        if any(word in query_lower for word in ['resumo', 'describe', 'info']):
            return 'summary'
        elif any(word in query_lower for word in ['an√°lise', 'analyze', 'estat√≠stica']):
            return 'analysis'
        elif any(word in query_lower for word in ['busca', 'search', 'procura']):
            return 'search'
        elif any(word in query_lower for word in ['contagem', 'count', 'quantos']):
            return 'count'
        else:
            return 'general'
    
    def _handle_summary_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de resumo usando dados dos embeddings."""
        analysis = self._analyze_embeddings_data()
        
        summary = f"""üìä **Resumo dos Dados (via Embeddings)**
        
**Fonte:** Tabela embeddings do Supabase
**Total de Embeddings:** {analysis['embeddings_count']:,}
**Datasets Identificados:** {', '.join(self.dataset_metadata.get('sources', ['N/A']))}
**Tipos de Chunk:** {', '.join(self.dataset_metadata.get('chunk_types', ['N/A']))}

**Colunas Detectadas nos Dados:**
{', '.join(analysis.get('detected_columns', ['Nenhuma detectada']))}

**An√°lise de Conte√∫do:**
‚Ä¢ Tamanho m√©dio dos chunks: {analysis['content_analysis']['avg_chunk_length']:.1f} caracteres
‚Ä¢ Conte√∫do total analisado: {analysis['content_analysis']['total_content_length']:,} caracteres

‚ö†Ô∏è **Conformidade:** Dados obtidos exclusivamente da tabela embeddings
        """
        
        return self._build_response(summary, metadata={
            **analysis,
            'conformidade': 'embeddings_only',
            'query_type': 'summary'
        })
    
    def _handle_analysis_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de an√°lise usando embeddings.
        
        ‚ö†Ô∏è DEPRECATION: Este m√©todo est√° obsoleto. Use RAGDataAgent ao inv√©s.
        """
        if not self.current_embeddings:
            return self._build_response("‚ùå Nenhum embedding dispon√≠vel para an√°lise")
        
        # An√°lise gen√©rica baseada no conte√∫do dos chunks (SEM detec√ß√£o de fraude)
        chunk_texts = [emb['chunk_text'] for emb in self.current_embeddings]
        
        # Estat√≠sticas gen√©ricas sobre chunks
        total_chunks = len(chunk_texts)
        avg_length = sum(len(text) for text in chunk_texts) / total_chunks if total_chunks > 0 else 0
        
        response = f"""üîç **An√°lise de Dados (via Embeddings)**
        
**Estat√≠sticas Gerais:**
‚Ä¢ Total de chunks analisados: {total_chunks}
‚Ä¢ Tamanho m√©dio dos chunks: {avg_length:.0f} caracteres

‚ö†Ô∏è **AVISO:** Para an√°lises detalhadas, use RAGDataAgent com busca vetorial.
        """
        
        return self._build_response(response, metadata={
            'total_chunks': total_chunks,
            'avg_chunk_length': avg_length,
            'conformidade': 'embeddings_only',
            'deprecation_warning': 'Use RAGDataAgent instead'
        })
    
    def _handle_search_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de busca nos embeddings."""
        search_term = query.lower()
        
        # Buscar termo nos chunks
        matches = []
        for i, embedding in enumerate(self.current_embeddings):
            chunk_text = embedding['chunk_text'].lower()
            if any(term in chunk_text for term in search_term.split()):
                matches.append({
                    'index': i,
                    'chunk_preview': embedding['chunk_text'][:200] + '...',
                    'metadata': embedding.get('metadata', {})
                })
        
        if matches:
            response = f"üîç **Resultados da Busca (via Embeddings)**\n\n"
            response += f"Encontrados {len(matches)} chunks relevantes:\n\n"
            
            for i, match in enumerate(matches[:5]):  # Limite de 5 resultados
                response += f"**Resultado {i+1}:**\n{match['chunk_preview']}\n\n"
        else:
            response = f"‚ùå Nenhum resultado encontrado para: '{query}'"
        
        return self._build_response(response, metadata={
            'matches_count': len(matches),
            'query': query,
            'conformidade': 'embeddings_only'
        })
    
    def _handle_count_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas de contagem usando embeddings."""
        analysis = self._analyze_embeddings_data()
        
        response = f"""üìä **Contagens dos Dados (via Embeddings)**
        
‚Ä¢ Total de embeddings: {analysis['embeddings_count']:,}
‚Ä¢ Datasets identificados: {len(self.dataset_metadata.get('sources', []))}
‚Ä¢ Tipos de chunk: {len(self.dataset_metadata.get('chunk_types', []))}
‚Ä¢ Colunas detectadas: {len(analysis.get('detected_columns', []))}
        """
        
        return self._build_response(response, metadata={
            **analysis,
            'conformidade': 'embeddings_only'
        })
    
    def _handle_statistics_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre estat√≠sticas (min, max, intervalos) usando dados reais dos embeddings.
        
        Args:
            query: Pergunta do usu√°rio sobre estat√≠sticas
            context: Contexto adicional
            
        Returns:
            Resposta com estat√≠sticas calculadas a partir dos dados reais
        """
        try:
            self.logger.info("üìä Calculando estat√≠sticas reais dos dados via embeddings...")
            
            # Importar Python Analyzer para processar chunk_text
            try:
                from src.tools.python_analyzer import PythonDataAnalyzer
                analyzer = PythonDataAnalyzer()
            except ImportError as e:
                self.logger.error(f"Erro ao importar PythonDataAnalyzer: {e}")
                return self._build_response(
                    "‚ùå Erro: PythonDataAnalyzer n√£o dispon√≠vel para calcular estat√≠sticas",
                    metadata={"error": True}
                )
            
            # Obter DataFrame real dos chunks
            df = analyzer.get_data_from_embeddings(limit=None, parse_chunk_text=True)
            
            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel obter dados dos embeddings para calcular estat√≠sticas",
                    metadata={"error": True}
                )
            
            self.logger.info(f"‚úÖ DataFrame carregado: {len(df)} registros, {len(df.columns)} colunas")
            
            # Calcular intervalos (min/max) para TODAS as colunas num√©ricas
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if not numeric_cols:
                return self._build_response(
                    "‚ùå Nenhuma coluna num√©rica encontrada nos dados",
                    metadata={"error": True}
                )
            
            # Calcular estat√≠sticas de intervalo
            stats_data = []
            for col in numeric_cols:
                col_min = df[col].min()
                col_max = df[col].max()
                col_range = col_max - col_min
                stats_data.append({
                    'variavel': col,
                    'minimo': col_min,
                    'maximo': col_max,
                    'amplitude': col_range
                })
            
            # Formatar resposta
            response = f"""üìä **Intervalo de Cada Vari√°vel (M√≠nimo e M√°ximo)**

**Fonte:** Dados reais extra√≠dos da tabela embeddings (coluna chunk_text parseada)
**Total de registros analisados:** {len(df):,}
**Total de vari√°veis num√©ricas:** {len(numeric_cols)}

"""
            
            # Adicionar tabela formatada
            response += "| Vari√°vel | M√≠nimo | M√°ximo | Amplitude |\n"
            response += "|----------|--------|--------|----------|\n"
            
            # Mostrar TODAS as vari√°veis (removida limita√ß√£o de 15)
            for stat in stats_data:
                var_name = stat['variavel']
                var_min = stat['minimo']
                var_max = stat['maximo']
                var_range = stat['amplitude']
                
                # Formatar valores com precis√£o adequada
                if abs(var_min) < 1000 and abs(var_max) < 1000:
                    response += f"| {var_name} | {var_min:.6f} | {var_max:.6f} | {var_range:.6f} |\n"
                else:
                    response += f"| {var_name} | {var_min:.2f} | {var_max:.2f} | {var_range:.2f} |\n"
            
            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"
            response += f"‚úÖ **M√©todo:** Parsing de chunk_text + an√°lise com pandas\n"
            
            return self._build_response(response, metadata={
                'total_records': len(df),
                'total_numeric_columns': len(numeric_cols),
                'statistics': stats_data,
                'conformidade': 'embeddings_only',
                'query_type': 'statistics'
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao calcular estat√≠sticas: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao calcular estat√≠sticas dos dados: {str(e)}",
                metadata={"error": True, "conformidade": "embeddings_only"}
            )
    
    def _handle_variability_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre VARIABILIDADE/DISPERS√ÉO (desvio padr√£o, vari√¢ncia) usando dados REAIS dos embeddings.
        
        Este m√©todo √© GEN√âRICO e funciona com qualquer CSV carregado nos embeddings.
        
        Args:
            query: Pergunta do usu√°rio sobre variabilidade (ex: "Qual a variabilidade dos dados?")
            context: Contexto adicional
            
        Returns:
            Resposta com desvio padr√£o, vari√¢ncia e coeficiente de varia√ß√£o calculados
        """
        try:
            self.logger.info("üìä Calculando VARIABILIDADE (desvio padr√£o, vari√¢ncia) dos dados via embeddings...")
            
            # Importar Python Analyzer para processar chunk_text
            try:
                from src.tools.python_analyzer import PythonDataAnalyzer
                analyzer = PythonDataAnalyzer()
            except ImportError as e:
                self.logger.error(f"Erro ao importar PythonDataAnalyzer: {e}")
                return self._build_response(
                    "‚ùå Erro: PythonDataAnalyzer n√£o dispon√≠vel para calcular variabilidade",
                    metadata={"error": True}
                )
            
            # Obter DataFrame real dos chunks (GEN√âRICO - qualquer CSV)
            df = analyzer.get_data_from_embeddings(limit=None, parse_chunk_text=True)
            
            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel obter dados dos embeddings para calcular variabilidade",
                    metadata={"error": True}
                )
            
            self.logger.info(f"‚úÖ DataFrame carregado: {len(df)} registros, {len(df.columns)} colunas")
            
            # Calcular VARIABILIDADE para TODAS as colunas num√©ricas (GEN√âRICO)
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if not numeric_cols:
                return self._build_response(
                    "‚ùå Nenhuma coluna num√©rica encontrada nos dados",
                    metadata={"error": True}
                )
            
            # Calcular medidas de DISPERS√ÉO
            variability_data = []
            for col in numeric_cols:
                col_std = df[col].std()  # Desvio padr√£o
                col_var = df[col].var()  # Vari√¢ncia
                col_mean = df[col].mean()
                col_cv = (col_std / col_mean) * 100 if col_mean != 0 else 0  # Coeficiente de Varia√ß√£o
                
                variability_data.append({
                    'variavel': col,
                    'desvio_padrao': col_std,
                    'variancia': col_var,
                    'coeficiente_variacao': col_cv
                })
            
            # Formatar resposta
            response = f"""üìä **Variabilidade dos Dados (Desvio Padr√£o e Vari√¢ncia)**

**Fonte:** Dados reais extra√≠dos da tabela embeddings (coluna chunk_text parseada)
**Total de registros analisados:** {len(df):,}
**Total de vari√°veis num√©ricas:** {len(numeric_cols)}

"""
            
            # Adicionar tabela formatada
            response += "| Vari√°vel | Desvio Padr√£o | Vari√¢ncia | Coef. Varia√ß√£o (%) |\n"
            response += "|----------|---------------|-----------|--------------------| \n"
            
            # Mostrar TODAS as vari√°veis
            for stat in variability_data:
                var_name = stat['variavel']
                var_std = stat['desvio_padrao']
                var_var = stat['variancia']
                var_cv = stat['coeficiente_variacao']
                
                # Formatar valores com precis√£o adequada
                response += f"| {var_name} | {var_std:.6f} | {var_var:.6f} | {var_cv:.2f} |\n"
            
            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"
            response += f"‚úÖ **M√©todo:** Parsing de chunk_text + c√°lculo std() e var() com pandas\n"
            response += f"\n**Interpreta√ß√£o:**\n"
            response += f"- **Desvio Padr√£o:** Mede a dispers√£o dos dados em rela√ß√£o √† m√©dia\n"
            response += f"- **Vari√¢ncia:** Quadrado do desvio padr√£o (mesma medida, escala diferente)\n"
            response += f"- **Coef. Varia√ß√£o:** Percentual de dispers√£o relativa (√∫til para comparar vari√°veis)\n"
            
            return self._build_response(response, metadata={
                'total_records': len(df),
                'total_numeric_columns': len(numeric_cols),
                'variability_data': variability_data,
                'conformidade': 'embeddings_only',
                'query_type': 'variability'
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao calcular variabilidade: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao calcular variabilidade dos dados: {str(e)}",
                metadata={"error": True, "conformidade": "embeddings_only"}
            )
    
    def _handle_central_tendency_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre medidas de tend√™ncia central (m√©dia, mediana, moda) usando dados REAIS dos embeddings.
        
        Args:
            query: Pergunta do usu√°rio sobre medidas de tend√™ncia central
            context: Contexto adicional
            
        Returns:
            Resposta com medidas de tend√™ncia central calculadas a partir dos dados reais
        """
        try:
            self.logger.info("üìä Calculando medidas de tend√™ncia central dos dados via embeddings...")
            
            # Importar Python Analyzer para processar chunk_text
            try:
                from src.tools.python_analyzer import PythonDataAnalyzer
                analyzer = PythonDataAnalyzer(caller_agent=self.name)
            except ImportError as e:
                self.logger.error(f"Erro ao importar PythonDataAnalyzer: {e}")
                return self._build_response(
                    "‚ùå Erro: PythonDataAnalyzer n√£o dispon√≠vel para calcular medidas de tend√™ncia central",
                    metadata={"error": True}
                )
            
            # Obter DataFrame real dos chunks (APENAS EMBEDDINGS - NUNCA CSV)
            df = analyzer.get_data_from_embeddings(limit=None, parse_chunk_text=True)
            
            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel obter dados dos embeddings para calcular medidas de tend√™ncia central",
                    metadata={"error": True}
                )
            
            self.logger.info(f"‚úÖ DataFrame carregado: {len(df)} registros, {len(df.columns)} colunas")
            
            # Calcular medidas de tend√™ncia central para TODAS as colunas num√©ricas
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if not numeric_cols:
                return self._build_response(
                    "‚ùå Nenhuma coluna num√©rica encontrada nos dados",
                    metadata={"error": True}
                )
            
            # Calcular m√©dia, mediana e moda
            stats_data = []
            for col in numeric_cols:
                col_mean = df[col].mean()
                col_median = df[col].median()
                
                # Moda (pode ter m√∫ltiplas modas)
                mode_values = df[col].mode()
                col_mode = mode_values.iloc[0] if len(mode_values) > 0 else None
                
                stats_data.append({
                    'variavel': col,
                    'media': col_mean,
                    'mediana': col_median,
                    'moda': col_mode
                })
            
            # Formatar resposta
            response = f"""üìä **Medidas de Tend√™ncia Central**

**Fonte:** Dados reais extra√≠dos da tabela embeddings (coluna chunk_text parseada)
**Total de registros analisados:** {len(df):,}
**Total de vari√°veis num√©ricas:** {len(numeric_cols)}

**O que s√£o Medidas de Tend√™ncia Central?**

As medidas de tend√™ncia central s√£o estat√≠sticas que descrevem o valor central de uma distribui√ß√£o de dados:

‚Ä¢ **M√©dia**: Soma de todos os valores dividida pelo n√∫mero de valores. Sens√≠vel a outliers.
‚Ä¢ **Mediana**: Valor do meio quando os dados est√£o ordenados. Mais robusta a outliers.
‚Ä¢ **Moda**: Valor que aparece com maior frequ√™ncia nos dados.

"""
            
            # Adicionar tabela formatada
            response += "| Vari√°vel | M√©dia | Mediana | Moda |\n"
            response += "|----------|-------|---------|------|\n"
            
            # Mostrar TODAS as vari√°veis (removida limita√ß√£o de 15)
            for stat in stats_data:
                var_name = stat['variavel']
                var_mean = stat['media']
                var_median = stat['mediana']
                var_mode = stat['moda']
                
                # Formatar valores com precis√£o adequada
                if abs(var_mean) < 1000 and abs(var_median) < 1000:
                    mode_str = f"{var_mode:.6f}" if var_mode is not None else "N/A"
                    response += f"| {var_name} | {var_mean:.6f} | {var_median:.6f} | {mode_str} |\n"
                else:
                    mode_str = f"{var_mode:.2f}" if var_mode is not None else "N/A"
                    response += f"| {var_name} | {var_mean:.2f} | {var_median:.2f} | {mode_str} |\n"
            
            response += f"\n**Diferen√ßa entre M√©dia e Mediana:**\n"
            response += f"‚Ä¢ A m√©dia √© sens√≠vel a valores extremos (outliers), enquanto a mediana n√£o.\n"
            response += f"‚Ä¢ Se houver outliers nos dados, a mediana √© uma medida mais representativa do centro.\n"
            response += f"‚Ä¢ Para distribui√ß√µes sim√©tricas, m√©dia e mediana t√™m valores pr√≥ximos.\n"
            
            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"
            response += f"‚úÖ **M√©todo:** Parsing de chunk_text + an√°lise com pandas\n"
            
            return self._build_response(response, metadata={
                'total_records': len(df),
                'total_numeric_columns': len(numeric_cols),
                'central_tendency': stats_data,
                'conformidade': 'embeddings_only',
                'query_type': 'central_tendency'
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao calcular medidas de tend√™ncia central: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao calcular medidas de tend√™ncia central dos dados: {str(e)}",
                metadata={"error": True, "conformidade": "embeddings_only"}
            )
    
    def _handle_correlation_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre correla√ß√£o entre vari√°veis usando dados dos embeddings.
        
        Args:
            query: Pergunta sobre correla√ß√£o
            context: Contexto adicional
            
        Returns:
            Resposta com matriz de correla√ß√£o
        """
        try:
            self.logger.info("üìä Calculando correla√ß√µes entre vari√°veis...")
            
            from src.tools.python_analyzer import PythonDataAnalyzer
            analyzer = PythonDataAnalyzer()
            
            # Parsing dos chunks para DataFrame
            # Primeiro, tentar reconstruir dados via Supabase embeddings (m√©todo p√∫blico recomendado)
            df = analyzer.reconstruct_original_data()

            # Se n√£o conseguiu (ex: ambiente de testes sem Supabase), tentar parsear current_embeddings diretamente
            if df is None:
                full_text = "\n".join([emb.get('chunk_text', '') for emb in self.current_embeddings])
                import pandas as pd
                embeddings_df = pd.DataFrame([{'chunk_text': full_text}])
                df = analyzer._parse_chunk_text_to_dataframe(embeddings_df=embeddings_df)

            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel extrair dados dos embeddings",
                    metadata={"error": True}
                )
            
            # Selecionar apenas colunas num√©ricas
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            if len(numeric_cols) < 2:
                return self._build_response(
                    "‚ùå √â necess√°rio pelo menos 2 vari√°veis num√©ricas para calcular correla√ß√£o",
                    metadata={"error": True}
                )
            
            # Calcular matriz de correla√ß√£o
            corr_matrix = df[numeric_cols].corr()
            
            response = f"## üîó Matriz de Correla√ß√£o\n\n"
            response += f"**Total de vari√°veis analisadas:** {len(numeric_cols)}\n\n"
            
            # Formatar matriz
            response += "| Vari√°veis |"
            for col in numeric_cols[:10]:  # Limitar para n√£o ficar muito grande
                response += f" {col} |"
            response += "\n|"
            response += "---|" * (len(numeric_cols[:10]) + 1)
            response += "\n"
            
            for idx in numeric_cols[:10]:
                response += f"| {idx} |"
                for col in numeric_cols[:10]:
                    corr_val = corr_matrix.loc[idx, col]
                    response += f" {corr_val:.3f} |"
                response += "\n"
            
            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"
            
            return self._build_response(response, metadata={
                'total_variables': len(numeric_cols),
                'correlation_matrix': corr_matrix.to_dict(),
                'conformidade': 'embeddings_only',
                'query_type': 'correlation'
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao calcular correla√ß√£o: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao calcular correla√ß√£o: {str(e)}",
                metadata={"error": True}
            )
    
    def _handle_distribution_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre distribui√ß√£o de dados usando embeddings.
        
        Args:
            query: Pergunta sobre distribui√ß√£o
            context: Contexto adicional
            
        Returns:
            Resposta com an√°lise de distribui√ß√£o
        """
        try:
            self.logger.info("üìä Analisando distribui√ß√£o dos dados...")
            
            from src.tools.python_analyzer import PythonDataAnalyzer
            from scipy import stats
            analyzer = PythonDataAnalyzer()
            
            # Tentar reconstruir via m√©todo p√∫blico
            df = analyzer.reconstruct_original_data()

            # Se n√£o conseguiu (ambiente de testes), tentar parsear current_embeddings diretamente
            if df is None:
                full_text = "\n".join([emb.get('chunk_text', '') for emb in self.current_embeddings])
                import pandas as pd
                embeddings_df = pd.DataFrame([{'chunk_text': full_text}])
                df = analyzer._parse_chunk_text_to_dataframe(embeddings_df=embeddings_df)

            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel extrair dados dos embeddings",
                    metadata={"error": True}
                )
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            # Construir sum√°rio estat√≠stico (texto)
            response = f"## üìä An√°lise de Distribui√ß√£o\n\n"
            response += f"**Total de vari√°veis num√©ricas:** {len(numeric_cols)}\n\n"

            stats_summary = {}
            for col in numeric_cols[:10]:  # Limitar
                try:
                    if len(df[col].dropna()) > 3:
                        stat, pvalue = stats.shapiro(df[col].dropna()[:5000])  # Max 5000 amostras
                        is_normal = "Sim" if pvalue > 0.05 else "N√£o"
                        skewness = df[col].skew()
                        kurtosis_val = df[col].kurtosis()

                        response += f"### {col}\n"
                        response += f"- **Normal?** {is_normal} (p-value: {pvalue:.4f})\n"
                        response += f"- **Assimetria (skewness):** {skewness:.3f}\n"
                        response += f"- **Curtose (kurtosis):** {kurtosis_val:.3f}\n\n"

                        stats_summary[col] = {
                            'normal': is_normal,
                            'pvalue': float(pvalue),
                            'skewness': float(skewness),
                            'kurtosis': float(kurtosis_val)
                        }
                except Exception as e:
                    self.logger.warning(f"Falha ao calcular estat√≠sticas para {col}: {e}")

            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"

            # Delegar gera√ß√£o de gr√°ficos para o handler de visualiza√ß√£o para garantir que os PNGs sejam criados
            try:
                vis_result = self._handle_visualization_query(query, context)
                # Mesclar metadados e resposta textual
                metadata = vis_result.get('metadata', {}) if isinstance(vis_result, dict) else {}
                # Garantir campos estat√≠sticos incluso
                metadata.setdefault('distribution_stats', stats_summary)

                # Se a visualiza√ß√£o gerou gr√°ficos, anexar ao texto
                if metadata.get('visualization_success'):
                    response += f"\nüìà Gr√°ficos gerados: {len(metadata.get('graficos_gerados', []))}\n"

                return self._build_response(response, metadata={
                    'total_variables': len(numeric_cols),
                    'conformidade': 'embeddings_only',
                    'query_type': 'distribution',
                    **metadata
                })
            except Exception as e:
                self.logger.warning(f"Falha ao gerar visualiza√ß√µes a partir de distribui√ß√£o: {e}")
                return self._build_response(response, metadata={
                    'total_variables': len(numeric_cols),
                    'conformidade': 'embeddings_only',
                    'query_type': 'distribution',
                    'visualization_error': str(e)
                })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao analisar distribui√ß√£o: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao analisar distribui√ß√£o: {str(e)}",
                metadata={"error": True}
            )
    
    def _handle_outliers_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas sobre outliers usando dados dos embeddings.
        
        Args:
            query: Pergunta sobre outliers
            context: Contexto adicional
            
        Returns:
            Resposta com detec√ß√£o de outliers
        """
        try:
            self.logger.info("üìä Detectando outliers nos dados...")
            
            from src.tools.python_analyzer import PythonDataAnalyzer
            analyzer = PythonDataAnalyzer()
            
            full_text = "\n".join([emb.get('chunk_text', '') for emb in self.current_embeddings])
            df = analyzer.parse_chunk_text(full_text)
            
            if df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel extrair dados dos embeddings",
                    metadata={"error": True}
                )
            
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            
            response = f"## üîç Detec√ß√£o de Outliers (M√©todo IQR)\n\n"
            response += f"**M√©todo:** Interquartile Range (IQR)\n"
            response += f"**Crit√©rio:** Valores abaixo de Q1 - 1.5*IQR ou acima de Q3 + 1.5*IQR\n\n"
            
            response += "| Vari√°vel | Outliers Inferiores | Outliers Superiores | Total Outliers | % do Total |\n"
            response += "|----------|---------------------|---------------------|----------------|------------|\n"
            
            total_records = len(df)
            
            for col in numeric_cols[:15]:  # Limitar
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                lower_outliers = (df[col] < lower_bound).sum()
                upper_outliers = (df[col] > upper_bound).sum()
                total_outliers = lower_outliers + upper_outliers
                pct_outliers = (total_outliers / total_records) * 100
                
                response += f"| {col} | {lower_outliers} | {upper_outliers} | {total_outliers} | {pct_outliers:.2f}% |\n"
            
            response += f"\n‚úÖ **Conformidade:** Dados obtidos exclusivamente da tabela embeddings\n"
            
            return self._build_response(response, metadata={
                'total_variables': len(numeric_cols),
                'total_records': total_records,
                'conformidade': 'embeddings_only',
                'query_type': 'outliers'
            })
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao detectar outliers: {str(e)}")
            return self._build_response(
                f"‚ùå Erro ao detectar outliers: {str(e)}",
                metadata={"error": True}
            )
    
    def _handle_visualization_query(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas que solicitam visualiza√ß√µes (histogramas, gr√°ficos, distribui√ß√£o).
        
        Args:
            query: Pergunta do usu√°rio solicitando visualiza√ß√£o
            context: Contexto adicional
            
        Returns:
            Resposta com histogramas gerados e salvos em arquivos
        """
        try:
            self.logger.info("üìä Processando solicita√ß√£o de visualiza√ß√£o...")
            
            # Importar m√≥dulos necess√°rios
            from src.tools.python_analyzer import PythonDataAnalyzer
            import matplotlib.pyplot as plt
            import seaborn as sns
            import os
            from pathlib import Path
            
            # Configurar estilo dos gr√°ficos
            sns.set_style("whitegrid")
            
            # Inicializar analyzer
            analyzer = PythonDataAnalyzer(caller_agent=self.name)
            
            # Reconstruir DataFrame a partir dos embeddings
            self.logger.info("üîÑ Reconstruindo DataFrame a partir dos embeddings...")
            df = analyzer.reconstruct_original_data()
            
            if df is None or df.empty:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel reconstruir os dados para gerar visualiza√ß√µes. Verifique se h√° dados na tabela embeddings.",
                    metadata={"error": True, "conformidade": "embeddings_only"}
                )
            
            self.logger.info(f"‚úÖ DataFrame reconstru√≠do: {df.shape[0]} linhas, {df.shape[1]} colunas")
            
            # Criar diret√≥rio de sa√≠da
            output_dir = Path('outputs/histogramas')
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Separar vari√°veis num√©ricas e categ√≥ricas
            numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
            
            self.logger.info(f"üìä Gerando histogramas para {len(numeric_cols)} vari√°veis num√©ricas...")
            self.logger.info(f"üìä Gerando gr√°ficos de barras para {len(categorical_cols)} vari√°veis categ√≥ricas...")
            
            graficos_gerados = []
            estatisticas_geradas = {}
            
            # Gerar histogramas para vari√°veis num√©ricas
            for col in numeric_cols:
                try:
                    self.logger.info(f"  Gerando histograma para: {col}")
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    
                    # Histograma
                    ax.hist(df[col].dropna(), bins=50, alpha=0.7, color='steelblue', edgecolor='black')
                    
                    # Estat√≠sticas
                    mean_val = df[col].mean()
                    median_val = df[col].median()
                    std_val = df[col].std()
                    
                    # Linhas de refer√™ncia
                    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'M√©dia: {mean_val:.2f}')
                    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2f}')
                    
                    ax.set_xlabel(col, fontsize=12)
                    ax.set_ylabel('Frequ√™ncia', fontsize=12)
                    ax.set_title(f'Distribui√ß√£o de {col}', fontsize=14, fontweight='bold')
                    ax.legend()
                    ax.grid(True, alpha=0.3)
                    
                    # Salvar
                    filename = output_dir / f'hist_{col}.png'
                    plt.tight_layout()
                    plt.savefig(filename, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    graficos_gerados.append(str(filename))
                    estatisticas_geradas[col] = {
                        'mean': float(mean_val),
                        'median': float(median_val),
                        'std': float(std_val),
                        'min': float(df[col].min()),
                        'max': float(df[col].max()),
                        'count': int(df[col].count()),
                        'missing': int(df[col].isna().sum())
                    }
                    
                    self.logger.info(f"  ‚úÖ Histograma salvo: {filename}")
                    
                except Exception as e:
                    self.logger.error(f"  ‚ùå Erro ao gerar histograma para {col}: {e}")
            
            # Gerar gr√°ficos de barras para vari√°veis categ√≥ricas (limitado a vari√°veis com poucos valores √∫nicos)
            for col in categorical_cols:
                try:
                    unique_count = df[col].nunique()
                    if unique_count > 20:  # Limitar a vari√°veis com at√© 20 valores √∫nicos
                        self.logger.info(f"  Pulando {col} (muitos valores √∫nicos: {unique_count})")
                        continue
                    
                    self.logger.info(f"  Gerando gr√°fico de barras para: {col}")
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    
                    # Contagem de valores
                    value_counts = df[col].value_counts()
                    
                    # Gr√°fico de barras
                    value_counts.plot(kind='bar', ax=ax, color='coral', edgecolor='black', alpha=0.7)
                    
                    ax.set_xlabel(col, fontsize=12)
                    ax.set_ylabel('Frequ√™ncia', fontsize=12)
                    ax.set_title(f'Distribui√ß√£o de {col}', fontsize=14, fontweight='bold')
                    ax.grid(True, alpha=0.3, axis='y')
                    
                    # Rotacionar labels se necess√°rio
                    plt.xticks(rotation=45, ha='right')
                    
                    # Salvar
                    filename = output_dir / f'bar_{col}.png'
                    plt.tight_layout()
                    plt.savefig(filename, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                    graficos_gerados.append(str(filename))
                    estatisticas_geradas[col] = {
                        'unique_values': unique_count,
                        'most_common': str(value_counts.index[0]) if len(value_counts) > 0 else None,
                        'most_common_count': int(value_counts.values[0]) if len(value_counts) > 0 else 0,
                        'count': int(df[col].count()),
                        'missing': int(df[col].isna().sum())
                    }
                    
                    self.logger.info(f"  ‚úÖ Gr√°fico de barras salvo: {filename}")
                    
                except Exception as e:
                    self.logger.error(f"  ‚ùå Erro ao gerar gr√°fico para {col}: {e}")
            
            # Construir resposta
            if graficos_gerados:
                response = f"""üìä **Visualiza√ß√µes Geradas com Sucesso!**

‚úÖ Total de gr√°ficos gerados: {len(graficos_gerados)}
   ‚Ä¢ Histogramas (vari√°veis num√©ricas): {len([g for g in graficos_gerados if 'hist_' in g])}
   ‚Ä¢ Gr√°ficos de barras (vari√°veis categ√≥ricas): {len([g for g in graficos_gerados if 'bar_' in g])}

üìÅ **Local dos arquivos:**
   {output_dir.absolute()}

üìà **Gr√°ficos salvos:**
"""
                for i, grafico in enumerate(graficos_gerados, 1):
                    response += f"   {i}. {Path(grafico).name}\n"
                
                response += f"\nüí° **Dica:** Voc√™ pode visualizar os gr√°ficos abrindo os arquivos PNG no diret√≥rio indicado."
                
                return self._build_response(response, metadata={
                    'graficos_gerados': graficos_gerados,
                    'estatisticas': estatisticas_geradas,
                    'output_dir': str(output_dir.absolute()),
                    'numeric_cols': numeric_cols,
                    'categorical_cols': categorical_cols,
                    'conformidade': 'embeddings_only',
                    'visualization_success': True
                })
            else:
                return self._build_response(
                    "‚ùå N√£o foi poss√≠vel gerar visualiza√ß√µes. Verifique os logs para mais detalhes.",
                    metadata={'error': True, 'conformidade': 'embeddings_only'}
                )
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao processar visualiza√ß√£o: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._build_response(
                f"‚ùå Erro ao gerar visualiza√ß√µes: {str(e)}",
                metadata={'error': True, 'conformidade': 'embeddings_only', 'exception': str(e)}
            )
    
    def _handle_general_query_from_embeddings(self, query: str, context: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Processa consultas gerais usando embeddings."""
        if not self.current_embeddings:
            return self._build_response("‚ùå Nenhum embedding dispon√≠vel")
        
        analysis = self._analyze_embeddings_data()
        
        response = f"""üí° **Informa√ß√µes Dispon√≠veis (via Embeddings)**
        
Dados carregados: {analysis['embeddings_count']} embeddings
Datasets: {', '.join(self.dataset_metadata.get('sources', ['N/A']))}

**Para an√°lises espec√≠ficas, tente:**
‚Ä¢ "resumo" - vis√£o geral dos dados
‚Ä¢ "an√°lise" - an√°lise de padr√µes
‚Ä¢ "busca [termo]" - buscar conte√∫do espec√≠fico
‚Ä¢ "contagem" - estat√≠sticas b√°sicas
        """
        
        return self._build_response(response, metadata={
            **analysis,
            'conformidade': 'embeddings_only',
            'suggestions': ['resumo', 'an√°lise', 'busca', 'contagem']
        })
    
    def get_embeddings_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes dos embeddings carregados.
        
        CONFORMIDADE: Apenas dados da tabela embeddings.
        """
        self._validate_embeddings_access_only()
        
        if not self.current_embeddings:
            return {"error": "Nenhum embedding carregado", "conformidade": "embeddings_only"}
        
        return {
            **self._analyze_embeddings_data(),
            'conformidade': 'embeddings_only',
            'agente': self.name
        }
    
    def validate_architecture_compliance(self) -> Dict[str, Any]:
        """Valida conformidade com arquitetura de embeddings-only.
        
        Returns:
            Relat√≥rio de conformidade
        """
        compliance_report = {
            'compliant': True,
            'violations': [],
            'data_source': 'embeddings_table_only',
            'csv_access': False,
            'agent_name': self.name,
            'compliance_score': 1.0  # Score inicial perfeito
        }
        
        # Verificar se h√° vest√≠gios de acesso a CSV
        forbidden_attributes = ['current_df', 'current_file_path', 'pandas_agent']
        for attr in forbidden_attributes:
            if hasattr(self, attr):
                compliance_report['compliant'] = False
                compliance_report['violations'].append(f"Atributo proibido encontrado: {attr}")
                compliance_report['compliance_score'] -= 0.3  # Penalizar por viola√ß√£o
        
        # Verificar se usa apenas Supabase
        if not SUPABASE_AVAILABLE:
            compliance_report['compliant'] = False
            compliance_report['violations'].append("Supabase n√£o dispon√≠vel")
            compliance_report['compliance_score'] -= 0.5
        
        # Garantir score m√≠nimo 0
        compliance_report['compliance_score'] = max(0.0, compliance_report['compliance_score'])
        
        return compliance_report
    
    def export_embeddings_analysis(self, output_path: str) -> Dict[str, Any]:
        """Exporta an√°lise dos embeddings para arquivo.
        
        CONFORMIDADE: Exporta apenas dados derivados da tabela embeddings.
        """
        self._validate_embeddings_access_only()
        
        try:
            export_data = {
                'timestamp': datetime.now().isoformat(),
                'agent': self.name,
                'conformidade': 'embeddings_only',
                'embeddings_analysis': self._analyze_embeddings_data(),
                'dataset_metadata': self.dataset_metadata,
                'compliance_report': self.validate_architecture_compliance()
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
            
            return self._build_response(
                f"‚úÖ An√°lise de embeddings exportada para: {output_path}",
                metadata={
                    "export_path": output_path,
                    "conformidade": "embeddings_only"
                }
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao exportar an√°lise de embeddings: {str(e)}")
            return self._build_response(
                f"Erro na exporta√ß√£o: {str(e)}",
                metadata={"error": True, "conformidade": "embeddings_only"}
            )


# Alias para compatibilidade com c√≥digo existente
# DEPRECATED: Use EmbeddingsAnalysisAgent diretamente
CSVAnalysisAgent = EmbeddingsAnalysisAgent