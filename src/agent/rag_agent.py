"""Agente RAG (Retrieval Augmented Generation) para consultas inteligentes.

‚ö†Ô∏è CONFORMIDADE: Este agente funciona como AGENTE DE INGEST√ÉO autorizado.
Pode ler CSV diretamente para indexa√ß√£o na tabela embeddings.

Este agente combina:
- Chunking de texto/dados
- Gera√ß√£o de embeddings  
- Busca vetorial
- Gera√ß√£o de respostas contextualizadas via LLM
"""
from __future__ import annotations
from typing import List, Dict, Any, Optional, Union, Tuple
import time
import io
from pathlib import Path

import pandas as pd

from src.agent.base_agent import BaseAgent, AgentError
from src.embeddings.chunker import TextChunker, ChunkStrategy, TextChunk
from src.embeddings.generator import EmbeddingGenerator, EmbeddingProvider
from src.embeddings.vector_store import VectorStore, VectorSearchResult
from src.api.sonar_client import send_sonar_query


class RAGAgent(BaseAgent):
    """Agente RAG para consultas inteligentes com contexto vetorial.
    
    ‚ö†Ô∏è CONFORMIDADE: Este agente √© o AGENTE DE INGEST√ÉO AUTORIZADO do sistema.
    Tem permiss√£o para ler CSV diretamente e indexar na tabela embeddings.
    """
    
    def __init__(self, 
                 embedding_provider: EmbeddingProvider = EmbeddingProvider.SENTENCE_TRANSFORMER,
                 chunk_size: int = 512,
                 chunk_overlap: int = 50,
                 csv_chunk_size_rows: int = 20,
                 csv_overlap_rows: int = 4):
        """Inicializa o agente RAG.
        
        Args:
            embedding_provider: Provedor de embeddings
            chunk_size: Tamanho dos chunks em caracteres
            chunk_overlap: Sobreposi√ß√£o entre chunks
        """
        super().__init__(
            name="rag_agent",
            description="Agente RAG para consultas contextualizadas com busca vetorial",
            enable_memory=True  # Habilita sistema de mem√≥ria
        )
        # Cache de buscas em mem√≥ria local (otimiza√ß√£o)
        self._search_cache: Dict[str, Any] = {}
        self._relevance_scores: Dict[str, float] = {}
        
        # Inicializar componentes
        try:
            self.chunker = TextChunker(
                chunk_size=chunk_size,
                overlap_size=chunk_overlap,
                min_chunk_size=50,
                csv_chunk_size_rows=csv_chunk_size_rows,
                csv_overlap_rows=csv_overlap_rows
            )
            
            self.embedding_generator = EmbeddingGenerator(
                provider=embedding_provider
            )
            
            self.vector_store = VectorStore()
            
            self.logger.info("Agente RAG inicializado com sucesso e sistema de mem√≥ria")
            
        except Exception as e:
            self.logger.error(f"Erro na inicializa√ß√£o do RAG: {str(e)}")
            raise AgentError(self.name, f"Falha na inicializa√ß√£o: {str(e)}")
    
    def ingest_text(self, 
                   text: str, 
                   source_id: str,
                   source_type: str = "text",
                   chunk_strategy: ChunkStrategy = ChunkStrategy.FIXED_SIZE) -> Dict[str, Any]:
        """Ingesta texto no sistema RAG (chunking + embeddings + armazenamento).
        
        Args:
            text: Texto para processar
            source_id: Identificador √∫nico da fonte
            source_type: Tipo da fonte (text, csv, document)
            chunk_strategy: Estrat√©gia de chunking
        
        Returns:
            Resultado do processamento com estat√≠sticas
        """
        self.logger.info(f"Iniciando ingest√£o: {len(text)} chars, fonte: {source_id}")
        start_time = time.perf_counter()
        
        try:
            # 1. Chunking
            self.logger.info("Executando chunking...")
            chunks = self.chunker.chunk_text(text, source_id, chunk_strategy)
            
            if not chunks:
                return self._build_response(
                    "Nenhum chunk v√°lido foi criado a partir do texto",
                    metadata={"error": True}
                )

            # OTIMIZA√á√ÉO BALANCEADA: Enriquecimento leve para manter precis√£o sem comprometer velocidade
            if chunk_strategy == ChunkStrategy.CSV_ROW:
                chunks = self._enrich_csv_chunks_light(chunks)
            
            chunk_stats = self.chunker.get_stats(chunks)
            self.logger.info(f"Criados {len(chunks)} chunks")
            
            # 2. Gera√ß√£o de embeddings (MODO ASS√çNCRONO para performance)
            self.logger.info("Gerando embeddings com processamento ass√≠ncrono...")
            
            # Usar gera√ß√£o ass√≠ncrona se dispon√≠vel
            try:
                from src.embeddings.async_generator import run_async_embeddings
                embedding_results = run_async_embeddings(
                    chunks=chunks,
                    provider=self.embedding_generator.provider,
                    max_workers=4  # 4 workers paralelos
                )
                self.logger.info("‚úÖ Embeddings gerados com processamento ass√≠ncrono")
            except ImportError:
                # Fallback para processamento s√≠ncrono
                self.logger.warning("Processamento ass√≠ncrono n√£o dispon√≠vel, usando s√≠ncrono")
                embedding_results = self.embedding_generator.generate_embeddings_batch(chunks)
            except Exception as e:
                self.logger.error(f"Erro no processamento ass√≠ncrono: {e}, fallback para s√≠ncrono")
                embedding_results = self.embedding_generator.generate_embeddings_batch(chunks)
            
            if not embedding_results:
                return self._build_response(
                    "Falha na gera√ß√£o de embeddings",
                    metadata={"error": True, "chunk_stats": chunk_stats}
                )
            
            embedding_stats = self.embedding_generator.get_embedding_stats(embedding_results)
            self.logger.info(f"Gerados {len(embedding_results)} embeddings")
            
            # 3. Armazenamento
            self.logger.info("Armazenando no vector store...")
            stored_ids = self.vector_store.store_embeddings(embedding_results, source_type)
            
            processing_time = time.perf_counter() - start_time
            
            # Estat√≠sticas consolidadas
            stats = {
                "source_id": source_id,
                "source_type": source_type,
                "processing_time": processing_time,
                "chunks_created": len(chunks),
                "embeddings_generated": len(embedding_results),
                "embeddings_stored": len(stored_ids),
                "chunk_strategy": chunk_strategy.value,
                "chunk_stats": chunk_stats,
                "embedding_stats": embedding_stats,
                "success_rate": len(stored_ids) / len(chunks) * 100 if chunks else 0
            }
            
            response = f"‚úÖ Ingest√£o conclu√≠da para '{source_id}'\n" \
                      f"üìä {len(chunks)} chunks ‚Üí {len(embedding_results)} embeddings ‚Üí {len(stored_ids)} armazenados\n" \
                      f"‚è±Ô∏è Processado em {processing_time:.2f}s"
            
            self.logger.info(f"Ingest√£o conclu√≠da: {stats['success_rate']:.1f}% sucesso")
            
            return self._build_response(response, metadata=stats)
            
        except Exception as e:
            self.logger.error(f"Erro na ingest√£o: {str(e)}")
            return self._build_response(
                f"Erro na ingest√£o: {str(e)}",
                metadata={"error": True}
            )
    
    def ingest_csv_data(self, 
                       csv_text: str, 
                       source_id: str,
                       include_headers: bool = True) -> Dict[str, Any]:
        """Ingesta dados CSV (conte√∫do bruto) usando estrat√©gia especializada.
        
        ‚ö†Ô∏è CONFORMIDADE: RAGAgent √© o AGENTE DE INGEST√ÉO AUTORIZADO.
        Este m√©todo tem permiss√£o para processar CSV diretamente.
        """
        self.logger.info(f"‚úÖ INGEST√ÉO AUTORIZADA: RAGAgent processando CSV: {source_id}")
        self.logger.info("‚úÖ CONFORMIDADE: Agente de ingest√£o tem permiss√£o para ler CSV")
        
        # Primeiro, ingestar dados normais
        result = self.ingest_text(
            text=csv_text,
            source_id=source_id,
            source_type="csv",
            chunk_strategy=ChunkStrategy.CSV_ROW
        )
        
        # Se ingest√£o foi bem-sucedida, adicionar chunks de metadados
        if not result.get("metadata", {}).get("error"):
            try:
                self.logger.info("üìä Gerando chunks de metadados do dataset...")
                metadata_chunks = self._generate_metadata_chunks(csv_text, source_id)
                if metadata_chunks:
                    # Gerar embeddings para chunks de metadados
                    metadata_embeddings = self.embedding_generator.generate_embeddings_batch(metadata_chunks)
                    if metadata_embeddings:
                        # Armazenar embeddings de metadados
                        metadata_stored_ids = self.vector_store.store_embeddings(metadata_embeddings, "csv")
                        self.logger.info(f"‚úÖ {len(metadata_chunks)} chunks de metadados criados e armazenados")
                    else:
                        self.logger.warning("‚ö†Ô∏è Falha ao gerar embeddings para chunks de metadados")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Falha ao gerar chunks de metadados: {e}")
        
        return result

    def _enrich_csv_chunks_light(self, chunks: List[TextChunk]) -> List[TextChunk]:
        """VERS√ÉO BALANCEADA - Enriquecimento leve que mant√©m precis√£o sem comprometer velocidade."""
        enriched_chunks: List[TextChunk] = []

        for chunk in chunks:
            info = chunk.metadata.additional_info or {}
            start_row = info.get("start_row")
            end_row = info.get("end_row")
            row_span = f"linhas {start_row} a {end_row}" if start_row and end_row else "intervalo n√£o identificado"
            
            # An√°lise r√°pida sem pandas
            lines = chunk.content.split('\n')
            header_line = lines[0] if lines else ""
            data_lines = [line for line in lines[1:] if line.strip()]
            
            # Extrair nome do arquivo CSV do metadata do chunk
            csv_filename = chunk.metadata.additional_info.get('source_file', 'dataset.csv') if chunk.metadata.additional_info else 'dataset.csv'
            if not csv_filename.endswith('.csv'):
                # Tentar extrair do conte√∫do do chunk
                import re
                csv_match = re.search(r'([\w-]+\.csv)', chunk.content)
                if csv_match:
                    csv_filename = csv_match.group(1)
            
            # Detectar automaticamente colunas do header (gen√©rico para qualquer CSV)
            detected_columns = []
            if header_line:
                # Parsear header (com ou sem aspas)
                detected_columns = [col.strip().strip('"') for col in header_line.split(',')]
                detected_columns = [col for col in detected_columns if col and not col.startswith('#')]
            
            # An√°lise gen√©rica: detectar poss√≠veis colunas de classifica√ß√£o/target (√∫ltima coluna)
            target_column = None
            binary_class_count = 0
            if detected_columns and len(detected_columns) > 0:
                target_column = detected_columns[-1]  # √öltima coluna geralmente √© o target
                # Verificar se √© bin√°ria (0 ou 1)
                for line in data_lines[:100]:  # Amostra
                    parts = line.split(',')
                    if parts and parts[-1].strip() in ['0', '1', '"0"', '"1"']:
                        binary_class_count += 1
            
            # Construir descri√ß√£o contextual gen√©rica e otimizada
            summary_lines = [
                f"Chunk do dataset {csv_filename} ({row_span}) - {len(data_lines)} registros",
            ]
            
            # Adicionar informa√ß√µes sobre colunas detectadas
            if detected_columns:
                num_cols = len(detected_columns)
                col_sample = ', '.join(detected_columns[:3])
                if num_cols > 3:
                    col_sample += f", ... ({num_cols} colunas no total)"
                summary_lines.append(f"Colunas: {col_sample}")
            
            # Se detectar poss√≠vel classifica√ß√£o bin√°ria
            if binary_class_count > 0:
                binary_ratio = (binary_class_count / min(len(data_lines), 100)) * 100
                if binary_ratio > 50:  # Se >50% das linhas s√£o bin√°rias na √∫ltima coluna
                    if target_column:
                        summary_lines.append(f"Coluna '{target_column}': Vari√°vel bin√°ria detectada (~{binary_ratio:.1f}% de valores bin√°rios na amostra)")
                    else:
                        summary_lines.append(f"Classifica√ß√£o bin√°ria detectada (~{binary_ratio:.1f}% na amostra)")
            
            # Adicionar informa√ß√£o sobre tipo de dados
            if len(detected_columns) > 5:
                summary_lines.append(f"Dataset com {len(detected_columns)} features para an√°lise")
            
            # Amostra das primeiras linhas para contexto
            if len(data_lines) >= 2:
                sample_line = data_lines[0][:150] + "..." if len(data_lines[0]) > 150 else data_lines[0]
                summary_lines.append(f"Exemplo de registro: {sample_line}")
            
            # Incluir cabe√ßalho para refer√™ncia
            summary_lines.append(f"Colunas: {header_line}")

            # CORRE√á√ÉO CR√çTICA: Manter dados originais + adicionar contexto enriquecido
            context_summary = "\n".join(summary_lines)
            enriched_content = f"{context_summary}\n\n=== DADOS ORIGINAIS ===\n{chunk.content}"
            
            enriched_chunks.append(TextChunk(content=enriched_content, metadata=chunk.metadata))

        return enriched_chunks

    def _generate_metadata_chunks(self, csv_text: str, source_id: str) -> List[TextChunk]:
        """Gera chunks adicionais sobre metadados do dataset para melhorar RAG.
        
        Cria chunks espec√≠ficos para responder perguntas sobre:
        1. Tipos de dados (num√©ricos, categ√≥ricos)
        2. Distribui√ß√£o das vari√°veis (histogramas, quartis, percentis)
        3. Intervalos (min, max)
        4. Medidas de tend√™ncia central (m√©dia, mediana)
        5. Variabilidade (desvio padr√£o, vari√¢ncia, IQR)
        6. Valores frequentes/raros
        7. Outliers e anomalias
        8. Correla√ß√µes entre vari√°veis
        9. Padr√µes temporais (se houver)
        10. Estrutura e informa√ß√µes gerais
        
        Sistema gen√©rico para QUALQUER CSV.
        """
        from src.embeddings.chunker import ChunkMetadata, ChunkStrategy
        import pandas as pd
        import numpy as np
        import io
        
        chunks = []
        self.logger.info(f"üìä Gerando chunks de metadados anal√≠ticos para {source_id}...")
        
        try:
            # Ler CSV completo para an√°lise robusta
            df = pd.read_csv(io.StringIO(csv_text))
            total_rows = len(df)
            
            # Identificar colunas num√©ricas e categ√≥ricas
            numeric_cols_raw = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()
            
            # üîç DETEC√á√ÉO INTELIGENTE: Colunas num√©ricas com poucos valores √∫nicos s√£o CATEG√ìRICAS
            # Heur√≠stica: Se tem <= 10 valores √∫nicos OU <= 0.5% de cardinalidade, √© categ√≥rico
            categorical_from_numeric = []
            truly_numeric = []
            
            for col in numeric_cols_raw:
                n_unique = df[col].nunique()
                cardinality_ratio = n_unique / total_rows if total_rows > 0 else 0
                
                # Crit√©rio: <= 10 valores √∫nicos OU cardinalidade < 0.5% (Ex: Class com 2 valores)
                if n_unique <= 10 or cardinality_ratio < 0.005:
                    categorical_from_numeric.append(col)
                else:
                    truly_numeric.append(col)
            
            # Consolidar categ√≥ricos
            categorical_cols.extend(categorical_from_numeric)
            numeric_cols = truly_numeric
            
            # === CHUNK 1: TIPOS DE DADOS E ESTRUTURA ===
            types_content = f"""AN√ÅLISE DE TIPOLOGIA E ESTRUTURA - DATASET: {source_id.upper()}

ESTRUTURA GERAL:
- Total de registros: {total_rows:,}
- Total de colunas: {len(df.columns)}
- Colunas num√©ricas: {len(numeric_cols)}
- Colunas categ√≥ricas: {len(categorical_cols)}
- Colunas temporais: {len(datetime_cols)}

COLUNAS NUM√âRICAS ({len(numeric_cols)}):
{chr(10).join([f"  ‚Ä¢ {col} ({df[col].dtype})" for col in numeric_cols]) or "  Nenhuma"}

COLUNAS CATEG√ìRICAS ({len(categorical_cols)}):
{chr(10).join([f"  ‚Ä¢ {col} ({df[col].nunique()} valores √∫nicos)" for col in categorical_cols]) or "  Nenhuma"}

COLUNAS TEMPORAIS ({len(datetime_cols)}):
{chr(10).join([f"  ‚Ä¢ {col}" for col in datetime_cols]) or "  Nenhuma"}

Este chunk cont√©m informa√ß√µes completas sobre a tipologia e estrutura das colunas do dataset.
"""
            
            chunks.append(TextChunk(
                content=types_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=0, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(types_content), word_count=len(types_content.split()),
                    start_position=0, end_position=len(types_content),
                    additional_info={
                        "chunk_type": "metadata_types",
                        "topic": "data_types_structure"
                    }
                )
            ))
            
            # === CHUNK 2: DISTRIBUI√á√ïES E INTERVALOS ===
            dist_content = f"""AN√ÅLISE DE DISTRIBUI√á√ïES E INTERVALOS - DATASET: {source_id.upper()}

ESTAT√çSTICAS DESCRITIVAS (TODAS AS COLUNAS NUM√âRICAS):
"""
            if numeric_cols:
                desc = df[numeric_cols].describe(percentiles=[.25, .50, .75, .90, .95, .99])
                dist_content += desc.to_string()
                
                dist_content += "\n\nINTERVALOS (MIN-MAX) POR COLUNA:\n"
                for col in numeric_cols:
                    min_val = df[col].min()
                    max_val = df[col].max()
                    dist_content += f"  ‚Ä¢ {col}: [{min_val:.2f}, {max_val:.2f}]\n"
                
                dist_content += "\n\nQUARTIS E PERCENTIS:\n"
                for col in numeric_cols[:5]:  # Primeiras 5 colunas
                    q25, q50, q75 = df[col].quantile([0.25, 0.50, 0.75])
                    dist_content += f"  ‚Ä¢ {col}: Q1={q25:.2f}, Mediana={q50:.2f}, Q3={q75:.2f}\n"
            
            dist_content += "\n\nEste chunk cont√©m distribui√ß√µes estat√≠sticas completas, intervalos (min-max), quartis e percentis de todas as vari√°veis num√©ricas."
            
            chunks.append(TextChunk(
                content=dist_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=1, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(dist_content), word_count=len(dist_content.split()),
                    start_position=0, end_position=len(dist_content),
                    additional_info={
                        "chunk_type": "metadata_distribution",
                        "topic": "distributions_intervals"
                    }
                )
            ))
            
            # === CHUNK 3: TEND√äNCIA CENTRAL E VARIABILIDADE ===
            central_content = f"""AN√ÅLISE ESTAT√çSTICA: TEND√äNCIA CENTRAL E VARIABILIDADE - DATASET: {source_id.upper()}

MEDIDAS DE TEND√äNCIA CENTRAL:
"""
            if numeric_cols:
                central_content += "COLUNA | M√âDIA | MEDIANA | MODA\n"
                central_content += "-" * 60 + "\n"
                for col in numeric_cols:
                    mean_val = df[col].mean()
                    median_val = df[col].median()
                    mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else "N/A"
                    central_content += f"{col} | {mean_val:.2f} | {median_val:.2f} | {mode_val}\n"
                
                central_content += "\n\nMEDIDAS DE VARIABILIDADE:\n"
                central_content += "COLUNA | DESVIO PADR√ÉO | VARI√ÇNCIA | IQR (Intervalo Interquartil)\n"
                central_content += "-" * 80 + "\n"
                for col in numeric_cols:
                    std_val = df[col].std()
                    var_val = df[col].var()
                    q1, q3 = df[col].quantile([0.25, 0.75])
                    iqr_val = q3 - q1
                    central_content += f"{col} | {std_val:.2f} | {var_val:.2f} | {iqr_val:.2f}\n"
            
            central_content += "\n\nEste chunk cont√©m todas as medidas de tend√™ncia central (m√©dia, mediana, moda) e variabilidade (desvio padr√£o, vari√¢ncia, IQR) para todas as colunas num√©ricas."
            
            chunks.append(TextChunk(
                content=central_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=2, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(central_content), word_count=len(central_content.split()),
                    start_position=0, end_position=len(central_content),
                    additional_info={
                        "chunk_type": "metadata_central_variability",
                        "topic": "central_tendency_variability"
                    }
                )
            ))
            
            # === CHUNK 4: VALORES FREQUENTES E OUTLIERS ===
            freq_content = f"""AN√ÅLISE DE FREQU√äNCIA E DETEC√á√ÉO DE OUTLIERS - DATASET: {source_id.upper()}

VALORES MAIS FREQUENTES (TOP 5) POR COLUNA:
"""
            for col in categorical_cols[:5]:  # Primeiras 5 categ√≥ricas
                top_values = df[col].value_counts().head(5)
                freq_content += f"\n{col}:\n"
                for val, count in top_values.items():
                    pct = (count / total_rows) * 100
                    freq_content += f"  ‚Ä¢ {val}: {count} ({pct:.2f}%)\n"
            
            freq_content += "\n\nOUTLIERS DETECTADOS (M√©todo IQR):\n"
            outliers_detected = False
            if numeric_cols:
                for col in numeric_cols[:10]:  # Primeiras 10 num√©ricas
                    q1, q3 = df[col].quantile([0.25, 0.75])
                    iqr = q3 - q1
                    lower_bound = q1 - 1.5 * iqr
                    upper_bound = q3 + 1.5 * iqr
                    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
                    if len(outliers) > 0:
                        outliers_detected = True
                        pct_outliers = (len(outliers) / total_rows) * 100
                        freq_content += f"  ‚Ä¢ {col}: {len(outliers)} outliers ({pct_outliers:.2f}%)\n"
                        freq_content += f"    Intervalo normal: [{lower_bound:.2f}, {upper_bound:.2f}]\n"
            
            if not outliers_detected:
                freq_content += "  Nenhum outlier significativo detectado nas primeiras colunas.\n"
            
            freq_content += "\n\nEste chunk identifica valores mais frequentes em colunas categ√≥ricas e detecta outliers usando o m√©todo IQR (1.5√óIQR) com estat√≠sticas de preval√™ncia."
            
            chunks.append(TextChunk(
                content=freq_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=3, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(freq_content), word_count=len(freq_content.split()),
                    start_position=0, end_position=len(freq_content),
                    additional_info={
                        "chunk_type": "metadata_frequency_outliers",
                        "topic": "frequent_values_outliers"
                    }
                )
            ))
            
            # === CHUNK 5: CORRELA√á√ïES ENTRE VARI√ÅVEIS ===
            corr_content = f"""AN√ÅLISE DE CORRELA√á√ïES E RELACIONAMENTOS - DATASET: {source_id.upper()}

MATRIZ DE CORRELA√á√ÉO (Primeiras 15 colunas num√©ricas):
"""
            if len(numeric_cols) >= 2:
                corr_matrix = df[numeric_cols[:15]].corr()
                corr_content += corr_matrix.to_string()
                
                corr_content += "\n\nCORRELA√á√ïES FORTES (|r| > 0.7):\n"
                strong_corrs = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        corr_val = corr_matrix.iloc[i, j]
                        if abs(corr_val) > 0.7:
                            col1 = corr_matrix.columns[i]
                            col2 = corr_matrix.columns[j]
                            strong_corrs.append(f"  ‚Ä¢ {col1} <-> {col2}: {corr_val:.3f}")
                
                corr_content += "\n".join(strong_corrs) if strong_corrs else "  Nenhuma correla√ß√£o forte detectada.\n"
            else:
                corr_content += "  Dataset possui menos de 2 colunas num√©ricas para an√°lise de correla√ß√£o.\n"
            
            corr_content += "\n\nEste chunk apresenta a matriz de correla√ß√£o completa entre vari√°veis num√©ricas e destaca correla√ß√µes fortes (|r| > 0.7) indicando relacionamentos significativos entre vari√°veis."
            
            chunks.append(TextChunk(
                content=corr_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=4, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(corr_content), word_count=len(corr_content.split()),
                    start_position=0, end_position=len(corr_content),
                    additional_info={
                        "chunk_type": "metadata_correlations",
                        "topic": "correlations_relationships"
                    }
                )
            ))
            
            # === CHUNK 6: PADR√ïES TEMPORAIS E AGRUPAMENTOS ===
            pattern_content = f"""AN√ÅLISE DE PADR√ïES TEMPORAIS E AGRUPAMENTOS - DATASET: {source_id.upper()}

AN√ÅLISE TEMPORAL:
"""
            if datetime_cols:
                for col in datetime_cols:
                    pattern_content += f"\nColuna temporal: {col}\n"
                    pattern_content += f"  ‚Ä¢ Per√≠odo: {df[col].min()} at√© {df[col].max()}\n"
                    pattern_content += f"  ‚Ä¢ Intervalo: {(df[col].max() - df[col].min()).days} dias\n"
            elif 'Time' in df.columns or 'time' in df.columns:
                time_col = 'Time' if 'Time' in df.columns else 'time'
                pattern_content += f"\nColuna temporal detectada: {time_col}\n"
                pattern_content += f"  ‚Ä¢ Min: {df[time_col].min()}, Max: {df[time_col].max()}\n"
                pattern_content += f"  ‚Ä¢ Valores crescentes: {'Sim' if df[time_col].is_monotonic_increasing else 'N√£o'}\n"
            else:
                pattern_content += "  Nenhuma coluna temporal expl√≠cita detectada.\n"
            
            pattern_content += "\n\nAGRUPAMENTOS NATURAIS:\n"
            if categorical_cols:
                for col in categorical_cols[:3]:
                    groups = df[col].value_counts()
                    pattern_content += f"\n{col} - {len(groups)} grupos distintos:\n"
                    for group, count in groups.head(5).items():
                        pct = (count / total_rows) * 100
                        pattern_content += f"  ‚Ä¢ Grupo '{group}': {count} registros ({pct:.2f}%)\n"
            else:
                pattern_content += "  Dataset focado em vari√°veis cont√≠nuas sem agrupamentos categ√≥ricos √≥bvios.\n"
            
            pattern_content += "\n\nEste chunk analisa a presen√ßa de padr√µes temporais (se existirem colunas de data/tempo) e identifica agrupamentos naturais baseados em colunas categ√≥ricas com distribui√ß√£o de grupos."
            
            chunks.append(TextChunk(
                content=pattern_content,
                metadata=ChunkMetadata(
                    source=source_id, chunk_index=5, strategy=ChunkStrategy.CSV_ROW,
                    char_count=len(pattern_content), word_count=len(pattern_content.split()),
                    start_position=0, end_position=len(pattern_content),
                    additional_info={
                        "chunk_type": "metadata_patterns_clusters",
                        "topic": "temporal_patterns_clustering"
                    }
                )
            ))
            
            self.logger.info(f"‚úÖ Criados {len(chunks)} chunks anal√≠ticos de metadados para {source_id}")
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Falha ao gerar metadados para {source_id}: {e}")
            return []
        
        return chunks

    def ingest_csv_file(self,
                        file_path: str,
                        source_id: Optional[str] = None,
                        encoding: str = "utf-8",
                        errors: str = "ignore") -> Dict[str, Any]:
        """L√™ um arquivo CSV do disco e ingesta utilizando a estrat√©gia CSV_ROW.

        ‚ö†Ô∏è CONFORMIDADE: RAGAgent √© o AGENTE DE INGEST√ÉO AUTORIZADO.
        Este m√©todo tem permiss√£o para ler arquivos CSV diretamente.

        Args:
            file_path: Caminho absoluto ou relativo para o arquivo CSV.
            source_id: Identificador opcional para a fonte; usa o nome do arquivo se n√£o fornecido.
            encoding: Codifica√ß√£o utilizada para leitura do arquivo.
            errors: Pol√≠tica de tratamento de erros de decodifica√ß√£o.

        Returns:
            Resposta padr√£o do agente com estat√≠sticas do processamento.
        """
        path = Path(file_path)
        if not path.exists():
            message = f"Arquivo CSV n√£o encontrado: {file_path}"
            self.logger.error(message)
            return self._build_response(message, metadata={"error": True, "file_path": file_path})

        try:
            csv_text = path.read_text(encoding=encoding, errors=errors)
        except Exception as exc:
            message = f"Falha ao ler arquivo CSV '{file_path}': {exc}"
            self.logger.error(message)
            return self._build_response(
                message,
                metadata={"error": True, "file_path": file_path, "exception": str(exc)}
            )

        resolved_source_id = source_id or path.stem
        
        # ‚ö†Ô∏è CONFORMIDADE: Logging de acesso autorizado
        self.logger.info(f"‚úÖ INGEST√ÉO AUTORIZADA: RAGAgent lendo arquivo CSV: {file_path}")
        self.logger.info("‚úÖ CONFORMIDADE: Agente de ingest√£o tem permiss√£o para ler CSV")
        
        self.logger.info(
            "Iniciando ingest√£o do arquivo CSV",
            extra={"file_path": str(path.resolve()), "source_id": resolved_source_id}
        )

        return self.ingest_csv_data(csv_text=csv_text, source_id=resolved_source_id)
    
    async def process_with_search_memory(self, query: str, context: Optional[Dict[str, Any]] = None,
                                       session_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Processa consulta RAG com mem√≥ria de buscas e aprendizado de relev√¢ncia.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional
            session_id: ID da sess√£o
            
        Returns:
            Resposta contextualizada com otimiza√ß√£o baseada em hist√≥rico
        """
        import time
        start_time = time.time()
        
        try:
            # 1. Inicializar sess√£o de mem√≥ria se necess√°rio
            if session_id and self.has_memory:
                if not self._current_session_id or self._current_session_id != session_id:
                    await self.init_memory_session(session_id)
            elif not self._current_session_id and self.has_memory:
                await self.init_memory_session()
            
            # 2. Verificar cache de buscas similares
            search_key = self._generate_search_cache_key(query, context)
            cached_search = await self.recall_cached_search(search_key)
            
            if cached_search:
                self.logger.info(f"üîç Busca recuperada do cache: {search_key}")
                cached_search['metadata']['from_search_cache'] = True
                return cached_search
            
            # 3. Recuperar hist√≥rico de relev√¢ncia
            relevance_history = await self.recall_relevance_history()
            if relevance_history:
                self.logger.debug(f"üìä Aplicando hist√≥rico de relev√¢ncia: {len(relevance_history)} registros")
                context = context or {}
                context['relevance_history'] = relevance_history
            
            # 4. Ajustar threshold baseado em aprendizado
            similarity_threshold = self._adaptive_similarity_threshold(query, context)
            if context:
                context['similarity_threshold'] = similarity_threshold
            else:
                context = {'similarity_threshold': similarity_threshold}
            
            # 5. Processar consulta com otimiza√ß√µes
            result = self.process(query, context)
            
            # 6. Calcular tempo de processamento
            processing_time_ms = int((time.time() - start_time) * 1000)
            result.setdefault('metadata', {})['processing_time_ms'] = processing_time_ms
            
            # 7. Aprender relev√¢ncia dos resultados
            await self.learn_search_relevance(query, result)
            
            # 8. Cachear busca se significativa
            if self._should_cache_search(result, processing_time_ms):
                await self.cache_search_result(search_key, result, expiry_hours=6)
                self.logger.debug(f"üíæ Busca salva no cache: {search_key}")
            
            # 9. Salvar intera√ß√£o na mem√≥ria
            if self.has_memory and self._current_session_id:
                await self.remember_interaction(
                    query=query,
                    response=result.get('content', str(result)),
                    processing_time_ms=processing_time_ms,
                    metadata=result.get('metadata', {})
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"Erro no processamento RAG com mem√≥ria: {e}")
            # Fallback para processamento sem mem√≥ria
            return self.process(query, context)

    def process(self, query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Processa consulta RAG com busca vetorial e gera√ß√£o contextualizada.
        
        Args:
            query: Consulta do usu√°rio
            context: Contexto adicional (filtros, configura√ß√µes)
        
        Returns:
            Resposta contextualizada baseada na busca vetorial
        """
        self.logger.info(f"Processando consulta RAG: '{query[:50]}...'")
        start_time = time.perf_counter()
        
        try:
            # Configura√ß√µes da busca
            config = context or {}
            similarity_threshold = config.get('similarity_threshold', 0.3)
            max_results = config.get('max_results', 5)
            include_context = config.get('include_context', True)
            ingestion_id = config.get('ingestion_id')
            source_id = config.get('source_id')

            # 1. Gerar embedding da query
            self.logger.debug("Gerando embedding da consulta...")
            query_embedding_result = self.embedding_generator.generate_embedding(query)
            query_embedding = query_embedding_result.embedding

            # 2. Busca vetorial com filtro obrigat√≥rio por ingestion_id/source_id
            filters = {}
            if ingestion_id:
                filters['ingestion_id'] = ingestion_id
            if source_id:
                filters['source'] = source_id
            if not filters:
                self.logger.warning("Nenhum filtro de dataset ativo (ingestion_id/source_id) fornecido. Risco de contamina√ß√£o de contexto!")

            self.logger.debug(f"Executando busca vetorial (threshold={similarity_threshold}, filters={filters})")
            # Se n√£o houver filtro, n√£o retorna nada!
            if not filters:
                self.logger.error("Busca vetorial sem filtro de contexto! Retornando lista vazia para evitar contamina√ß√£o.")
                search_results = []
            else:
                search_results = self.vector_store.search_similar(
                    query_embedding=query_embedding,
                    similarity_threshold=similarity_threshold,
                    limit=max_results,
                    filters=filters
                )
            # 3. Construir contexto a partir dos resultados
            context_pieces = []
            source_info = {}
            for idx, result in enumerate(search_results, 1):
                chunk_content = result.chunk_text
                context_pieces.append(f"[Fonte: {result.source}, Similaridade: {result.similarity_score:.3f}]\n{chunk_content}")
                source = result.source
                if source not in source_info:
                    source_info[source] = {
                        "chunks": 0,
                        "avg_similarity": 0,
                        "max_similarity": 0
                    }
                source_info[source]["chunks"] += 1
                source_info[source]["max_similarity"] = max(source_info[source]["max_similarity"], result.similarity_score)

            # Calcular m√©dias de similaridade
            for source in source_info:
                source_results = [r for r in search_results if r.source == source]
                source_info[source]["avg_similarity"] = sum(r.similarity_score for r in source_results) / len(source_results)

            # 4. S√≠ntese da resposta via agente especializado
            from src.agent.rag_synthesis_agent import synthesize_response
            # Filtrar chunks para garantir que s√≥ colunas do source_id atual sejam usadas
            filtered_chunks = []
            for result in search_results:
                # Se o chunk cont√©m colunas do CSV antigo (ex: V1, V28, Time, Amount), ignore
                chunk_text_lower = result.chunk_text.lower()
                if any(col in chunk_text_lower for col in ["v1", "v28", "time", "amount", "class"]):
                    # Se o source_id do resultado n√£o for igual ao context['source_id'], ignore
                    if context and result.source != context.get("source_id"):
                        continue
                filtered_chunks.append(result.chunk_text)
            # Se n√£o houver chunks filtrados, use apenas os do source_id atual
            if not filtered_chunks and context and context.get("source_id"):
                filtered_chunks = [r.chunk_text for r in search_results if r.source == context["source_id"]]
            # Se LLM dispon√≠vel, use s√≠ntese via LangChain; sen√£o, fallback manual
            try:
                content = synthesize_response(filtered_chunks, query, use_llm=include_context)
                self.logger.info("‚úÖ Resposta consolidada gerada pelo agente de s√≠ntese")
            except Exception as e:
                self.logger.error(f"‚ùå Falha na s√≠ntese via LLM, usando fallback manual: {e}")
                content = synthesize_response(filtered_chunks, query, use_llm=False)
            
            processing_time = time.perf_counter() - start_time
            
            # Metadados da resposta
            metadata = {
                "query": query,
                "processing_time": processing_time,
                "search_results_count": len(search_results),
                "sources_found": list(source_info.keys()),
                "source_stats": source_info,
                "similarity_threshold": similarity_threshold,
                "embedding_provider": query_embedding_result.provider.value,
                "embedding_model": query_embedding_result.model,
                "rag_mode": "contextual" if include_context else "search_only"
            }
            
            self.logger.info(f"Consulta RAG conclu√≠da: {len(search_results)} resultados em {processing_time:.2f}s")
            
            return self._build_response(content, metadata=metadata)
            
        except Exception as e:
            self.logger.error(f"Erro no processamento RAG: {str(e)}")
            return self._build_response(
                f"Erro no processamento RAG: {str(e)}",
                metadata={"error": True, "query": query}
            )
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas da base de conhecimento."""
        try:
            stats = self.vector_store.get_collection_stats()
            
            response = f"""üìä **Estat√≠sticas da Base de Conhecimento**

üìà **Geral**
‚Ä¢ Total de embeddings: {stats.get('total_embeddings', 0):,}

üìÅ **Por Fonte**
{self._format_stats_dict(stats.get('sources', {}))}

üîß **Por Provider**
{self._format_stats_dict(stats.get('providers', {}))}

ü§ñ **Por Modelo**
{self._format_stats_dict(stats.get('models', {}))}"""
            
            return self._build_response(response, metadata=stats)
            
        except Exception as e:
            self.logger.error(f"Erro ao obter estat√≠sticas: {str(e)}")
            return self._build_response(
                f"Erro ao obter estat√≠sticas: {str(e)}",
                metadata={"error": True}
            )
    
    # ========================================================================
    # M√âTODOS DE MEM√ìRIA ESPEC√çFICOS PARA RAG
    # ========================================================================
    
    def _generate_search_cache_key(self, query: str, context: Optional[Dict[str, Any]]) -> str:
        """Gera chave √∫nica para cache de busca."""
        import hashlib
        
        # Normaliza query para cache
        normalized_query = query.lower().strip()
        
        # Adiciona par√¢metros relevantes de busca
        search_params = ""
        if context:
            relevant_params = {
                'similarity_threshold': context.get('similarity_threshold', 0.7),
                'max_results': context.get('max_results', 5)
            }
            search_params = str(sorted(relevant_params.items()))
        
        # Gera hash
        cache_input = f"{normalized_query}_{search_params}"
        return f"search_{hashlib.md5(cache_input.encode()).hexdigest()[:12]}"
    
    def _should_cache_search(self, result: Dict[str, Any], processing_time_ms: int) -> bool:
        """Determina se uma busca deve ser cacheada."""
        # Cachear se:
        # 1. Busca demorada (> 1000ms)
        # 2. Encontrou resultados relevantes
        # 3. N√£o √© erro
        
        if result.get('metadata', {}).get('error', False):
            return False
        
        if processing_time_ms > 1000:
            return True
        
        metadata = result.get('metadata', {})
        search_results_count = metadata.get('search_results_count', 0)
        
        return search_results_count > 0
    
    async def cache_search_result(self, search_key: str, result: Dict[str, Any], 
                                expiry_hours: int = 6) -> None:
        """Salva resultado de busca no cache."""
        if not self.has_memory or not self._current_session_id:
            return
        
        try:
            await self.remember_analysis_result(search_key, result, expiry_hours)
            self.logger.debug(f"Resultado de busca cacheado: {search_key}")
        except Exception as e:
            self.logger.debug(f"Erro ao cachear busca: {e}")
    
    async def recall_cached_search(self, search_key: str) -> Optional[Dict[str, Any]]:
        """Recupera resultado de busca do cache."""
        if not self.has_memory or not self._current_session_id:
            return None
        
        try:
            cached_result = await self.recall_cached_analysis(search_key)
            if cached_result:
                self.logger.debug(f"Busca recuperada do cache: {search_key}")
            return cached_result
        except Exception as e:
            self.logger.debug(f"Erro ao recuperar busca cacheada: {e}")
            return None
    
    async def learn_search_relevance(self, query: str, result: Dict[str, Any]) -> None:
        """Aprende relev√¢ncia de buscas para otimiza√ß√£o futura."""
        if not self.has_memory or not self._current_session_id:
            return
        
        try:
            metadata = result.get('metadata', {})
            search_results_count = metadata.get('search_results_count', 0)
            avg_similarity = metadata.get('avg_similarity', 0.0)
            
            # Extrai caracter√≠sticas da busca
            relevance_data = {
                'query_length': len(query),
                'query_words': len(query.split()),
                'search_results_count': search_results_count,
                'avg_similarity': avg_similarity,
                'processing_time_ms': metadata.get('processing_time_ms', 0),
                'success': search_results_count > 0,
                'timestamp': time.time()
            }
            
            # Salva dados de relev√¢ncia
            relevance_key = f"relevance_{int(time.time())}"
            context_key = f"search_relevance_{relevance_key}"
            
            await self.remember_data_context(relevance_data, context_key)
            
            self.logger.debug(f"Relev√¢ncia de busca aprendida: {relevance_key}")
            
        except Exception as e:
            self.logger.debug(f"Erro ao aprender relev√¢ncia: {e}")
    
    async def recall_relevance_history(self) -> List[Dict[str, Any]]:
        """Recupera hist√≥rico de relev√¢ncia de buscas."""
        if not self.has_memory or not self._current_session_id:
            return []
        
        try:
            # Recupera contexto de relev√¢ncia
            context = await self.recall_conversation_context(hours=72)  # 3 dias
            
            relevance_history = []
            for key, data in context.get('data_context', {}).items():
                if key.startswith('search_relevance_'):
                    relevance_history.append(data)
            
            # Ordena por timestamp (mais recente primeiro)
            relevance_history.sort(key=lambda x: x.get('timestamp', 0), reverse=True)
            
            return relevance_history[:50]  # √öltimos 50 registros
            
        except Exception as e:
            self.logger.debug(f"Erro ao recuperar hist√≥rico de relev√¢ncia: {e}")
            return []
    
    def _adaptive_similarity_threshold(self, query: str, context: Optional[Dict[str, Any]]) -> float:
        """Calcula threshold de similaridade adaptativo baseado no hist√≥rico."""
        base_threshold = context.get('similarity_threshold', 0.7) if context else 0.7
        
        # Se n√£o h√° mem√≥ria, usa base
        if not self.has_memory or not self._current_session_id:
            return base_threshold
        
        try:
            # Recupera hist√≥rico de relev√¢ncia do cache local se dispon√≠vel
            relevance_history = self._relevance_scores.get('recent_searches', [])
            
            if not relevance_history:
                return base_threshold
            
            # Calcula estat√≠sticas de sucesso por threshold
            successful_searches = [r for r in relevance_history if r.get('success', False)]
            
            if not successful_searches:
                return base_threshold
            
            # Calcula threshold m√©dio de buscas bem-sucedidas
            avg_successful_similarity = sum(r.get('avg_similarity', 0.7) for r in successful_searches) / len(successful_searches)
            
            # Ajusta threshold baseado na taxa de sucesso
            success_rate = len(successful_searches) / len(relevance_history)
            
            if success_rate > 0.8:
                # Alta taxa de sucesso - pode ser mais restritivo
                adjusted_threshold = min(base_threshold + 0.1, avg_successful_similarity + 0.05)
            elif success_rate < 0.5:
                # Baixa taxa de sucesso - ser mais permissivo
                adjusted_threshold = max(base_threshold - 0.1, 0.5)
            else:
                # Taxa m√©dia - usar m√©dia das buscas bem-sucedidas
                adjusted_threshold = (base_threshold + avg_successful_similarity) / 2
            
            self.logger.debug(f"Threshold adaptativo: {base_threshold:.3f} ‚Üí {adjusted_threshold:.3f} (taxa sucesso: {success_rate:.1%})")
            
            return round(adjusted_threshold, 3)
            
        except Exception as e:
            self.logger.debug(f"Erro no threshold adaptativo: {e}")
            return base_threshold
    
    def _format_stats_dict(self, stats_dict: Dict[str, int]) -> str:
        """Formata dicion√°rio de estat√≠sticas."""
        if not stats_dict:
            return "‚Ä¢ Nenhum dado dispon√≠vel"
        
        formatted = []
        for key, count in sorted(stats_dict.items(), key=lambda x: x[1], reverse=True):
            formatted.append(f"‚Ä¢ {key}: {count:,}")
        
        return "\n".join(formatted)
    
    def clear_source(self, source_id: str) -> Dict[str, Any]:
        """Remove todos os embeddings de uma fonte espec√≠fica."""
        try:
            deleted_count = self.vector_store.delete_embeddings_by_source(source_id)
            
            if deleted_count > 0:
                message = f"‚úÖ Removidos {deleted_count:,} embeddings da fonte '{source_id}'"
            else:
                message = f"‚ÑπÔ∏è Nenhum embedding encontrado para a fonte '{source_id}'"
            
            return self._build_response(
                message,
                metadata={"source_id": source_id, "deleted_count": deleted_count}
            )
            
        except Exception as e:
            self.logger.error(f"Erro ao limpar fonte {source_id}: {str(e)}")
            return self._build_response(
                f"Erro ao limpar fonte: {str(e)}",
                metadata={"error": True, "source_id": source_id}
            )

    def _enrich_csv_chunks_simple(self, chunks: List[TextChunk]) -> List[TextChunk]:
        """Vers√£o simplificada do enriquecimento de chunks CSV."""
        enriched_chunks: List[TextChunk] = []

        for chunk in chunks:
            try:
                lines = chunk.content.strip().split('\n')
                if len(lines) < 2:  # Precisa pelo menos do header + 1 linha
                    enriched_chunks.append(chunk)
                    continue
                
                header = lines[0]
                data_lines = lines[1:]
                
                info = chunk.metadata.additional_info or {}
                start_row = info.get("start_row", "desconhecido")
                end_row = info.get("end_row", "desconhecido")
                
                # Detectar nome do arquivo CSV do metadata
                csv_filename = chunk.metadata.source or "dataset.csv"
                if not csv_filename.endswith('.csv'):
                    csv_filename = "dataset.csv"
                
                # Criar descri√ß√£o textual simples e gen√©rica
                summary = f"Dados do dataset {csv_filename} (linhas {start_row} a {end_row})\n"
                summary += f"Total de {len(data_lines)} registros\n"
                summary += f"Colunas: {header}\n"
                summary += f"Primeiras linhas como exemplo:\n"
                
                # Incluir algumas linhas de exemplo
                for i, line in enumerate(data_lines[:3]):
                    summary += f"  Linha {i+1}: {line}\n"
                
                enriched_chunks.append(TextChunk(content=summary, metadata=chunk.metadata))
                
            except Exception as exc:
                self.logger.warning("Erro no enriquecimento simples CSV: %s", exc)
                enriched_chunks.append(chunk)
                
        return enriched_chunks

    @staticmethod
    def _format_value(value: Any) -> str:
        """Formata valores num√©ricos para texto compacto."""
        if isinstance(value, (float, int)):
            return f"{value:.3f}" if isinstance(value, float) else str(value)
        return str(value)