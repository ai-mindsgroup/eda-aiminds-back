"""
üîç SANDBOX MONITOR - Sistema de Monitoramento de Execu√ß√µes

Coleta m√©tricas detalhadas de cada execu√ß√£o do sandbox seguro e
persiste no Supabase para an√°lise hist√≥rica.

M√©tricas coletadas:
- Tempo de execu√ß√£o (ms)
- Uso de mem√≥ria (MB)
- Status (sucesso/falha)
- Tipo de erro
- C√≥digo executado (hash)
- Timestamp

Autor: GitHub Copilot (GPT-4.1)
Data: 2025-10-17
"""

import time
import hashlib
import json
import psutil
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum

from src.utils.logging_config import get_logger
from src.vectorstore.supabase_client import supabase

logger = get_logger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ENUMS E DATACLASSES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class ExecutionStatus(Enum):
    """Status de execu√ß√£o do sandbox."""
    SUCCESS = "success"
    FAILURE = "failure"
    TIMEOUT = "timeout"
    MEMORY_EXCEEDED = "memory_exceeded"
    COMPILATION_ERROR = "compilation_error"
    RUNTIME_ERROR = "runtime_error"


@dataclass
class SandboxMetrics:
    """M√©tricas de uma execu√ß√£o do sandbox."""
    
    # Identifica√ß√£o
    execution_id: str
    timestamp: datetime
    
    # C√≥digo executado
    code_hash: str
    code_length: int
    
    # Resultado
    status: ExecutionStatus
    success: bool
    
    # Performance
    execution_time_ms: float
    memory_used_mb: float
    memory_peak_mb: float
    
    # Configura√ß√£o
    timeout_limit_s: int
    memory_limit_mb: int
    
    # Erro (se houver)
    error_type: Optional[str] = None
    error_message: Optional[str] = None
    
    # Metadata adicional
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio para persist√™ncia."""
        data = asdict(self)
        data['status'] = self.status.value
        data['timestamp'] = self.timestamp.isoformat()
        return data
    
    def to_json(self) -> str:
        """Converte para JSON."""
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# METRICS COLLECTOR
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class MetricsCollector:
    """
    Coletor de m√©tricas para execu√ß√µes do sandbox.
    
    Uso:
        collector = MetricsCollector()
        
        # No in√≠cio da execu√ß√£o
        collector.start_execution(code="resultado = 42")
        
        # No fim da execu√ß√£o
        metrics = collector.end_execution(
            result={'success': True, 'result': 42, ...}
        )
    """
    
    def __init__(self):
        """Inicializa coletor de m√©tricas."""
        self.start_time: Optional[float] = None
        self.start_memory: Optional[float] = None
        self.code: Optional[str] = None
        self.execution_id: Optional[str] = None
        self.process = psutil.Process()
    
    def start_execution(self, code: str, execution_id: Optional[str] = None) -> str:
        """
        Inicia coleta de m√©tricas para uma execu√ß√£o.
        
        Args:
            code: C√≥digo Python a ser executado
            execution_id: ID customizado (opcional)
            
        Returns:
            ID da execu√ß√£o
        """
        self.start_time = time.time()
        self.code = code
        
        # Mem√≥ria inicial
        try:
            mem_info = self.process.memory_info()
            self.start_memory = mem_info.rss / (1024 * 1024)  # MB
        except:
            self.start_memory = 0.0
        
        # Gerar execution_id se n√£o fornecido
        if execution_id is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            code_hash = hashlib.sha256(code.encode()).hexdigest()[:8]
            self.execution_id = f"exec_{timestamp}_{code_hash}"
        else:
            self.execution_id = execution_id
        
        logger.debug(f"üìä Iniciando coleta de m√©tricas: {self.execution_id}")
        
        return self.execution_id
    
    def end_execution(
        self,
        result: Dict[str, Any],
        timeout_limit_s: int = 10,
        memory_limit_mb: int = 200,
        metadata: Optional[Dict[str, Any]] = None
    ) -> SandboxMetrics:
        """
        Finaliza coleta de m√©tricas e cria objeto SandboxMetrics.
        
        Args:
            result: Resultado da execu√ß√£o do sandbox
            timeout_limit_s: Limite de timeout configurado
            memory_limit_mb: Limite de mem√≥ria configurado
            metadata: Metadata adicional
            
        Returns:
            Objeto SandboxMetrics completo
        """
        if self.start_time is None:
            raise RuntimeError("start_execution() n√£o foi chamado")
        
        # Calcular tempo de execu√ß√£o
        execution_time_ms = (time.time() - self.start_time) * 1000
        
        # Mem√≥ria final e pico
        try:
            mem_info = self.process.memory_info()
            memory_current = mem_info.rss / (1024 * 1024)  # MB
            memory_peak = memory_current  # Simplificado
            memory_used = max(0, memory_current - self.start_memory)
        except:
            memory_current = 0.0
            memory_peak = 0.0
            memory_used = 0.0
        
        # Determinar status
        if result.get('success'):
            status = ExecutionStatus.SUCCESS
        elif result.get('error_type') == 'TimeoutError':
            status = ExecutionStatus.TIMEOUT
        elif result.get('error_type') == 'MemoryLimitError':
            status = ExecutionStatus.MEMORY_EXCEEDED
        elif result.get('error_type') == 'CompilationError':
            status = ExecutionStatus.COMPILATION_ERROR
        else:
            status = ExecutionStatus.RUNTIME_ERROR
        
        # Criar objeto de m√©tricas
        metrics = SandboxMetrics(
            execution_id=self.execution_id,
            timestamp=datetime.now(),
            code_hash=hashlib.sha256(self.code.encode()).hexdigest(),
            code_length=len(self.code),
            status=status,
            success=result.get('success', False),
            execution_time_ms=execution_time_ms,
            memory_used_mb=memory_used,
            memory_peak_mb=memory_peak,
            timeout_limit_s=timeout_limit_s,
            memory_limit_mb=memory_limit_mb,
            error_type=result.get('error_type'),
            error_message=result.get('error'),
            metadata=metadata or {}
        )
        
        logger.info(
            f"üìä M√©tricas coletadas: {self.execution_id} | "
            f"Status: {status.value} | "
            f"Tempo: {execution_time_ms:.2f}ms | "
            f"Mem√≥ria: {memory_used:.2f}MB"
        )
        
        # Reset
        self.start_time = None
        self.start_memory = None
        self.code = None
        self.execution_id = None
        
        return metrics


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SANDBOX MONITOR
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SandboxMonitor:
    """
    Monitor principal do sandbox seguro.
    
    Responsabilidades:
    - Coletar m√©tricas de execu√ß√µes
    - Persistir m√©tricas no Supabase
    - Fornecer estat√≠sticas agregadas
    - Detectar padr√µes an√¥malos
    
    Uso:
        monitor = SandboxMonitor()
        
        # Registrar execu√ß√£o
        monitor.record_execution(
            code="resultado = 42",
            result={'success': True, 'result': 42, ...}
        )
        
        # Obter estat√≠sticas
        stats = monitor.get_statistics(period_hours=24)
    """
    
    def __init__(self, enable_persistence: bool = True):
        """
        Inicializa monitor.
        
        Args:
            enable_persistence: Se True, persiste m√©tricas no Supabase
        """
        self.enable_persistence = enable_persistence
        self.metrics_buffer: List[SandboxMetrics] = []
        self.buffer_size = 100  # Flush ap√≥s 100 m√©tricas
        
        logger.info("üîç SandboxMonitor inicializado")
    
    def record_execution(
        self,
        code: str,
        result: Dict[str, Any],
        timeout_limit_s: int = 10,
        memory_limit_mb: int = 200,
        metadata: Optional[Dict[str, Any]] = None,
        flush_immediately: bool = False
    ) -> SandboxMetrics:
        """
        Registra uma execu√ß√£o do sandbox.
        
        Args:
            code: C√≥digo executado
            result: Resultado da execu√ß√£o
            timeout_limit_s: Limite de timeout
            memory_limit_mb: Limite de mem√≥ria
            metadata: Metadata adicional
            flush_immediately: Se True, persiste imediatamente
            
        Returns:
            Objeto SandboxMetrics
        """
        # Criar coletor
        collector = MetricsCollector()
        collector.start_execution(code)
        
        # Coletar m√©tricas
        metrics = collector.end_execution(
            result=result,
            timeout_limit_s=timeout_limit_s,
            memory_limit_mb=memory_limit_mb,
            metadata=metadata
        )
        
        # Adicionar ao buffer
        self.metrics_buffer.append(metrics)
        
        # Flush se necess√°rio
        if flush_immediately or len(self.metrics_buffer) >= self.buffer_size:
            self.flush_metrics()
        
        return metrics
    
    def flush_metrics(self) -> int:
        """
        Persiste m√©tricas do buffer no Supabase.
        
        Returns:
            N√∫mero de m√©tricas persistidas
        """
        if not self.enable_persistence or not self.metrics_buffer:
            return 0
        
        try:
            # Preparar dados para inser√ß√£o
            records = [m.to_dict() for m in self.metrics_buffer]
            
            # Inserir no Supabase
            result = supabase.table('sandbox_metrics').insert(records).execute()
            
            count = len(self.metrics_buffer)
            logger.info(f"üíæ {count} m√©tricas persistidas no Supabase")
            
            # Limpar buffer
            self.metrics_buffer.clear()
            
            return count
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao persistir m√©tricas: {e}")
            return 0
    
    def get_statistics(
        self,
        period_hours: int = 24,
        filters: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Obt√©m estat√≠sticas agregadas do Supabase.
        
        Args:
            period_hours: Per√≠odo em horas para an√°lise
            filters: Filtros adicionais
            
        Returns:
            Dicion√°rio com estat√≠sticas
        """
        try:
            # Calcular timestamp inicial
            start_time = datetime.now() - timedelta(hours=period_hours)
            
            # Buscar m√©tricas do per√≠odo
            query = supabase.table('sandbox_metrics')\
                .select('*')\
                .gte('timestamp', start_time.isoformat())
            
            if filters:
                for key, value in filters.items():
                    query = query.eq(key, value)
            
            result = query.execute()
            metrics = result.data
            
            if not metrics:
                return {
                    'period_hours': period_hours,
                    'total_executions': 0,
                    'success_rate': 0.0,
                    'avg_execution_time_ms': 0.0,
                    'avg_memory_used_mb': 0.0
                }
            
            # Calcular estat√≠sticas
            total = len(metrics)
            successes = sum(1 for m in metrics if m['success'])
            
            stats = {
                'period_hours': period_hours,
                'total_executions': total,
                'success_rate': (successes / total) * 100,
                'failure_rate': ((total - successes) / total) * 100,
                'avg_execution_time_ms': sum(m['execution_time_ms'] for m in metrics) / total,
                'max_execution_time_ms': max(m['execution_time_ms'] for m in metrics),
                'avg_memory_used_mb': sum(m['memory_used_mb'] for m in metrics) / total,
                'max_memory_used_mb': max(m['memory_used_mb'] for m in metrics),
                'error_distribution': self._get_error_distribution(metrics),
                'status_distribution': self._get_status_distribution(metrics)
            }
            
            logger.info(
                f"üìà Estat√≠sticas ({period_hours}h): "
                f"{total} execu√ß√µes, "
                f"{stats['success_rate']:.1f}% sucesso"
            )
            
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter estat√≠sticas: {e}")
            return {}
    
    def _get_error_distribution(self, metrics: List[Dict]) -> Dict[str, int]:
        """Calcula distribui√ß√£o de tipos de erro."""
        distribution = {}
        for m in metrics:
            if not m['success'] and m.get('error_type'):
                error_type = m['error_type']
                distribution[error_type] = distribution.get(error_type, 0) + 1
        return distribution
    
    def _get_status_distribution(self, metrics: List[Dict]) -> Dict[str, int]:
        """Calcula distribui√ß√£o de status."""
        distribution = {}
        for m in metrics:
            status = m['status']
            distribution[status] = distribution.get(status, 0) + 1
        return distribution
    
    def detect_anomalies(
        self,
        period_hours: int = 1,
        failure_threshold: float = 10.0,
        timeout_threshold: int = 3
    ) -> List[Dict[str, Any]]:
        """
        Detecta padr√µes an√¥malos nas execu√ß√µes.
        
        Args:
            period_hours: Per√≠odo para an√°lise
            failure_threshold: Taxa de falha (%) para alertar
            timeout_threshold: N√∫mero de timeouts consecutivos para alertar
            
        Returns:
            Lista de anomalias detectadas
        """
        anomalies = []
        
        try:
            stats = self.get_statistics(period_hours=period_hours)
            
            if not stats or stats['total_executions'] == 0:
                return anomalies
            
            # Anomalia 1: Taxa de falha alta
            if stats['failure_rate'] > failure_threshold:
                anomalies.append({
                    'type': 'high_failure_rate',
                    'severity': 'HIGH',
                    'message': f"Taxa de falha {stats['failure_rate']:.1f}% excede limiar de {failure_threshold}%",
                    'value': stats['failure_rate'],
                    'threshold': failure_threshold
                })
            
            # Anomalia 2: M√∫ltiplos timeouts
            timeouts = stats['error_distribution'].get('TimeoutError', 0)
            if timeouts >= timeout_threshold:
                anomalies.append({
                    'type': 'excessive_timeouts',
                    'severity': 'MEDIUM',
                    'message': f"{timeouts} timeouts detectados em {period_hours}h",
                    'value': timeouts,
                    'threshold': timeout_threshold
                })
            
            # Anomalia 3: Uso excessivo de mem√≥ria
            if stats.get('max_memory_used_mb', 0) > 150:
                anomalies.append({
                    'type': 'high_memory_usage',
                    'severity': 'MEDIUM',
                    'message': f"Pico de mem√≥ria {stats['max_memory_used_mb']:.1f}MB detectado",
                    'value': stats['max_memory_used_mb'],
                    'threshold': 150
                })
            
            if anomalies:
                logger.warning(f"‚ö†Ô∏è {len(anomalies)} anomalia(s) detectada(s)")
            
            return anomalies
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao detectar anomalias: {e}")
            return []
    
    def __del__(self):
        """Flush m√©tricas ao destruir o objeto."""
        if self.metrics_buffer:
            self.flush_metrics()


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXEMPLO DE USO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

if __name__ == '__main__':
    # Criar monitor
    monitor = SandboxMonitor()
    
    # Simular execu√ß√£o bem-sucedida
    result_success = {
        'success': True,
        'result': 42,
        'execution_time_ms': 15.5,
        'error': None,
        'error_type': None
    }
    
    monitor.record_execution(
        code="resultado = 2 + 2",
        result=result_success,
        metadata={'source': 'test'}
    )
    
    # Simular execu√ß√£o com erro
    result_error = {
        'success': False,
        'result': None,
        'execution_time_ms': 2.3,
        'error': 'Import bloqueado',
        'error_type': 'CompilationError'
    }
    
    monitor.record_execution(
        code="import os",
        result=result_error,
        metadata={'source': 'test'}
    )
    
    # Obter estat√≠sticas
    stats = monitor.get_statistics(period_hours=1)
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # Detectar anomalias
    anomalies = monitor.detect_anomalies(period_hours=1)
    print(f"\nAnomalias: {len(anomalies)}")
