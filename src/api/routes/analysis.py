"""
Rotas para An√°lises Avan√ßadas
=============================

Endpoints para an√°lises especializadas e visualiza√ß√µes.
"""

import time
import uuid
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import JSONResponse

from src.agent.orchestrator_agent import OrchestratorAgent
from src.api.schemas import BaseResponse, SystemStats
from src.utils.logging_config import get_logger

logger = get_logger(__name__)
router = APIRouter()


@router.get(
    "/stats",
    response_model=SystemStats,
    summary="Estat√≠sticas do sistema",
    description="Estat√≠sticas gerais de uso do sistema",
)
async def get_system_stats():
    """
    Estat√≠sticas gerais do sistema.
    
    Retorna m√©tricas de uso, performance e dados armazenados.
    """
    try:
        from src.vectorstore.supabase_client import supabase
        
        # Contar embeddings
        embeddings_result = supabase.table('embeddings').select('id', count='exact').execute()
        embeddings_count = embeddings_result.count or 0
        
        # Contar an√°lises (metadados)
        metadata_result = supabase.table('metadata').select('id', count='exact').execute()
        analyses_count = metadata_result.count or 0
        
        # Simular outras m√©tricas (em produ√ß√£o, seria real)
        stats = SystemStats(
            total_files_processed=50,  # TODO: Implementar contador real
            total_analyses_performed=analyses_count,
            total_queries_answered=150,  # TODO: Implementar contador real
            active_sessions=len(getattr(router, '_chat_sessions', {})),
            database_size_mb=25.5,  # TODO: Calcular tamanho real
            vectorstore_embeddings=embeddings_count,
        )
        
        return stats
        
    except Exception as e:
        logger.error(f"‚ùå Erro obtendo estat√≠sticas: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno obtendo estat√≠sticas"
        )


@router.post(
    "/correlation",
    summary="An√°lise de correla√ß√£o",
    description="Analisa correla√ß√µes entre vari√°veis dos dados",
)
async def correlation_analysis(
    file_id: str,
    variables: Optional[List[str]] = None,
    correlation_method: str = "pearson",
):
    """
    An√°lise de correla√ß√£o entre vari√°veis.
    
    **M√©todos dispon√≠veis:**
    - **pearson**: Correla√ß√£o de Pearson (linear)
    - **spearman**: Correla√ß√£o de Spearman (n√£o-linear)
    - **kendall**: Correla√ß√£o de Kendall (ordinal)
    """
    start_time = time.time()
    
    try:
        # Importar cache de arquivos
        from src.api.routes.csv import _file_cache
        
        if file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado"
            )
        
        file_data = _file_cache[file_id]
        df = file_data['dataframe']
        
        logger.info(f"üìä An√°lise de correla√ß√£o: {file_id} (m√©todo: {correlation_method})")
        
        # Preparar contexto
        context = {
            'dataframe': df,
            'analysis_type': 'correlation',
            'variables': variables,
            'correlation_method': correlation_method,
        }
        
        # Executar an√°lise
        orchestrator = OrchestratorAgent(enable_csv_agent=True)
        query = f"Analise as correla√ß√µes entre as vari√°veis usando o m√©todo {correlation_method}"
        
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return {
            'success': True,
            'message': 'An√°lise de correla√ß√£o conclu√≠da',
            'analysis_id': f"corr_{uuid.uuid4().hex[:12]}",
            'result': result.get('content', 'An√°lise conclu√≠da'),
            'processing_time_ms': processing_time,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise de correla√ß√£o: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na an√°lise de correla√ß√£o"
        )


@router.post(
    "/clustering",
    summary="An√°lise de clustering",
    description="Agrupa dados similares usando algoritmos de clustering",
)
async def clustering_analysis(
    file_id: str,
    n_clusters: int = 3,
    algorithm: str = "kmeans",
    features: Optional[List[str]] = None,
):
    """
    An√°lise de clustering (agrupamento).
    
    **Algoritmos dispon√≠veis:**
    - **kmeans**: K-Means clustering
    - **dbscan**: DBSCAN clustering
    - **hierarchical**: Clustering hier√°rquico
    """
    start_time = time.time()
    
    try:
        from src.api.routes.csv import _file_cache
        
        if file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado"
            )
        
        file_data = _file_cache[file_id]
        df = file_data['dataframe']
        
        logger.info(f"üéØ An√°lise de clustering: {file_id} (algoritmo: {algorithm}, clusters: {n_clusters})")
        
        context = {
            'dataframe': df,
            'analysis_type': 'clustering',
            'n_clusters': n_clusters,
            'algorithm': algorithm,
            'features': features,
        }
        
        orchestrator = OrchestratorAgent(enable_csv_agent=True)
        query = f"Execute clustering com {algorithm} para {n_clusters} grupos"
        
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return {
            'success': True,
            'message': f'Clustering {algorithm} conclu√≠do',
            'analysis_id': f"cluster_{uuid.uuid4().hex[:12]}",
            'result': result.get('content', 'An√°lise conclu√≠da'),
            'n_clusters': n_clusters,
            'algorithm': algorithm,
            'processing_time_ms': processing_time,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro no clustering: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na an√°lise de clustering"
        )


@router.post(
    "/time-series",
    summary="An√°lise de s√©ries temporais",
    description="Analisa tend√™ncias e padr√µes temporais nos dados",
)
async def time_series_analysis(
    file_id: str,
    date_column: str,
    value_column: str,
    frequency: str = "D",
    forecast_periods: int = 30,
):
    """
    An√°lise de s√©ries temporais.
    
    **Par√¢metros:**
    - **date_column**: Coluna com datas
    - **value_column**: Coluna com valores a analisar
    - **frequency**: Frequ√™ncia dos dados (D=di√°rio, M=mensal, H=hor√°rio)
    - **forecast_periods**: Per√≠odos para previs√£o
    """
    start_time = time.time()
    
    try:
        from src.api.routes.csv import _file_cache
        
        if file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado"
            )
        
        file_data = _file_cache[file_id]
        df = file_data['dataframe']
        
        # Validar colunas
        if date_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Coluna de data '{date_column}' n√£o encontrada"
            )
        
        if value_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Coluna de valor '{value_column}' n√£o encontrada"
            )
        
        logger.info(f"üìà An√°lise temporal: {file_id} ({date_column} vs {value_column})")
        
        context = {
            'dataframe': df,
            'analysis_type': 'time_series',
            'date_column': date_column,
            'value_column': value_column,
            'frequency': frequency,
            'forecast_periods': forecast_periods,
        }
        
        orchestrator = OrchestratorAgent(enable_csv_agent=True)
        query = f"Analise a s√©rie temporal de {value_column} ao longo de {date_column}"
        
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return {
            'success': True,
            'message': 'An√°lise de s√©ries temporais conclu√≠da',
            'analysis_id': f"timeseries_{uuid.uuid4().hex[:12]}",
            'result': result.get('content', 'An√°lise conclu√≠da'),
            'date_column': date_column,
            'value_column': value_column,
            'forecast_periods': forecast_periods,
            'processing_time_ms': processing_time,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise temporal: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na an√°lise temporal"
        )


@router.post(
    "/anomaly-detection",
    summary="Detec√ß√£o de anomalias",
    description="Identifica valores at√≠picos e anomalias nos dados",
)
async def anomaly_detection(
    file_id: str,
    target_column: str,
    method: str = "isolation_forest",
    contamination: float = 0.1,
):
    """
    Detec√ß√£o de anomalias.
    
    **M√©todos dispon√≠veis:**
    - **isolation_forest**: Isolation Forest
    - **one_class_svm**: One-Class SVM
    - **local_outlier_factor**: Local Outlier Factor
    - **statistical**: M√©todos estat√≠sticos (Z-score, IQR)
    """
    start_time = time.time()
    
    try:
        from src.api.routes.csv import _file_cache
        
        if file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado"
            )
        
        file_data = _file_cache[file_id]
        df = file_data['dataframe']
        
        if target_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Coluna '{target_column}' n√£o encontrada"
            )
        
        logger.info(f"üîç Detec√ß√£o de anomalias: {file_id} (coluna: {target_column}, m√©todo: {method})")
        
        context = {
            'dataframe': df,
            'analysis_type': 'anomaly_detection',
            'target_column': target_column,
            'method': method,
            'contamination': contamination,
        }
        
        orchestrator = OrchestratorAgent(enable_csv_agent=True)
        query = f"Detecte anomalias na coluna {target_column} usando {method}"
        
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        return {
            'success': True,
            'message': f'Detec√ß√£o de anomalias conclu√≠da usando {method}',
            'analysis_id': f"anomaly_{uuid.uuid4().hex[:12]}",
            'result': result.get('content', 'An√°lise conclu√≠da'),
            'target_column': target_column,
            'method': method,
            'contamination': contamination,
            'processing_time_ms': processing_time,
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na detec√ß√£o de anomalias: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na detec√ß√£o de anomalias"
        )


@router.get(
    "/history",
    summary="Hist√≥rico de an√°lises",
    description="Lista an√°lises realizadas recentemente",
)
async def get_analysis_history(
    limit: int = Query(20, ge=1, le=100, description="N√∫mero m√°ximo de an√°lises"),
    analysis_type: Optional[str] = Query(None, description="Filtrar por tipo de an√°lise"),
):
    """
    Hist√≥rico de an√°lises realizadas.
    
    Lista as an√°lises mais recentes com informa√ß√µes b√°sicas.
    """
    try:
        # Em implementa√ß√£o real, buscar do banco de dados
        # Por enquanto, retornar estrutura simulada
        
        analyses = [
            {
                'analysis_id': f"analysis_{i}",
                'type': 'fraud_detection',
                'file_name': f'transactions_{i}.csv',
                'created_at': time.time() - (i * 3600),  # Cada uma 1h mais antiga
                'status': 'completed',
                'processing_time_ms': 1500 + (i * 100),
            }
            for i in range(1, min(limit + 1, 21))
        ]
        
        # Filtrar por tipo se especificado
        if analysis_type:
            analyses = [a for a in analyses if a['type'] == analysis_type]
        
        return {
            'success': True,
            'message': f'{len(analyses)} an√°lise(s) encontrada(s)',
            'analyses': analyses,
            'total_count': len(analyses),
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro obtendo hist√≥rico: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno obtendo hist√≥rico"
        )