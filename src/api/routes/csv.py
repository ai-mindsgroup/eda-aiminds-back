"""
Rotas para An√°lise de CSV
========================

Endpoints para upload, processamento e an√°lise de arquivos CSV.
"""

import io
import os
import time
import uuid
from typing import Any, Dict, List, Optional

import pandas as pd
from fastapi import APIRouter, File, Form, HTTPException, UploadFile, status
from fastapi.responses import JSONResponse

from src.agent.orchestrator_agent import OrchestratorAgent
from src.api.schemas import (
    AnalysisRequest,
    AnalysisResponse,
    CSVUploadResponse,
    FraudDetectionRequest,
    FraudDetectionResponse,
    SchemaExamples,
)
from src.utils.logging_config import get_logger

logger = get_logger(__name__)
router = APIRouter()

# Cache tempor√°rio para arquivos (em produ√ß√£o usar Redis ou S3)
_file_cache: Dict[str, Dict[str, Any]] = {}


@router.post(
    "/upload",
    response_model=CSVUploadResponse,
    summary="Upload de arquivo CSV",
    description="Faz upload de um arquivo CSV e retorna informa√ß√µes b√°sicas sobre os dados",
    responses={
        200: {"description": "Upload realizado com sucesso", "model": CSVUploadResponse},
        400: {"description": "Arquivo inv√°lido ou erro no processamento"},
        413: {"description": "Arquivo muito grande"},
        422: {"description": "Formato de arquivo n√£o suportado"},
    },
)
async def upload_csv(
    file: UploadFile = File(..., description="Arquivo CSV para upload"),
    delimiter: str = Form(",", description="Delimitador do CSV"),
    encoding: str = Form("utf-8", description="Codifica√ß√£o do arquivo"),
    has_header: bool = Form(True, description="Se o arquivo possui cabe√ßalho"),
    preview_rows: int = Form(5, description="N√∫mero de linhas para preview"),
):
    """
    Upload de arquivo CSV com valida√ß√£o e an√°lise inicial.
    
    **Par√¢metros:**
    - **file**: Arquivo CSV (m√°x 100MB)
    - **delimiter**: Separador de colunas (v√≠rgula, ponto-e-v√≠rgula, tab)
    - **encoding**: Codifica√ß√£o (utf-8, latin1, cp1252)
    - **has_header**: Se a primeira linha cont√©m nomes das colunas
    - **preview_rows**: Quantas linhas mostrar no preview
    
    **Retorna:**
    - ID √∫nico do arquivo para usar em an√°lises
    - Informa√ß√µes sobre estrutura dos dados
    - Preview das primeiras linhas
    - Tipos de dados detectados
    """
    start_time = time.time()
    
    try:
        # Valida√ß√µes b√°sicas
        if not file.filename:
            raise HTTPException(
                status_code=400,
                detail="Nome do arquivo √© obrigat√≥rio"
            )
        
        if not file.filename.lower().endswith('.csv'):
            raise HTTPException(
                status_code=422,
                detail="Apenas arquivos CSV s√£o suportados"
            )
        
        # Verificar tamanho do arquivo
        content = await file.read()
        file_size = len(content)
        
        if file_size > 100 * 1024 * 1024:  # 100MB
            raise HTTPException(
                status_code=413,
                detail="Arquivo muito grande. M√°ximo: 100MB"
            )
        
        if file_size == 0:
            raise HTTPException(
                status_code=400,
                detail="Arquivo est√° vazio"
            )
        
        logger.info(f"üìÅ Processando upload: {file.filename} ({file_size} bytes)")
        
        # Processar CSV
        try:
            # Ler CSV com pandas
            df = pd.read_csv(
                io.StringIO(content.decode(encoding)),
                delimiter=delimiter,
                header=0 if has_header else None,
                nrows=None,  # Ler arquivo completo
            )
            
            # Validar dados
            if df.empty:
                raise HTTPException(
                    status_code=400,
                    detail="Arquivo CSV est√° vazio ou mal formatado"
                )
            
            # Gerar ID √∫nico para o arquivo
            file_id = f"csv_{uuid.uuid4().hex[:12]}"
            
            # Preparar preview
            preview_df = df.head(preview_rows)
            preview = preview_df.to_dict('records')
            
            # Detectar tipos de dados
            data_types = {}
            for col in df.columns:
                dtype = str(df[col].dtype)
                if dtype.startswith('int'):
                    data_types[col] = 'integer'
                elif dtype.startswith('float'):
                    data_types[col] = 'float'
                elif dtype == 'bool':
                    data_types[col] = 'boolean'
                elif 'datetime' in dtype:
                    data_types[col] = 'datetime'
                else:
                    data_types[col] = 'text'
            
            # Armazenar no cache tempor√°rio
            _file_cache[file_id] = {
                'filename': file.filename,
                'dataframe': df,
                'file_size': file_size,
                'uploaded_at': time.time(),
                'metadata': {
                    'delimiter': delimiter,
                    'encoding': encoding,
                    'has_header': has_header,
                }
            }
            
            processing_time = int((time.time() - start_time) * 1000)
            
            logger.info(
                f"‚úÖ Upload conclu√≠do: {file_id} - {len(df)} linhas, {len(df.columns)} colunas - {processing_time}ms"
            )
            
            return CSVUploadResponse(
                success=True,
                message=f"Arquivo {file.filename} carregado com sucesso",
                file_id=file_id,
                filename=file.filename,
                file_size=file_size,
                rows_count=len(df),
                columns_count=len(df.columns),
                columns=list(df.columns),
                preview=preview,
                data_types=data_types,
            )
            
        except UnicodeDecodeError:
            raise HTTPException(
                status_code=400,
                detail=f"Erro de codifica√ß√£o. Tente usar: latin1 ou cp1252"
            )
        
        except pd.errors.ParserError as e:
            raise HTTPException(
                status_code=400,
                detail=f"Erro ao processar CSV: {str(e)}"
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro no upload: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno no processamento do arquivo"
        )


@router.post(
    "/analyze",
    response_model=AnalysisResponse,
    summary="An√°lise geral de dados",
    description="Executa an√°lise estat√≠stica e insights sobre o arquivo CSV",
)
async def analyze_csv(request: AnalysisRequest):
    """
    An√°lise abrangente de dados CSV.
    
    **Tipos de an√°lise dispon√≠veis:**
    - **statistical**: Estat√≠sticas descritivas b√°sicas
    - **correlation**: An√°lise de correla√ß√µes entre vari√°veis
    - **clustering**: Agrupamento de dados similares
    - **anomaly_detection**: Detec√ß√£o de valores at√≠picos
    - **time_series**: An√°lise temporal (se houver coluna de data)
    - **custom**: An√°lise customizada com par√¢metros espec√≠ficos
    """
    start_time = time.time()
    
    try:
        # Verificar se arquivo existe
        if request.file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado. Fa√ßa upload primeiro."
            )
        
        file_data = _file_cache[request.file_id]
        df = file_data['dataframe']
        
        logger.info(f"üîç Iniciando an√°lise {request.analysis_type} para {request.file_id}")
        
        # Inicializar orquestrador
        orchestrator = OrchestratorAgent(
            enable_csv_agent=True,
            enable_rag_agent=False,  # N√£o necess√°rio para an√°lise CSV
            enable_google_llm=True,
        )
        
        # Preparar contexto para an√°lise
        context = {
            'dataframe': df,
            'analysis_type': request.analysis_type,
            'target_column': request.target_column,
            'include_visualizations': request.include_visualizations,
            'parameters': request.parameters or {},
            'file_metadata': file_data['metadata'],
        }
        
        # Construir query baseada no tipo de an√°lise
        queries = {
            'statistical': f"Fa√ßa uma an√°lise estat√≠stica completa dos dados em {file_data['filename']}",
            'correlation': f"Analise as correla√ß√µes entre as vari√°veis nos dados de {file_data['filename']}",
            'clustering': f"Identifique grupos e padr√µes nos dados de {file_data['filename']}",
            'anomaly_detection': f"Detecte valores at√≠picos e anomalias em {file_data['filename']}",
            'time_series': f"Analise tend√™ncias temporais nos dados de {file_data['filename']}",
            'fraud_detection': f"Analise padr√µes suspeitos e poss√≠veis fraudes em {file_data['filename']}",
            'custom': request.parameters.get('custom_query', f"Analise os dados de {file_data['filename']}"),
        }
        
        query = queries.get(request.analysis_type, queries['statistical'])
        
        # Executar an√°lise via orquestrador
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        # Preparar resposta estruturada
        analysis_result = {
            'analysis_id': f"analysis_{uuid.uuid4().hex[:12]}",
            'analysis_type': request.analysis_type,
            'summary': result.get('content', 'An√°lise conclu√≠da'),
            'insights': _extract_insights(result),
            'statistics': result.get('statistics', {}),
            'visualizations': result.get('visualizations', []) if request.include_visualizations else [],
            'confidence_score': result.get('confidence_score', 0.8),
            'processing_time_ms': processing_time,
            'recommendations': _extract_recommendations(result),
        }
        
        logger.info(f"‚úÖ An√°lise conclu√≠da: {analysis_result['analysis_id']} - {processing_time}ms")
        
        return AnalysisResponse(
            success=True,
            message=f"An√°lise {request.analysis_type} conclu√≠da com sucesso",
            result=analysis_result,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na an√°lise dos dados"
        )


@router.post(
    "/fraud-detection",
    response_model=FraudDetectionResponse,
    summary="Detec√ß√£o de fraudes",
    description="An√°lise especializada para detec√ß√£o de fraudes em transa√ß√µes",
)
async def detect_fraud(request: FraudDetectionRequest):
    """
    Detec√ß√£o especializada de fraudes usando LLMs.
    
    **Algoritmos utilizados:**
    - An√°lise de padr√µes com LLMs
    - Detec√ß√£o de anomalias estat√≠sticas
    - An√°lise temporal de transa√ß√µes
    - Identifica√ß√£o de comportamentos suspeitos
    
    **Par√¢metros importantes:**
    - **amount_column**: Coluna com valores das transa√ß√µes
    - **user_column**: Coluna de identifica√ß√£o do usu√°rio
    - **timestamp_column**: Coluna de data/hora das transa√ß√µes
    - **threshold**: Sensibilidade da detec√ß√£o (0.0 = mais sens√≠vel, 1.0 = menos sens√≠vel)
    """
    start_time = time.time()
    
    try:
        # Verificar se arquivo existe
        if request.file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado. Fa√ßa upload primeiro."
            )
        
        file_data = _file_cache[request.file_id]
        df = file_data['dataframe']
        
        # Validar colunas obrigat√≥rias
        if request.amount_column not in df.columns:
            raise HTTPException(
                status_code=400,
                detail=f"Coluna '{request.amount_column}' n√£o encontrada no arquivo"
            )
        
        logger.info(f"üîç Iniciando detec√ß√£o de fraudes para {request.file_id}")
        
        # Preparar dados para an√°lise de fraude
        context = {
            'dataframe': df,
            'amount_column': request.amount_column,
            'user_column': request.user_column,
            'timestamp_column': request.timestamp_column,
            'threshold': request.threshold,
            'analysis_type': 'fraud_detection',
        }
        
        # Inicializar orquestrador com foco em LLM
        orchestrator = OrchestratorAgent(
            enable_csv_agent=True,
            enable_rag_agent=True,  # Para buscar padr√µes conhecidos de fraude
            enable_google_llm=True,
        )
        
        # Query especializada para detec√ß√£o de fraudes
        query = f"""
        Analise os dados de transa√ß√µes para detectar poss√≠veis fraudes.
        
        Foque em:
        - Transa√ß√µes com valores at√≠picos na coluna '{request.amount_column}'
        - Padr√µes suspeitos de comportamento
        - M√∫ltiplas transa√ß√µes pequenas em sequ√™ncia
        - Hor√°rios incomuns de transa√ß√£o
        - Varia√ß√µes bruscas no comportamento do usu√°rio
        
        Use threshold de {request.threshold} para classifica√ß√£o.
        """
        
        # Executar an√°lise
        result = orchestrator.process(query, context)
        
        processing_time = int((time.time() - start_time) * 1000)
        
        # Processar resultados espec√≠ficos de fraude
        fraud_result = _process_fraud_results(df, result, request)
        fraud_result['processing_time_ms'] = processing_time
        
        logger.info(f"‚úÖ Detec√ß√£o de fraudes conclu√≠da: {fraud_result['fraud_count']} fraudes detectadas - {processing_time}ms")
        
        return FraudDetectionResponse(
            success=True,
            message=f"An√°lise de fraudes conclu√≠da. {fraud_result['fraud_count']} poss√≠veis fraudes detectadas.",
            result=fraud_result,
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na detec√ß√£o de fraudes: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno na detec√ß√£o de fraudes"
        )


@router.get(
    "/files",
    summary="Listar arquivos carregados",
    description="Lista todos os arquivos CSV carregados na sess√£o atual",
)
async def list_uploaded_files():
    """Lista arquivos carregados na sess√£o atual."""
    try:
        files = []
        current_time = time.time()
        
        for file_id, file_data in _file_cache.items():
            df = file_data['dataframe']
            uploaded_ago = current_time - file_data['uploaded_at']
            
            files.append({
                'file_id': file_id,
                'filename': file_data['filename'],
                'file_size': file_data['file_size'],
                'rows_count': len(df),
                'columns_count': len(df.columns),
                'uploaded_ago_seconds': int(uploaded_ago),
                'columns': list(df.columns)[:10],  # Primeiras 10 colunas
            })
        
        return {
            'success': True,
            'message': f'{len(files)} arquivo(s) carregado(s)',
            'files': files,
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erro listando arquivos: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno listando arquivos"
        )


@router.delete(
    "/files/{file_id}",
    summary="Remover arquivo",
    description="Remove arquivo carregado da mem√≥ria",
)
async def delete_file(file_id: str):
    """Remove arquivo carregado da mem√≥ria."""
    try:
        if file_id not in _file_cache:
            raise HTTPException(
                status_code=404,
                detail="Arquivo n√£o encontrado"
            )
        
        filename = _file_cache[file_id]['filename']
        del _file_cache[file_id]
        
        logger.info(f"üóëÔ∏è Arquivo removido: {file_id} ({filename})")
        
        return {
            'success': True,
            'message': f'Arquivo {filename} removido com sucesso',
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro removendo arquivo: {e}")
        raise HTTPException(
            status_code=500,
            detail="Erro interno removendo arquivo"
        )


# ============================================================================
# FUN√á√ïES AUXILIARES
# ============================================================================

def _extract_insights(result: Dict[str, Any]) -> List[str]:
    """Extrai insights do resultado da an√°lise."""
    insights = []
    
    # Tentar extrair insights do conte√∫do
    content = result.get('content', '')
    
    # Procurar por padr√µes de insights
    if 'insight' in content.lower():
        lines = content.split('\n')
        for line in lines:
            if 'insight' in line.lower() or line.strip().startswith('- '):
                insights.append(line.strip('- ').strip())
    
    # Insights padr√£o se n√£o encontrar
    if not insights:
        insights = [
            "An√°lise conclu√≠da com sucesso",
            "Dados processados completamente",
            "Padr√µes identificados nos dados",
        ]
    
    return insights[:5]  # M√°ximo 5 insights


def _extract_recommendations(result: Dict[str, Any]) -> List[str]:
    """Extrai recomenda√ß√µes do resultado."""
    recommendations = []
    
    content = result.get('content', '')
    
    # Procurar por recomenda√ß√µes
    if 'recomenda' in content.lower():
        lines = content.split('\n')
        for line in lines:
            if 'recomenda' in line.lower() or line.strip().startswith('- '):
                recommendations.append(line.strip('- ').strip())
    
    # Recomenda√ß√µes padr√£o
    if not recommendations:
        recommendations = [
            "Continue monitorando os padr√µes identificados",
            "Considere an√°lises adicionais para insights mais profundos",
            "Valide os resultados com especialistas do dom√≠nio",
        ]
    
    return recommendations[:5]


def _process_fraud_results(df: pd.DataFrame, result: Dict[str, Any], request: FraudDetectionRequest) -> Dict[str, Any]:
    """Processa resultados espec√≠ficos de detec√ß√£o de fraude."""
    
    # An√°lise b√°sica dos dados
    total_transactions = len(df)
    amount_col = request.amount_column
    
    # Detectar outliers b√°sicos (valores muito altos)
    if amount_col in df.columns:
        q75 = df[amount_col].quantile(0.75)
        q25 = df[amount_col].quantile(0.25)
        iqr = q75 - q25
        upper_bound = q75 + 1.5 * iqr
        
        # Transa√ß√µes suspeitas (valores muito altos)
        suspicious_high = df[df[amount_col] > upper_bound]
        
        # Transa√ß√µes muito pequenas (poss√≠vel lavagem)
        mean_amount = df[amount_col].mean()
        suspicious_low = df[df[amount_col] < mean_amount * 0.1]
        
        # Combinar suspeitas
        fraud_count = len(suspicious_high) + len(suspicious_low)
        fraud_percentage = (fraud_count / total_transactions) * 100
        
        # Preparar transa√ß√µes fraudulentas
        fraud_transactions = []
        
        # Adicionar algumas transa√ß√µes suspeitas
        for _, row in suspicious_high.head(10).iterrows():
            fraud_transactions.append({
                'transaction_id': row.get('id', len(fraud_transactions)),
                'amount': float(row[amount_col]),
                'reason': 'Valor muito alto (outlier)',
                'risk_score': 0.8,
            })
        
        for _, row in suspicious_low.head(5).iterrows():
            fraud_transactions.append({
                'transaction_id': row.get('id', len(fraud_transactions)),
                'amount': float(row[amount_col]),
                'reason': 'Valor muito baixo (poss√≠vel lavagem)',
                'risk_score': 0.6,
            })
    
    else:
        fraud_count = 0
        fraud_percentage = 0.0
        fraud_transactions = []
    
    return {
        'total_transactions': total_transactions,
        'fraud_count': min(fraud_count, total_transactions),
        'fraud_percentage': round(fraud_percentage, 2),
        'suspicious_patterns': [
            {
                'pattern': 'Valores at√≠picos',
                'description': 'Transa√ß√µes com valores muito altos ou baixos',
                'occurrences': fraud_count,
            }
        ],
        'risk_analysis': {
            'overall_risk': 'medium' if fraud_percentage > 5 else 'low',
            'risk_factors': ['Outliers detectados', 'Varia√ß√£o de valores'],
            'confidence': 0.75,
        },
        'fraud_transactions': fraud_transactions[:20],  # M√°ximo 20
        'recommendations': [
            'Revisar transa√ß√µes com valores at√≠picos',
            'Implementar limites din√¢micos baseados no perfil do usu√°rio',
            'Monitorar padr√µes de hor√°rio das transa√ß√µes',
            'Configurar alertas para valores acima do percentil 95',
        ],
    }