"""Script para modificar RAGAgent.ingest_csv_data() para ingest√£o DUAL (metadata + ROW + COLUMN)."""

import sys

def modify_rag_agent_dual_chunking():
    """Modifica o m√©todo ingest_csv_data() para suportar chunking duplo."""
    
    file_path = r"c:\workstashion\eda-aiminds-i2a2-rb\src\agent\rag_agent.py"
    
    # Ler arquivo
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # String a ser substitu√≠da (inicio)
    old_code_start = '# ‚úÖ GERAR APENAS CHUNKS DE METADADOS ANAL√çTICOS'
    old_code_end = 'return self._build_response(response, metadata=stats)'
    
    # Nova implementa√ß√£o
    new_code = '''# ‚úÖ ESTRAT√âGIA DUAL: GERAR CHUNKS DE METADATA + ROW + COLUMN
            all_chunks = []
            
            # 1. CHUNKS DE METADADOS ANAL√çTICOS (6 chunks estruturados)
            self.logger.info("üìä Gerando chunks de metadados anal√≠ticos...")
            metadata_chunks = self._generate_metadata_chunks(csv_text, source_id)
            
            if not metadata_chunks:
                self.logger.warning("‚ö†Ô∏è Nenhum chunk de metadata foi criado")
            else:
                self.logger.info(f"‚úÖ {len(metadata_chunks)} chunks de metadados criados")
                all_chunks.extend(metadata_chunks)
            
            # 2. CHUNKS POR LINHA (CSV_ROW strategy) - Dados completos linha a linha
            self.logger.info("üìù Gerando chunks por LINHA (CSV_ROW)...")
            from src.embeddings.chunker import ChunkStrategy
            row_chunks = self.chunker.chunk_text(
                text=csv_text,
                source_id=source_id,
                strategy=ChunkStrategy.CSV_ROW
            )
            
            if not row_chunks:
                self.logger.warning("‚ö†Ô∏è Nenhum chunk de linha foi criado")
            else:
                self.logger.info(f"‚úÖ {len(row_chunks)} chunks de linhas criados")
                all_chunks.extend(row_chunks)
            
            # 3. CHUNKS POR COLUNA (CSV_COLUMN strategy) - An√°lise focada por coluna
            self.logger.info("üìä Gerando chunks por COLUNA (CSV_COLUMN)...")
            column_chunks = self.chunker.chunk_text(
                text=csv_text,
                source_id=source_id,
                strategy=ChunkStrategy.CSV_COLUMN
            )
            
            if not column_chunks:
                self.logger.warning("‚ö†Ô∏è Nenhum chunk de coluna foi criado")
            else:
                self.logger.info(f"‚úÖ {len(column_chunks)} chunks de colunas criados")
                all_chunks.extend(column_chunks)
            
            # Validar se ao menos algum chunk foi criado
            if not all_chunks:
                return self._build_response(
                    "Nenhum chunk foi criado (metadata/row/column)",
                    metadata={"error": True}
                )
            
            self.logger.info(f"‚úÖ TOTAL: {len(all_chunks)} chunks criados (metadata={len(metadata_chunks)}, row={len(row_chunks)}, column={len(column_chunks)})")
            
            # Gerar embeddings para TODOS os chunks
            self.logger.info("üî¢ Gerando embeddings para TODOS os chunks...")
            all_embeddings = self.embedding_generator.generate_embeddings_batch(all_chunks)
            
            if not all_embeddings:
                return self._build_response(
                    "Falha ao gerar embeddings para chunks",
                    metadata={"error": True}
                )
            
            self.logger.info(f"‚úÖ {len(all_embeddings)} embeddings gerados")
            
            # Armazenar embeddings no vector store
            self.logger.info("üíæ Armazenando embeddings no vector store...")
            stored_ids = self.vector_store.store_embeddings(all_embeddings, "csv")
            
            processing_time = time.perf_counter() - start_time
            
            # Estat√≠sticas consolidadas
            stats = {
                "source_id": source_id,
                "source_type": "csv_dual_chunking",
                "processing_time": processing_time,
                "metadata_chunks_created": len(metadata_chunks),
                "row_chunks_created": len(row_chunks),
                "column_chunks_created": len(column_chunks),
                "total_chunks_created": len(all_chunks),
                "total_embeddings_generated": len(all_embeddings),
                "total_embeddings_stored": len(stored_ids),
                "success_rate": len(stored_ids) / len(all_chunks) * 100 if all_chunks else 0
            }
            
            response = f"‚úÖ Ingest√£o DUAL conclu√≠da para '{source_id}'\\n" \\
                      f"üìä Metadata: {len(metadata_chunks)} chunks\\n" \\
                      f"üìù Linhas: {len(row_chunks)} chunks\\n" \\
                      f"üìä Colunas: {len(column_chunks)} chunks\\n" \\
                      f"üî¢ TOTAL: {len(all_chunks)} chunks ‚Üí {len(all_embeddings)} embeddings ‚Üí {len(stored_ids)} armazenados\\n" \\
                      f"‚è±Ô∏è Processado em {processing_time:.2f}s"
            
            self.logger.info(f"‚úÖ Ingest√£o DUAL conclu√≠da: {stats['success_rate']:.1f}% sucesso")
            
            return self._build_response(response, metadata=stats)'''
    
    # Encontrar o bloco a ser substitu√≠do
    start_idx = content.find(old_code_start)
    if start_idx == -1:
        print(f"‚ùå N√£o encontrou o in√≠cio do c√≥digo: {old_code_start}")
        return False
    
    # Encontrar o final do bloco (ap√≥s a √∫ltima linha que queremos substituir)
    end_marker = 'return self._build_response(response, metadata=stats)'
    end_idx = content.find(end_marker, start_idx)
    if end_idx == -1:
        print(f"‚ùå N√£o encontrou o final do c√≥digo")
        return False
    
    # Incluir o texto do end_marker na substitui√ß√£o
    end_idx += len(end_marker)
    
    # Montar novo conte√∫do
    new_content = content[:start_idx] + new_code + content[end_idx:]
    
    # Salvar backup
    backup_path = file_path + '.backup_dual_chunking'
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(content)
    print(f"‚úÖ Backup salvo em: {backup_path}")
    
    # Salvar novo conte√∫do
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print(f"‚úÖ Arquivo modificado: {file_path}")
    print(f"üìä Implementa√ß√£o DUAL CHUNKING (metadata + ROW + COLUMN) adicionada!")
    return True

if __name__ == "__main__":
    success = modify_rag_agent_dual_chunking()
    sys.exit(0 if success else 1)
