#!/usr/bin/env python3
"""
Test Interface Interativa V3.0 - Valida√ß√£o Completa

‚úÖ Testa chunking multi-coluna (CSV_COLUMN + CSV_ROW + METADATA)
‚úÖ Valida uso ativo de LLMs em 9 pontos cr√≠ticos
‚úÖ Confirma c√≥digo gen√©rico (sem hardcoding)
‚úÖ Testa mem√≥ria persistente e contexto conversacional
‚úÖ Valida RAG vetorial com busca sem√¢ntica

Objetivo: Garantir que as modifica√ß√µes recentes (chunking por coluna,
remo√ß√£o de hardcoding, LLMs ativos) est√£o funcionando corretamente
na interface interativa.
"""

import sys
import asyncio
from pathlib import Path
from uuid import uuid4
from typing import Dict, Any, List

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent))

from src.agent.orchestrator_agent import OrchestratorAgent
from src.agent.rag_agent import RAGAgent
from src.vectorstore.supabase_client import supabase
from src.utils.logging_config import get_logger
from src.settings import EDA_DATA_DIR_PROCESSANDO

logger = get_logger(__name__)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FUN√á√ïES DE VALIDA√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def print_section(title: str):
    """Imprime cabe√ßalho de se√ß√£o."""
    print(f"\n{'‚ïê' * 70}")
    print(f"  {title}")
    print('‚ïê' * 70)


def validate_chunking_strategy(rag_agent: RAGAgent) -> Dict[str, Any]:
    """
    Valida se o chunking multi-coluna est√° ativo.
    
    Returns:
        Dict com resultados da valida√ß√£o
    """
    print_section("üîç TESTE 1: Valida√ß√£o de Chunking Multi-Coluna")
    
    results = {
        'test_name': 'Chunking Strategy',
        'passed': False,
        'details': {}
    }
    
    try:
        # Verificar se chunker tem estrat√©gia CSV_COLUMN
        from src.embeddings.chunker import ChunkingStrategy
        
        has_column_strategy = hasattr(ChunkingStrategy, 'CSV_COLUMN')
        has_row_strategy = hasattr(ChunkingStrategy, 'CSV_ROW')
        
        print(f"  ‚úì ChunkingStrategy.CSV_COLUMN existe: {has_column_strategy}")
        print(f"  ‚úì ChunkingStrategy.CSV_ROW existe: {has_row_strategy}")
        
        # Verificar m√©todo de chunking
        from src.embeddings.chunker import Chunker
        chunker = Chunker()
        
        has_method = hasattr(chunker, '_chunk_csv_by_columns')
        print(f"  ‚úì M√©todo _chunk_csv_by_columns existe: {has_method}")
        
        # Verificar se o c√≥digo √© gen√©rico (n√£o hardcoded)
        import inspect
        if has_method:
            source_code = inspect.getsource(chunker._chunk_csv_by_columns)
            
            # N√ÉO deve conter nomes de colunas espec√≠ficas
            hardcoded_terms = ['Time', 'Amount', 'Class', 'V1', 'V2']
            has_hardcoding = any(term in source_code for term in hardcoded_terms)
            
            # DEVE conter itera√ß√£o din√¢mica
            has_dynamic_iteration = 'for col in df.columns' in source_code or 'for idx, col in enumerate(df.columns' in source_code
            has_type_detection = 'is_numeric_dtype' in source_code or 'pd.api.types' in source_code
            
            print(f"  ‚úì C√≥digo sem hardcoding de colunas: {not has_hardcoding}")
            print(f"  ‚úì Itera√ß√£o din√¢mica (for col in df.columns): {has_dynamic_iteration}")
            print(f"  ‚úì Detec√ß√£o autom√°tica de tipos: {has_type_detection}")
            
            results['details'] = {
                'has_column_strategy': has_column_strategy,
                'has_row_strategy': has_row_strategy,
                'has_method': has_method,
                'is_generic': not has_hardcoding,
                'has_dynamic_iteration': has_dynamic_iteration,
                'has_type_detection': has_type_detection
            }
            
            # Teste passa se todas as condi√ß√µes s√£o verdadeiras
            results['passed'] = all([
                has_column_strategy,
                has_row_strategy,
                has_method,
                not has_hardcoding,
                has_dynamic_iteration,
                has_type_detection
            ])
        
        if results['passed']:
            print(f"\n  ‚úÖ TESTE 1 PASSOU: Chunking multi-coluna est√° gen√©rico e funcional!")
        else:
            print(f"\n  ‚ùå TESTE 1 FALHOU: Problemas detectados no chunking")
    
    except Exception as e:
        print(f"  ‚ùå Erro na valida√ß√£o: {e}")
        logger.error(f"Erro em validate_chunking_strategy: {e}", exc_info=True)
    
    return results


def validate_llm_usage() -> Dict[str, Any]:
    """
    Valida se LLMs est√£o ativos em pontos cr√≠ticos.
    
    Returns:
        Dict com resultados da valida√ß√£o
    """
    print_section("ü§ñ TESTE 2: Valida√ß√£o de Uso Ativo de LLMs")
    
    results = {
        'test_name': 'LLM Active Usage',
        'passed': False,
        'llm_points': []
    }
    
    try:
        # Lista dos 9 pontos cr√≠ticos onde LLMs devem estar ativos
        critical_points = [
            {
                'file': 'src/embeddings/generator.py',
                'class': 'EmbeddingGenerator',
                'method': 'embed_documents',
                'llm_type': 'OpenAIEmbeddings',
                'line_approx': 60
            },
            {
                'file': 'src/embeddings/generator.py',
                'class': 'EmbeddingGenerator',
                'method': 'embed_query',
                'llm_type': 'OpenAIEmbeddings',
                'line_approx': 95
            },
            {
                'file': 'src/agent/query_analyzer.py',
                'class': 'QueryAnalyzer',
                'method': 'analyze_query',
                'llm_type': 'LLMManager',
                'line_approx': 85
            },
            {
                'file': 'src/agent/hybrid_query_processor_v2.py',
                'class': 'HybridQueryProcessorV2',
                'method': '_process_embeddings_query',
                'llm_type': 'LLMManager',
                'line_approx': 311
            },
            {
                'file': 'src/agent/hybrid_query_processor_v2.py',
                'class': 'HybridQueryProcessorV2',
                'method': '_process_csv_direct_query',
                'llm_type': 'LLMManager',
                'line_approx': 507
            },
            {
                'file': 'src/agent/hybrid_query_processor_v2.py',
                'class': 'HybridQueryProcessorV2',
                'method': '_fallback_to_llm',
                'llm_type': 'LLMManager',
                'line_approx': 586
            },
            {
                'file': 'src/llm/fast_fragmenter.py',
                'class': 'FastQueryFragmenter',
                'method': 'fragment_query',
                'llm_type': 'LLMManager',
                'line_approx': 150
            },
            {
                'file': 'src/llm/simple_aggregator.py',
                'class': 'SimpleQueryAggregator',
                'method': 'aggregate_results',
                'llm_type': 'LLMManager',
                'line_approx': 85
            },
            {
                'file': 'src/agent/rag_agent.py',
                'class': 'RAGAgent',
                'method': 'query',
                'llm_type': 'LLMManager',
                'line_approx': 159
            }
        ]
        
        print(f"  Verificando {len(critical_points)} pontos cr√≠ticos de uso de LLM...\n")
        
        active_count = 0
        for idx, point in enumerate(critical_points, 1):
            try:
                file_path = Path(__file__).parent / point['file']
                
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        source_code = f.read()
                    
                    # Verificar se o m√©todo existe e usa LLM
                    has_method = f"def {point['method']}" in source_code
                    uses_llm_manager = 'llm_manager' in source_code or 'LLMManager' in source_code
                    uses_langchain = 'langchain' in source_code.lower()
                    
                    is_active = has_method and (uses_llm_manager or uses_langchain)
                    
                    status = "‚úÖ" if is_active else "‚ùå"
                    print(f"  {status} Ponto {idx}: {point['class']}.{point['method']} ({point['llm_type']})")
                    
                    if is_active:
                        active_count += 1
                    
                    results['llm_points'].append({
                        'index': idx,
                        'file': point['file'],
                        'method': point['method'],
                        'active': is_active
                    })
                else:
                    print(f"  ‚ö†Ô∏è  Ponto {idx}: Arquivo {point['file']} n√£o encontrado")
            
            except Exception as e:
                print(f"  ‚ùå Erro ao verificar ponto {idx}: {e}")
        
        # Teste passa se pelo menos 7 dos 9 pontos est√£o ativos
        results['active_count'] = active_count
        results['total_points'] = len(critical_points)
        results['passed'] = active_count >= 7
        
        print(f"\n  üìä Resultado: {active_count}/{len(critical_points)} pontos ativos")
        
        if results['passed']:
            print(f"  ‚úÖ TESTE 2 PASSOU: LLMs est√£o ativos em pontos cr√≠ticos!")
        else:
            print(f"  ‚ùå TESTE 2 FALHOU: Apenas {active_count}/{len(critical_points)} pontos ativos")
    
    except Exception as e:
        print(f"  ‚ùå Erro na valida√ß√£o: {e}")
        logger.error(f"Erro em validate_llm_usage: {e}", exc_info=True)
    
    return results


async def validate_ingestion_with_multicolumn(csv_file_path: str) -> Dict[str, Any]:
    """
    Valida ingest√£o de CSV com chunking multi-coluna.
    
    Args:
        csv_file_path: Caminho para arquivo CSV de teste
    
    Returns:
        Dict com resultados da valida√ß√£o
    """
    print_section("üì• TESTE 3: Valida√ß√£o de Ingest√£o com Multi-Coluna")
    
    results = {
        'test_name': 'Ingestion with Multi-Column Chunking',
        'passed': False,
        'details': {}
    }
    
    try:
        # Limpar base vetorial
        print("  ‚Üí Limpando base vetorial...")
        supabase.table('embeddings').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
        supabase.table('chunks').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
        supabase.table('metadata').delete().neq('id', '00000000-0000-0000-0000-000000000000').execute()
        
        # Verificar se arquivo existe
        csv_path = Path(csv_file_path)
        if not csv_path.exists():
            print(f"  ‚ö†Ô∏è  Arquivo {csv_file_path} n√£o encontrado")
            return results
        
        print(f"  ‚Üí Carregando CSV: {csv_path.name}")
        
        # Contar colunas do CSV
        import pandas as pd
        df = pd.read_csv(csv_path)
        num_columns = len(df.columns)
        num_rows = len(df)
        
        print(f"  ‚Üí CSV tem {num_columns} colunas e {num_rows} linhas")
        
        # Executar ingest√£o
        print("  ‚Üí Executando ingest√£o com RAGAgent...")
        rag_agent = RAGAgent()
        source_id = csv_path.stem
        
        ingest_result = rag_agent.ingest_csv_file(
            file_path=str(csv_path),
            source_id=source_id,
            encoding="utf-8"
        )
        
        print(f"  ‚Üí Ingest√£o conclu√≠da")
        
        # Verificar embeddings gerados
        print("  ‚Üí Verificando embeddings na base vetorial...")
        embeddings_response = supabase.table('embeddings').select('*').eq('source_id', source_id).execute()
        embeddings_count = len(embeddings_response.data) if embeddings_response.data else 0
        
        print(f"  ‚Üí Total de embeddings gerados: {embeddings_count}")
        
        # Verificar chunks
        chunks_response = supabase.table('chunks').select('*').eq('source_id', source_id).execute()
        chunks_count = len(chunks_response.data) if chunks_response.data else 0
        
        print(f"  ‚Üí Total de chunks gerados: {chunks_count}")
        
        # Verificar tipos de chunks (deve ter METADATA, ROW e COLUMN)
        chunk_types = set()
        if chunks_response.data:
            for chunk in chunks_response.data:
                metadata = chunk.get('metadata', {})
                chunk_type = metadata.get('chunk_type', 'unknown')
                chunk_types.add(chunk_type)
        
        print(f"  ‚Üí Tipos de chunks encontrados: {', '.join(chunk_types)}")
        
        # Calcular chunks esperados:
        # - 1 METADATA
        # - N ROWs (sample de ~100 linhas)
        # - M COLUMNs (todas as colunas)
        expected_column_chunks = num_columns
        has_metadata = 'METADATA' in chunk_types
        has_row = 'ROW' in chunk_types
        has_column = 'COLUMN' in chunk_types
        
        print(f"\n  Valida√ß√£o de tipos:")
        print(f"    ‚úì METADATA chunk: {has_metadata}")
        print(f"    ‚úì ROW chunks: {has_row}")
        print(f"    ‚úì COLUMN chunks: {has_column}")
        print(f"    ‚úì Chunks esperados de COLUMN: ~{expected_column_chunks}")
        
        results['details'] = {
            'csv_columns': num_columns,
            'csv_rows': num_rows,
            'embeddings_generated': embeddings_count,
            'chunks_generated': chunks_count,
            'chunk_types': list(chunk_types),
            'has_metadata': has_metadata,
            'has_row': has_row,
            'has_column': has_column
        }
        
        # Teste passa se:
        # - Embeddings foram gerados
        # - Existem chunks de todos os 3 tipos
        # - N√∫mero de embeddings √© razo√°vel
        results['passed'] = all([
            embeddings_count > 0,
            has_metadata,
            has_row,
            has_column,
            chunks_count >= num_columns  # Pelo menos 1 chunk por coluna
        ])
        
        if results['passed']:
            print(f"\n  ‚úÖ TESTE 3 PASSOU: Ingest√£o multi-coluna funcionando corretamente!")
        else:
            print(f"\n  ‚ùå TESTE 3 FALHOU: Problemas na ingest√£o multi-coluna")
    
    except Exception as e:
        print(f"  ‚ùå Erro na valida√ß√£o: {e}")
        logger.error(f"Erro em validate_ingestion_with_multicolumn: {e}", exc_info=True)
    
    return results


async def validate_query_with_memory(orchestrator: OrchestratorAgent, session_id: str) -> Dict[str, Any]:
    """
    Valida query com mem√≥ria persistente e contexto conversacional.
    
    Args:
        orchestrator: Inst√¢ncia do OrchestratorAgent
        session_id: ID da sess√£o de teste
    
    Returns:
        Dict com resultados da valida√ß√£o
    """
    print_section("üí¨ TESTE 4: Valida√ß√£o de Query com Mem√≥ria Persistente")
    
    results = {
        'test_name': 'Query with Persistent Memory',
        'passed': False,
        'queries': []
    }
    
    try:
        # Preparar queries de teste
        test_queries = [
            {
                'query': 'Quais s√£o as colunas dispon√≠veis no dataset?',
                'expected_in_response': ['coluna', 'vari√°vel', 'campo']
            },
            {
                'query': 'Me explique as 3 primeiras colunas',
                'expected_in_response': ['primeira', 'segunda', 'terceira', 'coluna']
            },
            {
                'query': 'Qual foi minha primeira pergunta?',
                'expected_in_response': ['primeira', 'pergunta', 'coluna']
            }
        ]
        
        print(f"  Executando {len(test_queries)} queries de teste...\n")
        
        for idx, test in enumerate(test_queries, 1):
            try:
                print(f"  Query {idx}: {test['query']}")
                
                # Executar query com mem√≥ria persistente
                response = await orchestrator.process_with_persistent_memory(
                    test['query'],
                    context={},
                    session_id=session_id
                )
                
                if response and response.get('content'):
                    content = response['content'].lower()
                    
                    # Verificar se resposta cont√©m termos esperados
                    matches = [term for term in test['expected_in_response'] if term in content]
                    has_expected_content = len(matches) > 0
                    
                    # Verificar metadata de mem√≥ria
                    metadata = response.get('metadata', {})
                    has_session_id = metadata.get('session_id') == session_id
                    has_memory_context = metadata.get('previous_interactions') is not None
                    
                    status = "‚úÖ" if has_expected_content else "‚ö†Ô∏è"
                    print(f"    {status} Resposta recebida (termos encontrados: {len(matches)})")
                    print(f"    {'‚úÖ' if has_session_id else '‚ùå'} Session ID presente")
                    print(f"    {'‚úÖ' if has_memory_context else '‚ùå'} Contexto de mem√≥ria presente")
                    
                    results['queries'].append({
                        'index': idx,
                        'query': test['query'],
                        'has_response': True,
                        'has_expected_content': has_expected_content,
                        'has_session_id': has_session_id,
                        'has_memory_context': has_memory_context,
                        'response_length': len(content)
                    })
                else:
                    print(f"    ‚ùå Sem resposta")
                    results['queries'].append({
                        'index': idx,
                        'query': test['query'],
                        'has_response': False
                    })
                
                # Aguardar entre queries
                await asyncio.sleep(2)
            
            except Exception as e:
                print(f"    ‚ùå Erro na query {idx}: {e}")
                results['queries'].append({
                    'index': idx,
                    'query': test['query'],
                    'error': str(e)
                })
        
        # Teste passa se pelo menos 2 das 3 queries funcionaram
        successful_queries = sum(1 for q in results['queries'] if q.get('has_response', False))
        results['successful_queries'] = successful_queries
        results['total_queries'] = len(test_queries)
        results['passed'] = successful_queries >= 2
        
        print(f"\n  üìä Resultado: {successful_queries}/{len(test_queries)} queries bem-sucedidas")
        
        if results['passed']:
            print(f"  ‚úÖ TESTE 4 PASSOU: Query com mem√≥ria funcionando!")
        else:
            print(f"  ‚ùå TESTE 4 FALHOU: Problemas nas queries com mem√≥ria")
    
    except Exception as e:
        print(f"  ‚ùå Erro na valida√ß√£o: {e}")
        logger.error(f"Erro em validate_query_with_memory: {e}", exc_info=True)
    
    return results


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# EXECU√á√ÉO PRINCIPAL
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

async def main():
    """Executa todos os testes de valida√ß√£o."""
    print("\n" + "‚ïê" * 70)
    print("  üß™ TESTE INTERFACE INTERATIVA V3.0 - VALIDA√á√ÉO COMPLETA")
    print("‚ïê" * 70)
    print("\n  Objetivo: Validar modifica√ß√µes recentes")
    print("    ‚Ä¢ Chunking multi-coluna (CSV_COLUMN + CSV_ROW + METADATA)")
    print("    ‚Ä¢ C√≥digo 100% gen√©rico (sem hardcoding)")
    print("    ‚Ä¢ LLMs ativos em 9 pontos cr√≠ticos")
    print("    ‚Ä¢ Mem√≥ria persistente e contexto conversacional")
    
    all_results = []
    
    # TESTE 1: Validar chunking
    result1 = validate_chunking_strategy(RAGAgent())
    all_results.append(result1)
    
    # TESTE 2: Validar LLMs
    result2 = validate_llm_usage()
    all_results.append(result2)
    
    # TESTE 3: Validar ingest√£o (precisa de CSV)
    csv_files = list(EDA_DATA_DIR_PROCESSANDO.glob("*.csv"))
    if csv_files:
        result3 = await validate_ingestion_with_multicolumn(str(csv_files[0]))
        all_results.append(result3)
    else:
        print_section("‚ö†Ô∏è  TESTE 3: PULADO (Nenhum CSV em data/processando/)")
    
    # TESTE 4: Validar query com mem√≥ria
    print_section("üîß Inicializando sistema para TESTE 4...")
    try:
        orchestrator = OrchestratorAgent(
            enable_csv_agent=True,
            enable_rag_agent=True,
            enable_data_processor=True
        )
        session_id = str(uuid4())
        
        result4 = await validate_query_with_memory(orchestrator, session_id)
        all_results.append(result4)
    except Exception as e:
        print(f"  ‚ùå Erro ao inicializar orchestrador: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # RELAT√ìRIO FINAL
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    print_section("üìä RELAT√ìRIO FINAL DE VALIDA√á√ÉO")
    
    total_tests = len(all_results)
    passed_tests = sum(1 for r in all_results if r.get('passed', False))
    
    print(f"\n  Total de testes: {total_tests}")
    print(f"  Testes aprovados: {passed_tests}")
    print(f"  Testes reprovados: {total_tests - passed_tests}")
    print(f"  Taxa de sucesso: {(passed_tests/total_tests*100):.1f}%\n")
    
    print("  Resumo por teste:")
    for idx, result in enumerate(all_results, 1):
        status = "‚úÖ PASSOU" if result.get('passed', False) else "‚ùå FALHOU"
        test_name = result.get('test_name', f'Teste {idx}')
        print(f"    {status} - {test_name}")
    
    # Avalia√ß√£o final
    print("\n" + "‚ïê" * 70)
    if passed_tests == total_tests:
        print("  üéâ TODOS OS TESTES PASSARAM! Sistema V3.0 validado com sucesso!")
    elif passed_tests >= total_tests * 0.75:
        print(f"  ‚úÖ MAIORIA DOS TESTES PASSOU ({passed_tests}/{total_tests})")
        print("     Sistema V3.0 funcional, mas com pontos de aten√ß√£o")
    else:
        print(f"  ‚ö†Ô∏è  ATEN√á√ÉO: Apenas {passed_tests}/{total_tests} testes passaram")
        print("     Revis√£o necess√°ria antes de usar em produ√ß√£o")
    print("‚ïê" * 70 + "\n")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Teste interrompido pelo usu√°rio.\n")
    except Exception as e:
        print(f"\n\n‚ùå Erro cr√≠tico no teste: {e}\n")
        logger.error(f"Erro cr√≠tico em test_interface_interativa_v3: {e}", exc_info=True)
