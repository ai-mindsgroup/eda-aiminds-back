# üîß C√ìDIGO PROPOSTO: MELHORIAS DE PROMPTS E PAR√ÇMETROS

**Sistema:** EDA AI Minds  
**Data:** 18 de Outubro de 2025  
**Status:** Ready for Implementation

---

## üìã √çNDICE

1. [Prompt V2 para Tipos de Dados](#1-prompt-v2-para-tipos-de-dados)
2. [Configura√ß√µes Centralizadas](#2-configura√ß√µes-centralizadas)
3. [Temperature Din√¢mica](#3-temperature-din√¢mica)
4. [Ajustes de Chunking](#4-ajustes-de-chunking)
5. [Thresholds Padronizados](#5-thresholds-padronizados)
6. [Testes Automatizados](#6-testes-automatizados)

---

## 1. PROMPT V2 PARA TIPOS DE DADOS

### Arquivo: `src/prompts/manager.py`

**Substituir o prompt `data_types_analysis` (linhas 176-195) por:**

```python
"data_types_analysis_v2": PromptTemplate(
    role=AgentRole.CSV_ANALYST,
    type=PromptType.INSTRUCTION,
    content="""üîç **AN√ÅLISE ABRANGENTE DE TIPOS DE DADOS**

Forne√ßa uma an√°lise completa e contextualizada dos tipos de dados no dataset.

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìä SE√á√ÉO 1: CLASSIFICA√á√ÉO T√âCNICA (obrigat√≥rio)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Liste os tipos de dados baseados nos dtypes do Pandas:

**Tipos Dispon√≠veis:**
- **Num√©ricos:** int8, int16, int32, int64, float16, float32, float64
- **Categ√≥ricos:** object (strings/texto), category
- **Temporais:** datetime64, timedelta64
- **Booleanos:** bool
- **Outros:** mixed types (se aplic√°vel)

**Formato:**
```
üìã Resumo T√©cnico:
- Num√©ricas (X): [lista das colunas]
- Categ√≥ricas (Y): [lista das colunas ou "Nenhuma"]
- Temporais (Z): [lista das colunas ou "Nenhuma"]
- Total: X + Y + Z = N colunas
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üîç SE√á√ÉO 2: CONTEXTO ANAL√çTICO (recomendado quando dados dispon√≠veis)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Para cada tipo de dado, adicione contexto relevante SE DISPON√çVEL nos chunks:

**Para Vari√°veis Num√©ricas:**
- Range de valores (min-max) se dispon√≠vel
- Presen√ßa de valores extremos ou outliers
- Indica√ß√£o de normaliza√ß√£o/transforma√ß√£o (ex: PCA, StandardScaler)
- Identifica√ß√£o de vari√°veis target ou IDs

**Para Vari√°veis Categ√≥ricas:**
- Cardinalidade (n√∫mero de valores √∫nicos) se dispon√≠vel
- Valores mais frequentes (top 3-5) se dispon√≠vel
- Indica√ß√£o de encoding aplicado

**Para Vari√°veis Temporais:**
- Range temporal (in√≠cio-fim) se dispon√≠vel
- Granularidade (segundos, dias, meses)
- Presen√ßa de gaps ou irregularidades

**Formato:**
```
üìä Detalhamento:

Vari√°veis Num√©ricas:
- ColA (dtype): Range [min-max], caracter√≠stica especial
- ColB (dtype): Range [min-max], caracter√≠stica especial
[...]

Vari√°veis Categ√≥ricas:
- ColX (dtype): N valores √∫nicos, top 3 valores
[...]
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üí° SE√á√ÉO 3: INSIGHTS E RECOMENDA√á√ïES (quando aplic√°vel)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Forne√ßa insights anal√≠ticos baseados nos dados:

**Poss√≠veis Insights:**
- Detec√ß√£o de vari√°veis target (ex: "Class" ou "target" com 2 valores)
- Identifica√ß√£o de features derivadas (PCA, embeddings)
- Desbalanceamento de classes (se target identificado)
- Transforma√ß√µes aplicadas (normaliza√ß√£o, encoding)
- Armadilhas potenciais (ex: dtype num√©rico mas semanticamente categ√≥rico)

**Recomenda√ß√µes de An√°lise:**
- Tratamentos necess√°rios (normaliza√ß√£o, encoding)
- T√©cnicas apropriadas (clustering, classifica√ß√£o)
- Cuidados especiais (desbalanceamento, outliers)

**Formato:**
```
üîç Insights:
- [Insight 1 baseado nos dados]
- [Insight 2 baseado nos dados]

üí° Recomenda√ß√µes:
- [Recomenda√ß√£o 1 para an√°lise]
- [Recomenda√ß√£o 2 para an√°lise]
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚öñÔ∏è PRINC√çPIOS DE EQUIL√çBRIO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

1. **Precis√£o T√©cnica:** Sempre forne√ßa classifica√ß√£o correta dos dtypes
2. **Contexto √ötil:** Adicione informa√ß√µes relevantes quando dispon√≠veis
3. **Sem Suposi√ß√µes:** N√£o invente dados n√£o presentes nos chunks
4. **Clareza:** Use linguagem acess√≠vel mas tecnicamente correta
5. **Concis√£o:** Seja completo mas evite verbosidade desnecess√°ria

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üìê DIRETRIZES DE RESPOSTA
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚úÖ **FA√áA:**
- Liste todos os tipos presentes no dataset
- Mencione caracter√≠sticas importantes (ranges, cardinalidade)
- Identifique vari√°veis especiais (target, ID, timestamp)
- Sugira an√°lises apropriadas para o tipo de dados

‚ùå **N√ÉO FA√áA:**
- Inventar estat√≠sticas n√£o presentes nos chunks
- Fornecer respostas gen√©ricas sobre conceitos de tipos de dados
- Fazer suposi√ß√µes n√£o fundamentadas nos dados
- Ignorar informa√ß√µes relevantes dispon√≠veis nos chunks

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

**Exemplo de Resposta Ideal:**

```
O dataset cont√©m 30 vari√°veis num√©ricas e nenhuma categ√≥rica:

üìã Resumo T√©cnico:
- Num√©ricas (30): Time, V1-V28, Amount, Class
- Categ√≥ricas (0): Nenhuma
- Total: 30 colunas

üìä Detalhamento:

Vari√°veis Num√©ricas Principais:
- Time (int64): Segundos desde primeira transa√ß√£o [0-172,792]
- V1-V28 (float64): Componentes principais de PCA (normalizados)
- Amount (float64): Valor da transa√ß√£o em euros [‚Ç¨0.00-‚Ç¨25,691.16]
- Class (int64): Vari√°vel target bin√°ria [0=leg√≠tima, 1=fraude]

üîç Insights:
- Todas as vari√°veis V1-V28 s√£o features an√¥nimas obtidas via PCA
- Class √© tecnicamente int64 mas semanticamente uma vari√°vel categ√≥rica bin√°ria
- Dataset altamente desbalanceado: 99.83% classe 0 (leg√≠timas), 0.17% classe 1 (fraudes)
- Amount apresenta distribui√ß√£o assim√©trica com presen√ßa de valores extremos

üí° Recomenda√ß√µes:
- Tratar Class como vari√°vel categ√≥rica nas an√°lises (apesar do dtype int64)
- Considerar normaliza√ß√£o ou transforma√ß√£o logar√≠tmica de Amount (alta skewness)
- Aplicar t√©cnicas de balanceamento para modelagem (SMOTE, undersampling, class weights)
- Usar features V1-V28 diretamente (j√° normalizadas via PCA)
```

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Baseie sua resposta EXCLUSIVAMENTE nos dados fornecidos e no contexto anal√≠tico recuperado.""",
    variables=[]
)
```

---

## 2. CONFIGURA√á√ïES CENTRALIZADAS

### Novo Arquivo: `src/config/llm_config.py`

```python
"""Configura√ß√µes Centralizadas para LLM e Busca Vetorial

Este m√≥dulo centraliza todas as configura√ß√µes relacionadas a:
- Par√¢metros LLM (temperature, max_tokens, top_p)
- Thresholds de similaridade vetorial
- Limites de busca e chunking
- Mapeamentos de temperatura por inten√ß√£o

Uso:
    from src.config.llm_config import LLM_CONFIGS, SIMILARITY_THRESHOLDS
    
    threshold = SIMILARITY_THRESHOLDS['primary_search']
    temp = get_temperature_for_intent(AnalysisIntent.STATISTICAL)
"""

from typing import Dict, Any
from dataclasses import dataclass
from enum import Enum


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PAR√ÇMETROS LLM PADR√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class LLMParameters:
    """Par√¢metros padr√£o para chamadas LLM."""
    temperature: float = 0.2
    max_tokens: int = 2048  # ‚¨ÜÔ∏è Aumentado de 1024
    top_p: float = 0.9
    top_k: int = 40
    presence_penalty: float = 0.0
    frequency_penalty: float = 0.0


# Configura√ß√µes por tipo de modelo
LLM_CONFIGS: Dict[str, LLMParameters] = {
    'default': LLMParameters(
        temperature=0.2,
        max_tokens=2048,
        top_p=0.9
    ),
    'precise': LLMParameters(
        temperature=0.1,
        max_tokens=1536,
        top_p=0.85
    ),
    'creative': LLMParameters(
        temperature=0.4,
        max_tokens=2048,
        top_p=0.95
    ),
    'conversational': LLMParameters(
        temperature=0.35,
        max_tokens=2048,
        top_p=0.92
    )
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TEMPERATURE DIN√ÇMICA POR INTEN√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Importar dinamicamente para evitar depend√™ncias circulares
try:
    from src.analysis.intent_classifier import AnalysisIntent
    INTENT_AVAILABLE = True
except ImportError:
    INTENT_AVAILABLE = False
    AnalysisIntent = None


INTENT_TEMPERATURE_MAP: Dict[str, float] = {
    'STATISTICAL': 0.1,       # M√°xima precis√£o para c√°lculos
    'FREQUENCY': 0.15,        # Alta precis√£o para contagens
    'TEMPORAL': 0.15,         # Alta precis√£o para s√©ries temporais
    'CLUSTERING': 0.2,        # Precis√£o balanceada
    'CORRELATION': 0.15,      # Alta precis√£o para correla√ß√µes
    'OUTLIERS': 0.2,          # Precis√£o balanceada
    'COMPARISON': 0.25,       # Mais flexibilidade para compara√ß√µes
    'CONVERSATIONAL': 0.35,   # Alta diversidade para conversa√ß√£o
    'VISUALIZATION': 0.2,     # Precis√£o balanceada
    'GENERAL': 0.3,           # Explorat√≥ria com criatividade
}

# Fallback se Intent n√£o dispon√≠vel
INTENT_TEMPERATURE_MAP_FALLBACK: Dict[str, float] = {
    'statistical': 0.1,
    'frequency': 0.15,
    'temporal': 0.15,
    'clustering': 0.2,
    'correlation': 0.15,
    'outliers': 0.2,
    'comparison': 0.25,
    'conversational': 0.35,
    'visualization': 0.2,
    'general': 0.3,
    'default': 0.2
}


def get_temperature_for_intent(intent) -> float:
    """
    Retorna temperature apropriada para a inten√ß√£o anal√≠tica.
    
    Args:
        intent: AnalysisIntent enum ou string
        
    Returns:
        Temperature float (0.0-1.0)
    """
    if INTENT_AVAILABLE and isinstance(intent, AnalysisIntent):
        return INTENT_TEMPERATURE_MAP.get(intent.value.upper(), 0.2)
    
    # Fallback para string
    if isinstance(intent, str):
        return INTENT_TEMPERATURE_MAP_FALLBACK.get(intent.lower(), 0.2)
    
    return 0.2  # Default conservador


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# THRESHOLDS DE SIMILARIDADE VETORIAL
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

SIMILARITY_THRESHOLDS: Dict[str, float] = {
    # Busca principal (balanceado)
    'primary_search': 0.65,       # ‚¨áÔ∏è Reduzido de 0.7
    
    # Busca com expans√£o (recall maior)
    'fallback_search': 0.50,      # ‚¨áÔ∏è Reduzido de 0.55
    'expansion_search': 0.55,     # ‚¨áÔ∏è Reduzido de 0.65
    
    # Mem√≥ria conversacional (flex√≠vel)
    'memory_retrieval': 0.60,     # ‚¨áÔ∏è Reduzido de 0.8 (CR√çTICO!)
    'memory_short_term': 0.65,    # Mem√≥ria recente
    'memory_long_term': 0.55,     # Mem√≥ria hist√≥rica
    
    # Valida√ß√£o e verifica√ß√£o (restritivo)
    'validation': 0.75,           # Manter restritivo
    'exact_match': 0.85,          # Match quase exato
    
    # Classifica√ß√£o sem√¢ntica
    'intent_classification': 0.70,
    'entity_extraction': 0.65,
}

# Ranges de threshold por n√≠vel de rigor
THRESHOLD_RANGES: Dict[str, tuple] = {
    'strict': (0.75, 0.90),       # Alta precis√£o, baixo recall
    'balanced': (0.60, 0.75),     # Equil√≠brio
    'relaxed': (0.45, 0.60),      # Alto recall, menor precis√£o
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# LIMITES DE BUSCA E RECUPERA√á√ÉO
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

SEARCH_LIMITS: Dict[str, int] = {
    # Busca padr√£o
    'default': 5,                 # ‚¨ÜÔ∏è Aumentado de 3
    'minimum': 3,                 # M√≠nimo aceit√°vel
    'maximum': 20,                # M√°ximo permitido
    
    # Expans√£o e fallback
    'expansion': 15,              # ‚¨ÜÔ∏è Aumentado de 10
    'fallback': 10,               # Busca alternativa
    
    # Mem√≥ria
    'memory': 8,                  # ‚¨ÜÔ∏è Aumentado de 5
    'memory_recent': 5,           # Mem√≥ria recente
    'memory_all': 12,             # Toda mem√≥ria
    
    # Contexto
    'context_build': 7,           # Constru√ß√£o de contexto
    'summary': 10,                # Para resumos
}

# Fatores de expans√£o
EXPANSION_FACTORS: Dict[str, int] = {
    'query_variations': 3,        # Gerar 3 varia√ß√µes
    'limit_multiplier': 4,        # 5 * 4 = 20 na expans√£o
    'threshold_reduction': 0.15,  # Reduzir 0.15 no threshold
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# PAR√ÇMETROS DE CHUNKING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

CHUNKING_PARAMS: Dict[str, Any] = {
    # Texto geral
    'text_chunk_size': 1024,      # ‚¨ÜÔ∏è Aumentado de 512
    'text_overlap': 150,          # ‚¨ÜÔ∏è Aumentado de 50
    'min_chunk_size': 100,        # ‚¨ÜÔ∏è Aumentado de 50
    
    # CSV espec√≠fico
    'csv_chunk_rows': 30,         # ‚¨ÜÔ∏è Aumentado de 20
    'csv_overlap_rows': 6,        # ‚¨ÜÔ∏è Aumentado de 4
    'csv_min_rows': 5,            # M√≠nimo de linhas
    
    # Estrat√©gias
    'default_strategy': 'fixed_size',
    'csv_strategy': 'csv_row',
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FUN√á√ïES AUXILIARES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_config_for_query_type(query_type: str) -> LLMParameters:
    """
    Retorna configura√ß√£o LLM apropriada para tipo de query.
    
    Args:
        query_type: Tipo da query ('statistical', 'conversational', etc)
        
    Returns:
        LLMParameters configurado
    """
    type_map = {
        'statistical': 'precise',
        'conversational': 'conversational',
        'exploratory': 'creative',
        'general': 'default'
    }
    
    config_key = type_map.get(query_type.lower(), 'default')
    return LLM_CONFIGS[config_key]


def get_threshold_for_context(context: str) -> float:
    """
    Retorna threshold apropriado para contexto de busca.
    
    Args:
        context: Contexto da busca ('primary', 'memory', 'expansion', etc)
        
    Returns:
        Threshold float
    """
    key_map = {
        'primary': 'primary_search',
        'fallback': 'fallback_search',
        'expansion': 'expansion_search',
        'memory': 'memory_retrieval',
        'validation': 'validation'
    }
    
    key = key_map.get(context.lower(), 'primary_search')
    return SIMILARITY_THRESHOLDS[key]


def adjust_threshold_by_rigor(base_threshold: float, rigor: str) -> float:
    """
    Ajusta threshold baseado no n√≠vel de rigor desejado.
    
    Args:
        base_threshold: Threshold base
        rigor: N√≠vel de rigor ('strict', 'balanced', 'relaxed')
        
    Returns:
        Threshold ajustado
    """
    if rigor not in THRESHOLD_RANGES:
        return base_threshold
    
    min_thresh, max_thresh = THRESHOLD_RANGES[rigor]
    
    # Clamp threshold dentro do range
    return max(min_thresh, min(base_threshold, max_thresh))


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# VALIDA√á√ÉO DE CONFIGURA√á√ïES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def validate_configs() -> bool:
    """
    Valida todas as configura√ß√µes para garantir consist√™ncia.
    
    Returns:
        True se todas as configura√ß√µes s√£o v√°lidas
    """
    errors = []
    
    # Validar temperatures (0.0 - 1.0)
    for key, temp in INTENT_TEMPERATURE_MAP.items():
        if not 0.0 <= temp <= 1.0:
            errors.append(f"Temperature inv√°lida para {key}: {temp}")
    
    # Validar thresholds (0.0 - 1.0)
    for key, thresh in SIMILARITY_THRESHOLDS.items():
        if not 0.0 <= thresh <= 1.0:
            errors.append(f"Threshold inv√°lido para {key}: {thresh}")
    
    # Validar limites (positivos)
    for key, limit in SEARCH_LIMITS.items():
        if limit <= 0:
            errors.append(f"Limit inv√°lido para {key}: {limit}")
    
    # Validar chunk sizes
    if CHUNKING_PARAMS['text_overlap'] >= CHUNKING_PARAMS['text_chunk_size']:
        errors.append("Overlap n√£o pode ser maior que chunk_size")
    
    if CHUNKING_PARAMS['csv_overlap_rows'] >= CHUNKING_PARAMS['csv_chunk_rows']:
        errors.append("CSV overlap n√£o pode ser maior que csv_chunk_rows")
    
    if errors:
        for error in errors:
            print(f"‚ùå Erro de configura√ß√£o: {error}")
        return False
    
    print("‚úÖ Todas as configura√ß√µes s√£o v√°lidas")
    return True


# Executar valida√ß√£o ao importar
if __name__ == "__main__":
    validate_configs()
```

---

## 3. TEMPERATURE DIN√ÇMICA

### Arquivo: `src/llm/manager.py` (adicionar m√©todo)

```python
def chat_with_intent(self, 
                    prompt: str, 
                    intent,  # AnalysisIntent ou string
                    config: Optional[LLMConfig] = None,
                    system_prompt: Optional[str] = None) -> LLMResponse:
    """
    Envia prompt com temperature ajustada dinamicamente por inten√ß√£o.
    
    Args:
        prompt: Texto do prompt
        intent: AnalysisIntent enum ou string com tipo de inten√ß√£o
        config: Configura√ß√µes LLM (temperature ser√° sobrescrita)
        system_prompt: Prompt de sistema opcional
        
    Returns:
        LLMResponse com resultado da gera√ß√£o
        
    Exemplo:
        >>> from src.analysis.intent_classifier import AnalysisIntent
        >>> response = manager.chat_with_intent(
        ...     "Calcule a m√©dia de Amount",
        ...     intent=AnalysisIntent.STATISTICAL
        ... )
        >>> # Temperature ser√° 0.1 (m√°xima precis√£o)
    """
    from src.config.llm_config import get_temperature_for_intent
    
    # Criar config se n√£o fornecido
    if config is None:
        config = LLMConfig()
    
    # Ajustar temperature dinamicamente
    original_temp = config.temperature
    config.temperature = get_temperature_for_intent(intent)
    
    self.logger.info({
        'event': 'temperature_adjusted',
        'intent': str(intent),
        'original_temp': original_temp,
        'adjusted_temp': config.temperature
    })
    
    # Chamar m√©todo padr√£o
    return self.chat(prompt, config, system_prompt=system_prompt)
```

---

## 4. AJUSTES DE CHUNKING

### Arquivo: `src/embeddings/chunker.py` (modificar __init__)

```python
def __init__(self, 
             chunk_size: int = None,
             overlap_size: int = None,
             min_chunk_size: int = None,
             csv_chunk_size_rows: int = None,
             csv_overlap_rows: int = None):
    """
    Inicializa o sistema de chunking com configura√ß√µes centralizadas.
    
    Args:
        chunk_size: Tamanho alvo de cada chunk em caracteres (default: 1024)
        overlap_size: Sobreposi√ß√£o entre chunks consecutivos (default: 150)
        min_chunk_size: Tamanho m√≠nimo para considerar um chunk v√°lido (default: 100)
        csv_chunk_size_rows: N√∫mero de linhas por chunk CSV (default: 30)
        csv_overlap_rows: Overlap entre chunks CSV em linhas (default: 6)
    """
    from src.config.llm_config import CHUNKING_PARAMS
    
    # Usar valores centralizados se n√£o fornecidos
    self.chunk_size = chunk_size or CHUNKING_PARAMS['text_chunk_size']
    self.overlap_size = overlap_size or CHUNKING_PARAMS['text_overlap']
    self.min_chunk_size = min_chunk_size or CHUNKING_PARAMS['min_chunk_size']
    self.csv_chunk_size_rows = csv_chunk_size_rows or CHUNKING_PARAMS['csv_chunk_rows']
    self.csv_overlap_rows = csv_overlap_rows or CHUNKING_PARAMS['csv_overlap_rows']
    
    # Valida√ß√µes
    if self.csv_overlap_rows >= self.csv_chunk_size_rows:
        logger.warning(
            "CSV overlap (%s) >= chunk_size_rows (%s). Ajustando para chunk_size_rows - 1.",
            self.csv_overlap_rows,
            self.csv_chunk_size_rows
        )
        self.csv_overlap_rows = max(0, self.csv_chunk_size_rows - 1)
    
    if self.overlap_size >= self.chunk_size:
        logger.warning(
            "Overlap (%s) >= chunk_size (%s). Ajustando para chunk_size // 4.",
            self.overlap_size,
            self.chunk_size
        )
        self.overlap_size = self.chunk_size // 4
    
    self.logger = logger
    
    self.logger.info({
        'event': 'chunker_initialized',
        'chunk_size': self.chunk_size,
        'overlap_size': self.overlap_size,
        'min_chunk_size': self.min_chunk_size,
        'csv_chunk_rows': self.csv_chunk_size_rows,
        'csv_overlap_rows': self.csv_overlap_rows
    })
```

---

## 5. THRESHOLDS PADRONIZADOS

### Arquivo: `src/router/semantic_router.py` (modificar)

```python
def search_with_expansion(self, question: str,
                          base_threshold: float = None,
                          base_limit: int = None) -> List["VectorSearchResult"]:
    """Busca chunks com expans√£o autom√°tica e thresholds centralizados."""
    from src.config.llm_config import SIMILARITY_THRESHOLDS, SEARCH_LIMITS, EXPANSION_FACTORS
    
    # Usar configura√ß√µes centralizadas
    if base_threshold is None:
        base_threshold = SIMILARITY_THRESHOLDS['primary_search']
    
    if base_limit is None:
        base_limit = SEARCH_LIMITS['default']
    
    # Montar filtro
    filters = {}
    if self.ingestion_id:
        filters['ingestion_id'] = self.ingestion_id
    if self.source_id:
        filters['source_id'] = self.source_id

    # 1) Search original
    embedding = self.embed_question(question)
    results = self.vector_store.search_similar(
        query_embedding=embedding,
        similarity_threshold=base_threshold,
        limit=base_limit,
        filters=filters if filters else None
    )

    if results:
        return results

    # 2) Tentar refinamento via QueryRefiner
    try:
        refiner = QueryRefiner(embedding_provider=EmbeddingProvider.SENTENCE_TRANSFORMER)
        ref_result = refiner.refine_query(question)
        if ref_result and ref_result.success:
            emb = ref_result.embedding
            alt_results = self.vector_store.search_similar(
                query_embedding=emb,
                similarity_threshold=base_threshold,
                limit=base_limit,
                filters=filters if filters else None
            )
            if alt_results:
                return alt_results
    except Exception as e:
        self.logger.warning(f"QueryRefiner falhou: {e}")

    # 3) Expand queries com thresholds centralizados
    variations = StatisticalOntology.generate_simple_expansions(question)
    aggregated = []
    seen_ids = set()
    
    # Usar configura√ß√µes centralizadas para expans√£o
    expansion_threshold = SIMILARITY_THRESHOLDS['expansion_search']
    expansion_limit = SEARCH_LIMITS['expansion']

    for var in variations:
        try:
            emb = self.embed_question(var)
            alt_results = self.vector_store.search_similar(
                query_embedding=emb,
                similarity_threshold=expansion_threshold,
                limit=expansion_limit,
                filters=filters if filters else None
            )

            for r in alt_results:
                if r.embedding_id not in seen_ids:
                    aggregated.append(r)
                    seen_ids.add(r.embedding_id)
        except Exception:
            continue

    # Ordenar e retornar top results
    aggregated.sort(key=lambda x: x.similarity_score, reverse=True)
    return aggregated[:base_limit]
```

---

## 6. TESTES AUTOMATIZADOS

### Novo Arquivo: `tests/test_improved_configs.py`

```python
"""Testes para validar melhorias de configura√ß√£o do sistema."""

import pytest
from src.config.llm_config import (
    SIMILARITY_THRESHOLDS,
    SEARCH_LIMITS,
    CHUNKING_PARAMS,
    get_temperature_for_intent,
    validate_configs
)


class TestImprovedConfigs:
    """Suite de testes para configura√ß√µes melhoradas."""
    
    def test_validate_all_configs(self):
        """Verifica que todas as configura√ß√µes s√£o v√°lidas."""
        assert validate_configs() is True
    
    def test_thresholds_reduced(self):
        """Verifica que thresholds foram reduzidos conforme recomendado."""
        assert SIMILARITY_THRESHOLDS['primary_search'] == 0.65
        assert SIMILARITY_THRESHOLDS['memory_retrieval'] == 0.60  # Cr√≠tico!
        assert SIMILARITY_THRESHOLDS['expansion_search'] == 0.55
    
    def test_search_limits_increased(self):
        """Verifica que limites de busca foram aumentados."""
        assert SEARCH_LIMITS['default'] == 5  # Era 3
        assert SEARCH_LIMITS['expansion'] == 15  # Era 10
        assert SEARCH_LIMITS['memory'] == 8  # Era 5
    
    def test_chunk_size_increased(self):
        """Verifica que chunk size foi aumentado."""
        assert CHUNKING_PARAMS['text_chunk_size'] == 1024  # Era 512
        assert CHUNKING_PARAMS['text_overlap'] == 150  # Era 50
        assert CHUNKING_PARAMS['min_chunk_size'] == 100  # Era 50
    
    def test_temperature_dynamic_mapping(self):
        """Verifica mapeamento de temperature por inten√ß√£o."""
        from src.analysis.intent_classifier import AnalysisIntent
        
        # Statistical deve ter menor temperature
        temp_stat = get_temperature_for_intent(AnalysisIntent.STATISTICAL)
        assert temp_stat == 0.1
        
        # Conversational deve ter maior temperature
        temp_conv = get_temperature_for_intent(AnalysisIntent.CONVERSATIONAL)
        assert temp_conv == 0.35
        
        # Statistical < General < Conversational
        temp_gen = get_temperature_for_intent(AnalysisIntent.GENERAL)
        assert temp_stat < temp_gen < temp_conv
    
    def test_memory_threshold_critical_fix(self):
        """Verifica que threshold de mem√≥ria foi corrigido (cr√≠tico!)."""
        # Era 0.8 (muito alto), deve ser 0.6
        assert SIMILARITY_THRESHOLDS['memory_retrieval'] == 0.60
        assert SIMILARITY_THRESHOLDS['memory_retrieval'] < 0.65  # Deve ser mais baixo que busca padr√£o


class TestPromptV2Integration:
    """Testes para integra√ß√£o do Prompt V2."""
    
    @pytest.fixture
    def prompt_manager(self):
        from src.prompts.manager import get_prompt_manager, AgentRole
        return get_prompt_manager()
    
    def test_prompt_v2_exists(self, prompt_manager):
        """Verifica que Prompt V2 foi adicionado."""
        from src.prompts.manager import AgentRole
        
        prompts_available = prompt_manager.list_available_prompts(AgentRole.CSV_ANALYST)
        assert 'data_types_analysis_v2' in prompts_available[AgentRole.CSV_ANALYST.value]
    
    def test_prompt_v2_content(self, prompt_manager):
        """Verifica conte√∫do do Prompt V2."""
        from src.prompts.manager import AgentRole
        
        prompt_v2 = prompt_manager.get_prompt(AgentRole.CSV_ANALYST, 'data_types_analysis_v2')
        
        # Deve conter se√ß√µes principais
        assert 'SE√á√ÉO 1: CLASSIFICA√á√ÉO T√âCNICA' in prompt_v2
        assert 'SE√á√ÉO 2: CONTEXTO ANAL√çTICO' in prompt_v2
        assert 'SE√á√ÉO 3: INSIGHTS E RECOMENDA√á√ïES' in prompt_v2
        
        # N√£o deve ter instru√ß√µes excessivamente restritivas
        assert 'N√ÉO interprete semanticamente' not in prompt_v2
        assert 'siga RIGOROSAMENTE' not in prompt_v2


class TestChunkingImprovements:
    """Testes para melhorias de chunking."""
    
    def test_chunker_uses_new_defaults(self):
        """Verifica que chunker usa novos defaults."""
        from src.embeddings.chunker import TextChunker
        
        chunker = TextChunker()  # Sem argumentos, deve usar defaults
        
        assert chunker.chunk_size == 1024
        assert chunker.overlap_size == 150
        assert chunker.min_chunk_size == 100
    
    def test_chunker_respects_custom_values(self):
        """Verifica que valores customizados s√£o respeitados."""
        from src.embeddings.chunker import TextChunker
        
        chunker = TextChunker(chunk_size=2048, overlap_size=200)
        
        assert chunker.chunk_size == 2048
        assert chunker.overlap_size == 200


class TestLLMManagerIntegration:
    """Testes para integra√ß√£o do LLM Manager."""
    
    @pytest.fixture
    def llm_manager(self):
        from src.llm.manager import get_llm_manager
        return get_llm_manager()
    
    def test_chat_with_intent_method_exists(self, llm_manager):
        """Verifica que m√©todo chat_with_intent existe."""
        assert hasattr(llm_manager, 'chat_with_intent')
    
    def test_chat_with_intent_adjusts_temperature(self, llm_manager):
        """Verifica que temperature √© ajustada por inten√ß√£o."""
        from src.analysis.intent_classifier import AnalysisIntent
        from src.llm.manager import LLMConfig
        
        # Criar config com temperature padr√£o
        config = LLMConfig(temperature=0.5)
        
        # Mock para capturar temperatura final
        temps_used = []
        original_chat = llm_manager.chat
        
        def mock_chat(prompt, config=None, **kwargs):
            if config:
                temps_used.append(config.temperature)
            return original_chat(prompt, config, **kwargs)
        
        llm_manager.chat = mock_chat
        
        # Chamar com intent STATISTICAL
        try:
            llm_manager.chat_with_intent(
                "Calcule m√©dia", 
                intent=AnalysisIntent.STATISTICAL,
                config=config
            )
        except:
            pass  # Pode falhar na chamada real, mas capturamos temperature
        
        # Temperature deve ter sido ajustada para 0.1
        if temps_used:
            assert temps_used[0] == 0.1


# Executar testes
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

---

## üöÄ SCRIPT DE MIGRA√á√ÉO

### Arquivo: `scripts/apply_improvements.py`

```python
"""Script para aplicar melhorias de configura√ß√£o ao sistema.

Este script:
1. Valida configura√ß√µes atuais
2. Aplica novos par√¢metros
3. Re-inicializa componentes cr√≠ticos
4. Executa testes de valida√ß√£o

Uso:
    python scripts/apply_improvements.py --dry-run  # Simular apenas
    python scripts/apply_improvements.py --apply     # Aplicar mudan√ßas
"""

import argparse
import sys
from pathlib import Path

# Adicionar root ao path
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))


def validate_current_configs():
    """Valida configura√ß√µes atuais e reporta issues."""
    print("\nüîç Validando configura√ß√µes atuais...")
    
    from src.config.llm_config import validate_configs
    
    if validate_configs():
        print("‚úÖ Configura√ß√µes atuais s√£o v√°lidas")
        return True
    else:
        print("‚ùå Configura√ß√µes cont√™m erros")
        return False


def backup_current_configs():
    """Faz backup das configura√ß√µes atuais."""
    print("\nüíæ Fazendo backup das configura√ß√µes...")
    
    import shutil
    from datetime import datetime
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_dir = root_dir / "backups" / f"config_backup_{timestamp}"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    # Arquivos para backup
    files_to_backup = [
        "src/llm/manager.py",
        "src/prompts/manager.py",
        "src/embeddings/chunker.py",
        "src/router/semantic_router.py"
    ]
    
    for file_path in files_to_backup:
        source = root_dir / file_path
        if source.exists():
            dest = backup_dir / file_path
            dest.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(source, dest)
            print(f"  ‚úÖ Backup: {file_path}")
    
    print(f"‚úÖ Backup salvo em: {backup_dir}")
    return backup_dir


def apply_improvements(dry_run=False):
    """Aplica melhorias de configura√ß√£o."""
    print("\nüöÄ Aplicando melhorias...")
    
    if dry_run:
        print("  ‚ö†Ô∏è Modo DRY RUN - apenas simula√ß√£o")
    
    # 1. Validar configs
    if not validate_current_configs():
        print("‚ùå Abortando: configura√ß√µes inv√°lidas")
        return False
    
    # 2. Fazer backup
    if not dry_run:
        backup_dir = backup_current_configs()
    
    # 3. Importar novos configs
    print("\nüì¶ Importando novas configura√ß√µes...")
    try:
        from src.config.llm_config import (
            SIMILARITY_THRESHOLDS,
            SEARCH_LIMITS,
            CHUNKING_PARAMS,
            LLM_CONFIGS
        )
        print("  ‚úÖ Configura√ß√µes centralizadas carregadas")
    except ImportError as e:
        print(f"  ‚ùå Erro ao importar configs: {e}")
        return False
    
    # 4. Verificar mudan√ßas
    print("\nüìä Resumo das mudan√ßas:")
    print(f"  ‚Ä¢ Primary Search Threshold: 0.7 ‚Üí {SIMILARITY_THRESHOLDS['primary_search']}")
    print(f"  ‚Ä¢ Memory Threshold: 0.8 ‚Üí {SIMILARITY_THRESHOLDS['memory_retrieval']} (CR√çTICO!)")
    print(f"  ‚Ä¢ Search Limit: 3 ‚Üí {SEARCH_LIMITS['default']}")
    print(f"  ‚Ä¢ Chunk Size: 512 ‚Üí {CHUNKING_PARAMS['text_chunk_size']}")
    print(f"  ‚Ä¢ Overlap Size: 50 ‚Üí {CHUNKING_PARAMS['text_overlap']}")
    print(f"  ‚Ä¢ Max Tokens: 1024 ‚Üí {LLM_CONFIGS['default'].max_tokens}")
    
    if not dry_run:
        print("\n‚úÖ Melhorias aplicadas com sucesso!")
        print("\n‚ö†Ô∏è ATEN√á√ÉO: Re-ingest√£o de dados recomendada para aproveitar novo chunk_size")
        print("   Execute: python scripts/run_ingest_with_new_chunks.py")
    else:
        print("\n‚úÖ Simula√ß√£o conclu√≠da - sem mudan√ßas aplicadas")
    
    return True


def run_tests():
    """Executa suite de testes de valida√ß√£o."""
    print("\nüß™ Executando testes de valida√ß√£o...")
    
    import pytest
    
    test_file = root_dir / "tests" / "test_improved_configs.py"
    if not test_file.exists():
        print("  ‚ö†Ô∏è Arquivo de testes n√£o encontrado")
        return False
    
    result = pytest.main([str(test_file), "-v"])
    
    if result == 0:
        print("‚úÖ Todos os testes passaram")
        return True
    else:
        print("‚ùå Alguns testes falharam")
        return False


def main():
    parser = argparse.ArgumentParser(description="Aplicar melhorias de configura√ß√£o")
    parser.add_argument("--dry-run", action="store_true", 
                       help="Simular aplica√ß√£o sem fazer mudan√ßas")
    parser.add_argument("--apply", action="store_true",
                       help="Aplicar mudan√ßas efetivamente")
    parser.add_argument("--test", action="store_true",
                       help="Executar testes de valida√ß√£o")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("üîß APLICADOR DE MELHORIAS - EDA AI Minds")
    print("=" * 70)
    
    if args.test:
        run_tests()
    elif args.apply or args.dry_run:
        success = apply_improvements(dry_run=args.dry_run)
        if success and not args.dry_run:
            print("\nüéâ Melhorias aplicadas com sucesso!")
            print("\nüìã Pr√≥ximos passos:")
            print("  1. Executar testes: python scripts/apply_improvements.py --test")
            print("  2. Re-ingerir dados: python scripts/run_ingest_with_new_chunks.py")
            print("  3. Testar interface: python interface_interativa.py")
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
```

---

## ‚úÖ CHECKLIST DE IMPLEMENTA√á√ÉO

### Fase 1: Quick Wins (Imediato - 2 horas)

- [ ] Criar arquivo `src/config/llm_config.py` com configura√ß√µes centralizadas
- [ ] Aumentar `max_tokens` de 1024 para 2048 em `LLMConfig`
- [ ] Aplicar novos thresholds em `semantic_router.py`
- [ ] Executar testes automatizados

### Fase 2: Prompt V2 (Semana 1 - 6 horas)

- [ ] Adicionar Prompt V2 em `src/prompts/manager.py`
- [ ] Atualizar orchestrator para usar Prompt V2
- [ ] Criar testes A/B comparando respostas
- [ ] Validar com perguntas de exemplo

### Fase 3: Chunking (Semana 2-3 - 2 dias)

- [ ] Modificar `TextChunker.__init__` para usar configs centralizadas
- [ ] Re-ingerir dados com novo chunk_size (1024)
- [ ] Comparar qualidade de chunks antes/depois
- [ ] Validar impacto em busca vetorial

### Fase 4: Temperature Din√¢mica (Semana 4 - 6 horas)

- [ ] Adicionar m√©todo `chat_with_intent` no LLM Manager
- [ ] Integrar com IntentClassifier
- [ ] Adicionar logging de temperatures usadas
- [ ] Monitorar impacto em queries reais

### Fase 5: Valida√ß√£o Final (P√≥s-implementa√ß√£o)

- [ ] Executar suite completa de testes
- [ ] Comparar m√©tricas antes/depois
- [ ] Documentar resultados
- [ ] Ajustar par√¢metros se necess√°rio

---

**Arquivo criado por:** GitHub Copilot GPT-4.1  
**Data:** 18 de Outubro de 2025  
**Status:** ‚úÖ Ready for Implementation  
**Vers√£o:** 1.0
